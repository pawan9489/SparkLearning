<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Mastering Apache Spark</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>30 Sep 2015</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: €<strong>39.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781783987146</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Apache Spark</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Apache Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec08" class="sub-nav">
                                <a href="#ch01lvl1sec08">                    
                                    <div class="section-name">Overview</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Cluster design</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Cluster management</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Performance</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Cloud</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Apache Spark MLlib</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Apache Spark MLlib</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec14" class="sub-nav">
                                <a href="#ch02lvl1sec14">                    
                                    <div class="section-name">The environment configuration</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec15" class="sub-nav">
                                <a href="#ch02lvl1sec15">                    
                                    <div class="section-name">Classification with Nave Bayes</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec16" class="sub-nav">
                                <a href="#ch02lvl1sec16">                    
                                    <div class="section-name">Clustering with K-Means</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">ANN Artificial Neural Networks</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Apache Spark Streaming</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Apache Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec19" class="sub-nav">
                                <a href="#ch03lvl1sec19">                    
                                    <div class="section-name">Overview</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec20" class="sub-nav">
                                <a href="#ch03lvl1sec20">                    
                                    <div class="section-name">Errors and recovery</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec21" class="sub-nav">
                                <a href="#ch03lvl1sec21">                    
                                    <div class="section-name">Streaming sources</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec22" class="sub-nav">
                                <a href="#ch03lvl1sec22">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Apache Spark SQL</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Apache Spark SQL</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec23" class="sub-nav">
                                <a href="#ch04lvl1sec23">                    
                                    <div class="section-name">The SQL context</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec24" class="sub-nav">
                                <a href="#ch04lvl1sec24">                    
                                    <div class="section-name">Importing and saving data</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec25" class="sub-nav">
                                <a href="#ch04lvl1sec25">                    
                                    <div class="section-name">DataFrames</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec26" class="sub-nav">
                                <a href="#ch04lvl1sec26">                    
                                    <div class="section-name">Using SQL</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec27" class="sub-nav">
                                <a href="#ch04lvl1sec27">                    
                                    <div class="section-name">User-defined functions</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec28" class="sub-nav">
                                <a href="#ch04lvl1sec28">                    
                                    <div class="section-name">Using Hive</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec29" class="sub-nav">
                                <a href="#ch04lvl1sec29">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Apache Spark GraphX</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Apache Spark GraphX</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec30" class="sub-nav">
                                <a href="#ch05lvl1sec30">                    
                                    <div class="section-name">Overview</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec31" class="sub-nav">
                                <a href="#ch05lvl1sec31">                    
                                    <div class="section-name">GraphX coding</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec32" class="sub-nav">
                                <a href="#ch05lvl1sec32">                    
                                    <div class="section-name">Mazerunner for Neo4j</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec33" class="sub-nav">
                                <a href="#ch05lvl1sec33">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Graph-based Storage</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Graph-based Storage</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec34" class="sub-nav">
                                <a href="#ch06lvl1sec34">                    
                                    <div class="section-name">Titan</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec35" class="sub-nav">
                                <a href="#ch06lvl1sec35">                    
                                    <div class="section-name">TinkerPop</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec36" class="sub-nav">
                                <a href="#ch06lvl1sec36">                    
                                    <div class="section-name">Installing Titan</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec37" class="sub-nav">
                                <a href="#ch06lvl1sec37">                    
                                    <div class="section-name">Titan with HBase</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec38" class="sub-nav">
                                <a href="#ch06lvl1sec38">                    
                                    <div class="section-name">Titan with Cassandra</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec39" class="sub-nav">
                                <a href="#ch06lvl1sec39">                    
                                    <div class="section-name">Accessing Titan with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec40" class="sub-nav">
                                <a href="#ch06lvl1sec40">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Extending Spark with H2O</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Extending Spark with H2O</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec41" class="sub-nav">
                                <a href="#ch07lvl1sec41">                    
                                    <div class="section-name">Overview</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec42" class="sub-nav">
                                <a href="#ch07lvl1sec42">                    
                                    <div class="section-name">The processing environment</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec43" class="sub-nav">
                                <a href="#ch07lvl1sec43">                    
                                    <div class="section-name">Installing H2O</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec44" class="sub-nav">
                                <a href="#ch07lvl1sec44">                    
                                    <div class="section-name">The build environment</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec45" class="sub-nav">
                                <a href="#ch07lvl1sec45">                    
                                    <div class="section-name">Architecture</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec46" class="sub-nav">
                                <a href="#ch07lvl1sec46">                    
                                    <div class="section-name">Sourcing the data</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec47" class="sub-nav">
                                <a href="#ch07lvl1sec47">                    
                                    <div class="section-name">Data Quality</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec48" class="sub-nav">
                                <a href="#ch07lvl1sec48">                    
                                    <div class="section-name">Performance tuning</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec49" class="sub-nav">
                                <a href="#ch07lvl1sec49">                    
                                    <div class="section-name">Deep learning</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec50" class="sub-nav">
                                <a href="#ch07lvl1sec50">                    
                                    <div class="section-name">H2O Flow</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec51" class="sub-nav">
                                <a href="#ch07lvl1sec51">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Spark Databricks</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Spark Databricks</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec52" class="sub-nav">
                                <a href="#ch08lvl1sec52">                    
                                    <div class="section-name">Overview</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec53" class="sub-nav">
                                <a href="#ch08lvl1sec53">                    
                                    <div class="section-name">Installing Databricks</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec54" class="sub-nav">
                                <a href="#ch08lvl1sec54">                    
                                    <div class="section-name">AWS billing</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec55" class="sub-nav">
                                <a href="#ch08lvl1sec55">                    
                                    <div class="section-name">Databricks menus</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec56" class="sub-nav">
                                <a href="#ch08lvl1sec56">                    
                                    <div class="section-name">Account management</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec57" class="sub-nav">
                                <a href="#ch08lvl1sec57">                    
                                    <div class="section-name">Cluster management</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec58" class="sub-nav">
                                <a href="#ch08lvl1sec58">                    
                                    <div class="section-name">Notebooks and folders</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec59" class="sub-nav">
                                <a href="#ch08lvl1sec59">                    
                                    <div class="section-name">Jobs and libraries</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec60" class="sub-nav">
                                <a href="#ch08lvl1sec60">                    
                                    <div class="section-name">Development environments</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec61" class="sub-nav">
                                <a href="#ch08lvl1sec61">                    
                                    <div class="section-name">Databricks tables</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec62" class="sub-nav">
                                <a href="#ch08lvl1sec62">                    
                                    <div class="section-name">The DbUtils package</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec63" class="sub-nav">
                                <a href="#ch08lvl1sec63">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Databricks Visualization</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Databricks Visualization</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec64" class="sub-nav">
                                <a href="#ch09lvl1sec64">                    
                                    <div class="section-name">Data visualization</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec65" class="sub-nav">
                                <a href="#ch09lvl1sec65">                    
                                    <div class="section-name">REST interface</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec66" class="sub-nav">
                                <a href="#ch09lvl1sec66">                    
                                    <div class="section-name">Moving data</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec67" class="sub-nav">
                                <a href="#ch09lvl1sec67">                    
                                    <div class="section-name">Further reading</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec68" class="sub-nav">
                                <a href="#ch09lvl1sec68">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="22051" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Mastering Apache Spark</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Mike Frampton</h5>
                            <div>
                                <p class="mb20"><b>Gain expertise in processing and storing data by using advanced techniques with Apache Spark</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Explore the integration of Apache Spark with third party applications such as H20, Databricks and Titan</li>
                <li>Evaluate how Cassandra and Hbase can be used for storage</li>
                <li>An advanced guide with a combination of instructions and practical examples to extend the most up-to date Spark functionalities</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Extend the tools available for processing and storage</li>
                <li>Examine clustering and classification using MLlib</li>
                <li>Discover Spark stream processing via Flume, HDFS</li>
                <li>Create a schema in Spark SQL, and learn how a Spark schema can be populated with data</li>
                <li>Study Spark based graph processing using Spark GraphX</li>
                <li>Combine Spark with H20 and deep learning and learn why it is useful</li>
                <li>Evaluate how graph storage works with Apache Spark, Titan, HBase and Cassandra</li>
                <li>Use Apache Spark in the cloud with Databricks and AWS</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Apache Spark is an in-memory cluster based parallel processing system that provides a wide range of functionality like graph processing, machine learning, stream processing and SQL. It operates at unprecedented speeds, is easy to use and offers a rich set of data transformations.</p>
                <p>This book aims to take your limited knowledge of Spark to the next level by teaching you how to expand Spark functionality. The book commences with an overview of the Spark eco-system. You will learn how to use MLlib to create a fully working neural net for handwriting recognition. You will then discover how stream processing can be tuned for optimal performance and to ensure parallel processing. The book extends to show how to incorporate H20 for machine learning, Titan for graph based storage, Databricks for cloud-based Spark. Intermediate Scala based code examples are provided for Apache Spark module processing in a CentOS Linux and Databricks cloud environment.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Apache Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Apache Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec08" class="chapter-section">
                                                                    <a href="#ch01lvl1sec08">                    
                                                                        <div class="section-name">Overview</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Cluster design</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Cluster management</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Performance</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Cloud</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Apache Spark MLlib</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Apache Spark MLlib</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec14" class="chapter-section">
                                                                    <a href="#ch02lvl1sec14">                    
                                                                        <div class="section-name">The environment configuration</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec15" class="chapter-section">
                                                                    <a href="#ch02lvl1sec15">                    
                                                                        <div class="section-name">Classification with Nave Bayes</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec16" class="chapter-section">
                                                                    <a href="#ch02lvl1sec16">                    
                                                                        <div class="section-name">Clustering with K-Means</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">ANN Artificial Neural Networks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Apache Spark Streaming</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Apache Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec19" class="chapter-section">
                                                                    <a href="#ch03lvl1sec19">                    
                                                                        <div class="section-name">Overview</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec20" class="chapter-section">
                                                                    <a href="#ch03lvl1sec20">                    
                                                                        <div class="section-name">Errors and recovery</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec21" class="chapter-section">
                                                                    <a href="#ch03lvl1sec21">                    
                                                                        <div class="section-name">Streaming sources</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec22" class="chapter-section">
                                                                    <a href="#ch03lvl1sec22">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Apache Spark SQL</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Apache Spark SQL</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec23" class="chapter-section">
                                                                    <a href="#ch04lvl1sec23">                    
                                                                        <div class="section-name">The SQL context</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec24" class="chapter-section">
                                                                    <a href="#ch04lvl1sec24">                    
                                                                        <div class="section-name">Importing and saving data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec25" class="chapter-section">
                                                                    <a href="#ch04lvl1sec25">                    
                                                                        <div class="section-name">DataFrames</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec26" class="chapter-section">
                                                                    <a href="#ch04lvl1sec26">                    
                                                                        <div class="section-name">Using SQL</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec27" class="chapter-section">
                                                                    <a href="#ch04lvl1sec27">                    
                                                                        <div class="section-name">User-defined functions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec28" class="chapter-section">
                                                                    <a href="#ch04lvl1sec28">                    
                                                                        <div class="section-name">Using Hive</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec29" class="chapter-section">
                                                                    <a href="#ch04lvl1sec29">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Apache Spark GraphX</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Apache Spark GraphX</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec30" class="chapter-section">
                                                                    <a href="#ch05lvl1sec30">                    
                                                                        <div class="section-name">Overview</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec31" class="chapter-section">
                                                                    <a href="#ch05lvl1sec31">                    
                                                                        <div class="section-name">GraphX coding</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec32" class="chapter-section">
                                                                    <a href="#ch05lvl1sec32">                    
                                                                        <div class="section-name">Mazerunner for Neo4j</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec33" class="chapter-section">
                                                                    <a href="#ch05lvl1sec33">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Graph-based Storage</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Graph-based Storage</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec34" class="chapter-section">
                                                                    <a href="#ch06lvl1sec34">                    
                                                                        <div class="section-name">Titan</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec35" class="chapter-section">
                                                                    <a href="#ch06lvl1sec35">                    
                                                                        <div class="section-name">TinkerPop</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec36" class="chapter-section">
                                                                    <a href="#ch06lvl1sec36">                    
                                                                        <div class="section-name">Installing Titan</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec37" class="chapter-section">
                                                                    <a href="#ch06lvl1sec37">                    
                                                                        <div class="section-name">Titan with HBase</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec38" class="chapter-section">
                                                                    <a href="#ch06lvl1sec38">                    
                                                                        <div class="section-name">Titan with Cassandra</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec39" class="chapter-section">
                                                                    <a href="#ch06lvl1sec39">                    
                                                                        <div class="section-name">Accessing Titan with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec40" class="chapter-section">
                                                                    <a href="#ch06lvl1sec40">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Extending Spark with H2O</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Extending Spark with H2O</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec41" class="chapter-section">
                                                                    <a href="#ch07lvl1sec41">                    
                                                                        <div class="section-name">Overview</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec42" class="chapter-section">
                                                                    <a href="#ch07lvl1sec42">                    
                                                                        <div class="section-name">The processing environment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec43" class="chapter-section">
                                                                    <a href="#ch07lvl1sec43">                    
                                                                        <div class="section-name">Installing H2O</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec44" class="chapter-section">
                                                                    <a href="#ch07lvl1sec44">                    
                                                                        <div class="section-name">The build environment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec45" class="chapter-section">
                                                                    <a href="#ch07lvl1sec45">                    
                                                                        <div class="section-name">Architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec46" class="chapter-section">
                                                                    <a href="#ch07lvl1sec46">                    
                                                                        <div class="section-name">Sourcing the data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec47" class="chapter-section">
                                                                    <a href="#ch07lvl1sec47">                    
                                                                        <div class="section-name">Data Quality</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec48" class="chapter-section">
                                                                    <a href="#ch07lvl1sec48">                    
                                                                        <div class="section-name">Performance tuning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec49" class="chapter-section">
                                                                    <a href="#ch07lvl1sec49">                    
                                                                        <div class="section-name">Deep learning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec50" class="chapter-section">
                                                                    <a href="#ch07lvl1sec50">                    
                                                                        <div class="section-name">H2O Flow</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec51" class="chapter-section">
                                                                    <a href="#ch07lvl1sec51">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Spark Databricks</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Spark Databricks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec52" class="chapter-section">
                                                                    <a href="#ch08lvl1sec52">                    
                                                                        <div class="section-name">Overview</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec53" class="chapter-section">
                                                                    <a href="#ch08lvl1sec53">                    
                                                                        <div class="section-name">Installing Databricks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec54" class="chapter-section">
                                                                    <a href="#ch08lvl1sec54">                    
                                                                        <div class="section-name">AWS billing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec55" class="chapter-section">
                                                                    <a href="#ch08lvl1sec55">                    
                                                                        <div class="section-name">Databricks menus</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec56" class="chapter-section">
                                                                    <a href="#ch08lvl1sec56">                    
                                                                        <div class="section-name">Account management</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec57" class="chapter-section">
                                                                    <a href="#ch08lvl1sec57">                    
                                                                        <div class="section-name">Cluster management</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec58" class="chapter-section">
                                                                    <a href="#ch08lvl1sec58">                    
                                                                        <div class="section-name">Notebooks and folders</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec59" class="chapter-section">
                                                                    <a href="#ch08lvl1sec59">                    
                                                                        <div class="section-name">Jobs and libraries</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec60" class="chapter-section">
                                                                    <a href="#ch08lvl1sec60">                    
                                                                        <div class="section-name">Development environments</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec61" class="chapter-section">
                                                                    <a href="#ch08lvl1sec61">                    
                                                                        <div class="section-name">Databricks tables</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec62" class="chapter-section">
                                                                    <a href="#ch08lvl1sec62">                    
                                                                        <div class="section-name">The DbUtils package</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec63" class="chapter-section">
                                                                    <a href="#ch08lvl1sec63">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Databricks Visualization</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Databricks Visualization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec64" class="chapter-section">
                                                                    <a href="#ch09lvl1sec64">                    
                                                                        <div class="section-name">Data visualization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec65" class="chapter-section">
                                                                    <a href="#ch09lvl1sec65">                    
                                                                        <div class="section-name">REST interface</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec66" class="chapter-section">
                                                                    <a href="#ch09lvl1sec66">                    
                                                                        <div class="section-name">Moving data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec67" class="chapter-section">
                                                                    <a href="#ch09lvl1sec67">                    
                                                                        <div class="section-name">Further reading</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec68" class="chapter-section">
                                                                    <a href="#ch09lvl1sec68">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Mike Frampton</strong></p>
                                            <div>
                                                <p>Mike Frampton is an IT contractor, blogger, and IT author with a keen interest in new technology and big data. He has worked in the IT industry since 1990 in a range of roles (tester, developer, support, and author). He has also worked in many other sectors (energy, banking, telecoms, and insurance). He now lives by the beach in Pa raparaumu, New Zealand, with his wife and teenage son. Being married to a Thai national, he divides his time between Paraparaumu and their house in Roi Et, Thailand, between writing and IT consulting. He is always keen to hear about new ideas and technologies in the areas of big data, AI, IT and hardware, so look him up on LinkedIn (<a title="http://linkedin.com/profile/view?id=73219349" href="http://linkedin.com/profile/view?id=73219349" target="_blank">http://linkedin.com/profile/view?id=73219349</a>) or his website (<a title="http://www.semtech-solutions.co.nz/#!/pageHome" href="http://www.semtech-solutions.co.nz/#!/pageHome" target="_blank">http://www.semtech-solutions.co.nz/#!/pageHome</a>) to ask questions or just to say hi.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>Chapter 1. Apache Spark</h2></div></div></div><p>Apache Spark is a distributed and highly scalable in-memory data analytics system, providing the <a id="id0" class="indexterm"></a>ability to develop applications in Java, Scala, Python, as well as languages like R. It has one of the highest contribution/involvement rates among the Apache top level projects at this time. Apache systems, such as Mahout, now use it as a processing engine instead of MapReduce. Also, as will be shown in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Apache Spark SQL</em></span>, it is possible to use a Hive context to have the Spark applications process data directly to and from Apache Hive.</p><p>Apache Spark provides four main submodules, which are SQL, MLlib, GraphX, and Streaming. They will all be explained in their own chapters, but a simple overview would be useful here. The modules are interoperable, so data can be passed between them. For instance, streamed data can be passed to SQL, and a temporary table can be created.</p><p>The following figure explains how this book will address Apache Spark and its modules. The top two rows show Apache Spark, and its four submodules described earlier. However, wherever possible, I always try to show by giving an example how the functionality may be extended using the extra tools:</p><div class="mediaobject"><img src="graphics/B01989_01_01.jpg" /></div><p>For instance, the data streaming module explained in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Apache Spark Streaming</em></span>, will have worked examples, showing how data movement is performed using Apache <span class="strong"><strong>Kafka</strong></span> <a id="id1" class="indexterm"></a>and <span class="strong"><strong>Flume</strong></span>. The <span class="strong"><strong>MLlib</strong></span> <a id="id2" class="indexterm"></a>or the machine learning module will have its functionality examined<a id="id3" class="indexterm"></a> in terms of the data processing functions that are available, but it will also be extended using the H2O system and deep learning.</p><p>The previous<a id="id4" class="indexterm"></a> figure is, of course, simplified. It represents the system relationships presented in this book. For instance, there are many more routes between Apache Spark modules and HDFS than the ones shown in the preceding diagram.</p><p>The Spark SQL chapter will also show how Spark can use a Hive Context. So, a Spark application can be developed to create Hive-based objects, and run Hive QL against Hive tables, stored in HDFS.</p><p>
<a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Apache Spark GraphX</em></span>, and <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Graph-based Storage</em></span>, will show how the Spark GraphX module can be used to process big data scale graphs, and how they can be stored using the Titan graph database. It will be shown that Titan will allow big data scale graphs to be stored, and queried as graphs. It will show, by an example, that Titan can use both, <span class="strong"><strong>HBase</strong></span> and <span class="strong"><strong>Cassandra</strong></span> as a storage mechanism. When using HBase, it will be shown that<a id="id5" class="indexterm"></a> implicitly, Titan<a id="id6" class="indexterm"></a> uses HDFS as a cheap and reliable distributed storage mechanism.</p><p>So, I think that this section has explained that Spark is an in-memory processing system. When used at scale, it cannot exist alone—the data must reside somewhere. It will probably be<a id="id7" class="indexterm"></a> used along with the Hadoop tool set, and the associated eco-system. Luckily, Hadoop stack providers, such as Cloudera, provide the CDH Hadoop stack and cluster manager, which integrates with Apache Spark, Hadoop, and most of the current stable tool set. During this book, I will use a small CDH 5.3 cluster installed on CentOS 6.5 64 bit servers. You can use an alternative configuration, but I find that CDH provides most of the tools that I need, and automates the configuration, leaving me more time for development.</p><p>Having mentioned the Spark modules and the software that will be introduced in this book, the next section will describe the possible design of a big data cluster.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec08"></a>Overview</h2></div></div><hr /></div><p>In this section, I <a id="id8" class="indexterm"></a>wish to provide an overview of the functionality that will be introduced in this book in terms of Apache Spark, and the systems that will be used to extend it. I will also try to examine the future of Apache Spark, as it integrates with cloud storage.</p><p>When you examine<a id="id9" class="indexterm"></a> the documentation on the Apache Spark website (<a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>), you will see that there are topics that cover SparkR and Bagel. Although<a id="id10" class="indexterm"></a> I will cover the four main Spark modules in this book, I will not cover these two topics. I have limited time and scope in this book so I will leave these topics for reader investigation or for a future date.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec06"></a>Spark Machine Learning</h3></div></div></div><p>The Spark MLlib <a id="id11" class="indexterm"></a>module offers machine learning functionality over a <a id="id12" class="indexterm"></a>number of domains. The documentation available at the Spark website introduces the data types used (for example, vectors and the LabeledPoint structure). This module offers functionality that includes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Statistics</p></li><li style="list-style-type: disc"><p>Classification</p></li><li style="list-style-type: disc"><p>Regression</p></li><li style="list-style-type: disc"><p>Collaborative Filtering</p></li><li style="list-style-type: disc"><p>Clustering</p></li><li style="list-style-type: disc"><p>Dimensionality Reduction</p></li><li style="list-style-type: disc"><p>Feature Extraction</p></li><li style="list-style-type: disc"><p>Frequent Pattern Mining</p></li><li style="list-style-type: disc"><p>Optimization</p></li></ul></div><p>The Scala-based<a id="id13" class="indexterm"></a> practical examples of KMeans, Naïve <a id="id14" class="indexterm"></a>Bayes, and Artificial Neural Networks have been introduced and discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Apache Spark MLlib</em></span> of this book.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>Spark Streaming</h3></div></div></div><p>Stream processing is another big and popular topic for Apache Spark. It involves the processing of data in Spark as <a id="id15" class="indexterm"></a>streams, and covers topics such as input and output operations, transformations, persistence, and check pointing among others.</p><p>
<a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Apache Spark Streaming</em></span>, covers this area of processing, and provides practical examples of <a id="id16" class="indexterm"></a>different types of stream processing. It discusses batch and <a id="id17" class="indexterm"></a>window stream configuration, and provides a practical example of checkpointing. It also covers different examples of stream processing, including Kafka and Flume.</p><p>There are many more ways in which stream data can be used. Other Spark module functionality (for example, SQL, MLlib, and GraphX) can be used to process the stream. You can use Spark streaming with systems such as Kinesis or ZeroMQ. You can even create custom receivers for your own user-defined data sources.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec08"></a>Spark SQL</h3></div></div></div><p>From Spark version 1.3 data frames have been introduced into Apache Spark so that Spark data can be processed in a<a id="id18" class="indexterm"></a> tabular form and tabular functions (like select, filter, groupBy) can be used to process data. The Spark SQL module<a id="id19" class="indexterm"></a> integrates with Parquet and JSON formats to allow data to be stored in formats that better represent data. This also offers more options to integrate with external systems.</p><p>The idea of integrating Apache Spark into the Hadoop Hive big data database can also be introduced. Hive context-based Spark applications can be used to manipulate Hive-based table data. This <a id="id20" class="indexterm"></a>brings Spark's fast in-memory distributed processing to Hive's big data storage capabilities. It effectively lets Hive use Spark as a <a id="id21" class="indexterm"></a>processing engine.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec09"></a>Spark graph processing</h3></div></div></div><p>The Apache Spark GraphX <a id="id22" class="indexterm"></a>module allows Spark to offer fast, big <a id="id23" class="indexterm"></a>data in memory graph processing. A graph is represented by a list of vertices and edges (the lines that connect the vertices). GraphX is able to create and manipulate graphs using the property, structural, join, aggregation, cache, and uncache operators.</p><p>It introduces two new data types to support graph processing in Spark: VertexRDD and EdgeRDD to represent graph vertexes and edges. It also introduces graph processing example functions, such as PageRank and triangle processing. Many of these functions will be examined in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Apache Spark GraphX</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Extended ecosystem</h3></div></div></div><p>When examining big data processing systems, I think it is important to look at not just the system itself, but<a id="id24" class="indexterm"></a> also how it can be extended, and how it integrates with external systems, so that greater levels of functionality can be offered. In a book of this size, I cannot cover every option, but hopefully by introducing a topic, I can stimulate the reader's interest, so that they can investigate further.</p><p>I have used the H2O machine learning library system to extend Apache Spark's machine learning module. By using an H2O deep learning Scala-based example, I have shown how neural processing can be introduced to Apache Spark. I am, however, aware that I have just scratched the surface of H2O's functionality. I have only used a small neural cluster and a single type of classification functionality. Also, there is a lot more to H2O than deep learning.</p><p>As graph processing becomes more accepted and used in the coming years, so will graph based storage. I have investigated the use of Spark with the NoSQL database Neo4J, using the Mazerunner prototype application. I have also investigated the use of the Aurelius (Datastax) Titan database for graph-based storage. Again, Titan is a database in its infancy, which needs both community support and further development. But I wanted to examine the future options for Apache Spark integration.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>The future of Spark</h3></div></div></div><p>The next section will show that the Apache Spark release contains scripts to allow a Spark cluster to be created on AWS EC2 storage. There are a range of options available that allow the cluster<a id="id25" class="indexterm"></a> creator to define attributes such as cluster size and storage type. But this type of cluster is difficult to resize, which makes it difficult to manage changing requirements. If the data volume changes or grows over time a larger cluster maybe required with more memory.</p><p>Luckily, the people that developed Apache Spark have created a new start-up called Databricks <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>, which offers web console-based Spark cluster management, plus a <a id="id26" class="indexterm"></a>lot of other functionality. It offers the idea of work organized by notebooks, user access control, security, and a mass of other functionality. It is described at the end of this book.</p><p>It is a service in its infancy, currently only offering cloud-based storage on Amazon AWS, but it will probably extend to Google and Microsoft Azure in the future. The other cloud-based providers, that is, Google and Microsoft Azure, are also extending their services, so that they can offer Apache Spark processing in the cloud.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Cluster design</h2></div></div><hr /></div><p>As I already mentioned, Apache Spark is a distributed, in-memory, parallel processing system, which needs an <a id="id27" class="indexterm"></a>associated storage mechanism. So, when you build a big data cluster, you will probably use a distributed storage system <a id="id28" class="indexterm"></a>such as Hadoop, as well as tools to move data like Sqoop, Flume, and Kafka.</p><p>I wanted to introduce the idea of edge nodes in a big data cluster. Those nodes in the cluster will be client facing, on which reside the client facing components like the Hadoop NameNode or perhaps the Spark master. The majority of the big data cluster might be behind a firewall. The edge nodes would then reduce the complexity caused by the firewall, as they would be the only nodes that would be accessible. The following figure shows a simplified big data cluster:</p><div class="mediaobject"><img src="graphics/B01989_01_02.jpg" /></div><p>It shows four simplified cluster racks with switches and edge node computers, facing the client across the<a id="id29" class="indexterm"></a> firewall. This is, of course, stylized and simplified, but you get the idea. The general <a id="id30" class="indexterm"></a>processing nodes are hidden behind a firewall (the dotted line), and are available for general processing, in terms of Hadoop, Apache Spark, Zookeeper, Flume, and/or Kafka. The following figure represents a couple of big data cluster edge nodes, and attempts to show what applications might reside on them.</p><p>The edge node applications will be the master applications similar to the Hadoop NameNode, or the Apache Spark master server. It will be the components that are bringing the data into and out of the cluster such as Flume, Sqoop, and Kafka. It can be any component that makes a user interface available to the client user similar to Hive:</p><div class="mediaobject"><img src="graphics/B01989_01_03.jpg" /></div><p>Generally, firewalls, while adding security to the cluster, also increase the complexity. Ports between <a id="id31" class="indexterm"></a>system components need to be opened up, so that they can talk to each other. For instance, Zookeeper is used by many components for configuration. Apache Kafka, the publish subscribe messaging system, uses Zookeeper for configuring its topics, groups, consumers, and producers. So client ports to Zookeeper, potentially across the firewall, need to be open.</p><p>Finally, the allocation of systems to cluster nodes needs to be considered. For instance, if Apache Spark uses Flume or Kafka, then in-memory channels will be used. The size of these channels, and the memory used, caused by the data flow, need to be considered. Apache Spark should not be competing with other Apache components for memory usage. Depending<a id="id32" class="indexterm"></a> upon your data flows and memory usage, it might be necessary to have the Spark, Hadoop, Zookeeper, Flume, and other tools on distinct cluster nodes.</p><p>Generally, the edge nodes that act as cluster NameNode servers, or Spark master servers, will need greater resources than the cluster processing nodes within the firewall. For instance, a CDH cluster node manager server will need extra memory, as will the Spark master server. You should monitor edge nodes for resource usage, and adjust in terms of resources and/or application location as necessary.</p><p>This section has<a id="id33" class="indexterm"></a> briefly set the scene for the big data cluster in terms of Apache Spark, Hadoop, and other tools. However, how might the Apache Spark cluster itself, within the big data cluster, be configured? For instance, it is possible to have many<a id="id34" class="indexterm"></a> types of Spark cluster manager. The next section will examine this, and describe each type of Apache Spark cluster manager.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Cluster management</h2></div></div><hr /></div><p>The following<a id="id35" class="indexterm"></a> diagram, borrowed from the spark.apache.org <a id="id36" class="indexterm"></a>website, demonstrates the role of the Apache Spark cluster manager in terms of the master, slave (worker), executor, and Spark client applications:</p><div class="mediaobject"><img src="graphics/B01989_01_04.jpg" /></div><p>The Spark context, as you will see from many of the examples in this book, can be defined via a Spark configuration object, and a Spark URL. The Spark context connects to the Spark cluster manager, which then allocates resources across the worker nodes for the application. The cluster manager allocates executors across the cluster worker nodes. It copies the application jar file to the workers, and finally it allocates tasks.</p><p>The following<a id="id37" class="indexterm"></a> subsections describe the possible Apache Spark cluster manager options available at this time.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>Local</h3></div></div></div><p>By specifying a Spark configuration local URL, it is possible to have the application run locally. By specifying<a id="id38" class="indexterm"></a> local[n], it is possible to have Spark use <code class="literal">&lt;n&gt;</code> threads to run the application locally. This is a useful development and test option.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>Standalone</h3></div></div></div><p>Standalone mode uses a basic cluster manager that is supplied with Apache Spark. The spark master URL will <a id="id39" class="indexterm"></a>be as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Spark://&lt;hostname&gt;:7077</strong></span>
</pre></div><p>Here, <code class="literal">&lt;hostname&gt;</code> is the name of the host on which the Spark master is running. I have specified <code class="literal">7077</code> as the port, which is the default value, but it is configurable. This simple cluster manager, currently, only supports FIFO (first in first out) scheduling. You can contrive to allow concurrent application scheduling by setting the resource configuration options for each application. For instance, using <code class="literal">spark.core.max</code> to share cores between applications.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>Apache YARN</h3></div></div></div><p>At a larger scale when integrating with Hadoop YARN, the Apache Spark cluster manager can be<a id="id40" class="indexterm"></a> YARN, and the application can run in one of two modes. If the Spark master value is set as yarn-cluster, then the application can be <a id="id41" class="indexterm"></a>submitted to the cluster, and then terminated. The cluster will take care of allocating resources and running tasks. However, if the application master is submitted as yarn-client, then the application stays alive during the life cycle of processing, and requests resources from YARN.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec15"></a>Apache Mesos</h3></div></div></div><p>Apache Mesos is an<a id="id42" class="indexterm"></a> open source system for resource<a id="id43" class="indexterm"></a> sharing across a cluster. It allows multiple frameworks to share a cluster by managing and scheduling resources. It is a cluster manager, which provides isolation using Linux containers, allowing multiple systems, like Hadoop, Spark, Kafka, Storm, and more to share a cluster safely. It is highly scalable to thousands of nodes. It is a master slave-based system, and is fault tolerant, using Zookeeper for configuration management.</p><p>For a single <a id="id44" class="indexterm"></a>master node Mesos cluster, the Spark master URL will be in this form:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mesos://&lt;hostname&gt;:5050</strong></span>
</pre></div><p>Where <code class="literal">&lt;hostname&gt;</code> is the host name of the Mesos master server, the port is defined as <code class="literal">5050</code>, which is the<a id="id45" class="indexterm"></a> default Mesos master port (this is configurable). If there are multiple Mesos master servers in a large scale high availability Mesos cluster, then the Spark master URL would look like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mesos://zk://&lt;hostname&gt;:2181</strong></span>
</pre></div><p>So, the election of the Mesos master server will be controlled by Zookeeper. The <code class="literal">&lt;hostname&gt;</code> will be the name of a host in the Zookeeper quorum. Also, the port number <code class="literal">2181</code> is the default master port for Zookeeper.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec16"></a>Amazon EC2</h3></div></div></div><p>The Apache <a id="id46" class="indexterm"></a>Spark release contains scripts for running Spark in the cloud against Amazon AWS EC2-based servers. The following listing, as an example, shows Spark 1.3.1 installed on a Linux<a id="id47" class="indexterm"></a> CentOS server, under the directory called <code class="literal">/usr/local/spark/</code>. The EC2 resources are available in the Spark release EC2 subdirectory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ec2]$ pwd</strong></span>

<span class="strong"><strong>/usr/local/spark/ec2</strong></span>

<span class="strong"><strong>[hadoop@hc2nn ec2]$ ls</strong></span>
<span class="strong"><strong>deploy.generic  README  spark-ec2  spark_ec2.py</strong></span>
</pre></div><p>In order to use Apache Spark on EC2, you will need to set up an Amazon AWS account. You can set up<a id="id48" class="indexterm"></a> an initial free account to try it out here: <a class="ulink" href="http://aws.amazon.com/free/" target="_blank">http://aws.amazon.com/free/</a>.</p><p>If you take a look at <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark Databricks</em></span> you will see that such an account has been set up, and is used to<a id="id49" class="indexterm"></a> access <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>. The next thing that you will need to do is access your AWS IAM Console, and select the <span class="strong"><strong>Users</strong></span> option. You either create or select a user. Select the <span class="strong"><strong>User Actions</strong></span> option, and then select <span class="strong"><strong>Manage Access Keys</strong></span>. Then, select <span class="strong"><strong>Create Access Key</strong></span>, and then <span class="strong"><strong>Download Credentials</strong></span>. Make sure that your downloaded key file is secure, assuming that you are on Linux chmod the file with permissions <code class="literal">= 600</code> for user-only access.</p><p>You will now have your <span class="strong"><strong>Access Key ID</strong></span>, <span class="strong"><strong>Secret Access Key</strong></span>, key file, and key pair name. You can now create a Spark EC2 cluster using the <code class="literal">spark-ec2</code> script as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export AWS_ACCESS_KEY_ID="QQpl8Exxx"</strong></span>
<span class="strong"><strong>export AWS_SECRET_ACCESS_KEY="0HFzqt4xxx"</strong></span>

<span class="strong"><strong>./spark-ec2  \</strong></span>
<span class="strong"><strong>    --key-pair=pairname \</strong></span>
<span class="strong"><strong>    --identity-file=awskey.pem \</strong></span>
<span class="strong"><strong>    --region=us-west-1 \</strong></span>
<span class="strong"><strong>    --zone=us-west-1a  \</strong></span>
<span class="strong"><strong>    launch cluster1</strong></span>
</pre></div><p>Here, <code class="literal">&lt;pairname&gt;</code> is the key pair name that you gave when your access details were created; <code class="literal">&lt;awskey.pem&gt;</code> is the file that you downloaded. The name of the cluster that you are going to create is called <code class="literal">&lt;cluster1&gt;</code>. The region chosen here is in the western USA, <code class="literal">us-west-1</code>. If you<a id="id50" class="indexterm"></a> live in the Pacific, as I do, it might be wiser to choose a nearer region like <code class="literal">ap-southeast-2</code>. However, if you encounter<a id="id51" class="indexterm"></a> allowance access issues, then you will need to try another zone. Remember also that using cloud-based Spark clustering like this will have higher latency and poorer I/O in general. You share your cluster hosts with multiple users, and your cluster maybe in a remote region.</p><p>You can use a series of options to this basic command to configure the cloud-based Spark cluster that you create. The <code class="literal">–s</code> option can be used:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>-s &lt;slaves&gt;</strong></span>
</pre></div><p>This allows you to define how many worker nodes to create in your Spark EC2 cluster, that is, <code class="literal">–s 5</code> for a six node cluster, one master, and five slave workers. You can define the version of Spark that your cluster runs, rather than the default latest version. The following option starts a cluster with Spark version 1.3.1:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>--spark-version=1.3.1</strong></span>
</pre></div><p>The instance type used to create the cluster will define how much memory is used, and how many cores are available. For instance, the following option will set the instance type to be <code class="literal">m3.large</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>--instance-type=m3.large</strong></span>
</pre></div><p>The current instance types for <a id="id52" class="indexterm"></a>Amazon AWS can be found at: <a class="ulink" href="http://aws.amazon.com/ec2/instance-types/" target="_blank">http://aws.amazon.com/ec2/instance-types/</a>.</p><p>The following <a id="id53" class="indexterm"></a>figure shows the current (as of July 2015) AWS M3 instance types, model details, cores, memory, and storage. There are many instance types available at this time; for instance, T2, M4, M3, C4, C3, R3, and more. Examine the current availability and choose appropriately:</p><div class="mediaobject"><img src="graphics/B01989_01_05.jpg" /></div><p>Pricing is also very<a id="id54" class="indexterm"></a> important. The current AWS storage type prices can be found at: <a class="ulink" href="http://aws.amazon.com/ec2/pricing/" target="_blank">http://aws.amazon.com/ec2/pricing/</a>.</p><p>The prices are<a id="id55" class="indexterm"></a> shown by region with a drop-down menu, and a price by hour. Remember that each storage type is defined by cores, memory, and physical storage. The prices are also defined by operating system type, that is, Linux, RHEL, and Windows. Just select the OS via a top-level menu.</p><p>The following figure shows an example of pricing at the time of writing (July 2015); it is just provided to give an idea. Prices will differ over time, and by service provider. They will differ by the size of storage that you need, and the length of time that you are willing to commit to.</p><p>Be aware also of the costs of moving your data off of any storage platform. Try to think long term. Check whether you will need to move all, or some of your cloud-based data to the next system in, say, five years. Check the process to move data, and include that cost in your planning.</p><div class="mediaobject"><img src="graphics/B01989_01_06.jpg" /></div><p>As described, the<a id="id56" class="indexterm"></a> preceding figure shows the costs of AWS storage types by operating system, region, storage type, and hour. The costs are measured per <a id="id57" class="indexterm"></a>unit hour, so systems such as <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a> do not terminate EC2 instances, until a full hour has elapsed. These costs will change with time and need to be monitored via (for AWS) the AWS billing console.</p><p>You may also<a id="id58" class="indexterm"></a> have problems when wanting to resize your Spark EC2 cluster, so you will need to be sure of the master slave configuration before you start. Be sure how many workers you are going to require, and how much memory you need. If you feel that your requirements are going to change over time, then you might consider using <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>, if you definitely wish to work with Spark in the cloud. Go to <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark Databricks</em></span> and see how you can set up, and use <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>.</p><p>In the next section, I will examine Apache Spark cluster performance, and the issues that might impact it.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Performance</h2></div></div><hr /></div><p>Before moving on to the rest of the chapters covering functional areas of Apache Spark and extensions to it, I wanted to <a id="id59" class="indexterm"></a>examine the area of performance. What issues and areas need to be considered? What might impact Spark application performance starting at the cluster level, and finishing with actual Scala code? I don't want to just repeat what the Spark website says, so have a look at the following URL: <code class="literal">http://spark.apache.org/docs/&lt;version&gt;/tuning.html</code>.</p><p>Here, <code class="literal">&lt;version&gt;</code> relates<a id="id60" class="indexterm"></a> to the version of Spark that you are using, that is, latest, or 1.3.1 for a specific version. So, having looked at that page, I will briefly mention some of the topic areas. I am going to list some general points in this section without implying an order of importance.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec17"></a>The cluster structure</h3></div></div></div><p>The size and structure of your big data cluster is going to affect performance. If you have a cloud-based cluster, your IO and latency will suffer in comparison to an unshared hardware cluster. You will be sharing the underlying hardware with multiple customers, and that the cluster hardware maybe remote.</p><p>Also, the positioning <a id="id61" class="indexterm"></a>of cluster components on servers may cause resource contention. For instance, if possible, think carefully about locating Hadoop NameNodes, Spark servers, Zookeeper, Flume, and Kafka servers in large clusters. With high workloads, you might consider segregating servers to individual systems. You might also consider using an Apache system such as Mesos in order to share resources.</p><p>Also, consider potential parallelism. The greater the number of workers in your Spark cluster for large data sets, the greater the opportunity for parallelism.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec18"></a>The Hadoop file system</h3></div></div></div><p>You might consider <a id="id62" class="indexterm"></a>using an alternative to HDFS, depending upon your cluster requirements. For instance, MapR has the MapR-FS NFS-based read write<a id="id63" class="indexterm"></a> file system for improved performance. This file system has a full read write capability, whereas HDFS is designed as a write once, read many file system. It offers an improvement in performance over HDFS. It also integrates with Hadoop and the Spark <a id="id64" class="indexterm"></a>cluster tools. Bruce Penn, an architect at MapR, has written an interesting article describing its features at: <a class="ulink" href="https://www.mapr.com/blog/author/bruce-penn" target="_blank">https://www.mapr.com/blog/author/bruce-penn</a>.</p><p>Just look for the blog post entitled <code class="literal">Comparing MapR-FS and HDFS NFS and Snapshots</code>. The links in the article describe the MapR architecture, and possible performance gains.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec19"></a>Data locality</h3></div></div></div><p>Data locality or the location of the data being processed is going to affect latency and Spark processing. Is<a id="id65" class="indexterm"></a> the data sourced from AWS S3, HDFS, the local file system/network, or a remote source?</p><p>As the previous tuning link mentions, if the data is remote, then functionality and data must be brought together for processing. Spark will try to use the best data locality level possible for task processing.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec20"></a>Memory</h3></div></div></div><p>In order to avoid <span class="strong"><strong>OOM</strong></span> (<span class="strong"><strong>Out of Memory</strong></span>) messages for the tasks, on your Apache Spark cluster, you can<a id="id66" class="indexterm"></a> consider a number <a id="id67" class="indexterm"></a>of areas:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Consider the level of physical memory available on your Spark worker nodes. Can it be increased?</p></li><li style="list-style-type: disc"><p>Consider data partitioning. Can you increase the number of partitions in the data used by your Spark application code?</p></li><li style="list-style-type: disc"><p>Can you increase the storage fraction, the memory used by the JVM for storage and caching of RDD's?</p></li><li style="list-style-type: disc"><p>Consider tuning data structures used to reduce memory.</p></li><li style="list-style-type: disc"><p>Consider serializing your RDD storage to reduce the memory usage.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec21"></a>Coding</h3></div></div></div><p>Try to tune your<a id="id68" class="indexterm"></a> code to improve Spark application performance. For instance, filter your application-based data early in your ETL cycle. Tune your degree of parallelism, try to find the resource-expensive parts of your code, and find alternatives.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Cloud</h2></div></div><hr /></div><p>Although, most of this book will concentrate on examples of Apache Spark installed on physically server-based clusters (with the exception of <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>), I wanted to make the point that there are multiple cloud-based options out there. There are cloud-based systems that use Apache Spark as an integrated component, and cloud-based systems that offer Spark as a service. Even though this book cannot cover all of them in depth, I thought that it would be useful to mention some of them:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Databricks is<a id="id69" class="indexterm"></a> covered in two chapters in this book. It offers a Spark cloud-based service, currently using AWS EC2. There are plans to extend the service to other cloud suppliers (<a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>).</p></li><li style="list-style-type: disc"><p>At the time of writing (July 2015) this book, Microsoft Azure has been extended to offer Spark support.</p></li><li style="list-style-type: disc"><p>Apache Spark and Hadoop can be installed on Google Cloud.</p></li><li style="list-style-type: disc"><p>The Oryx system has<a id="id70" class="indexterm"></a> been built at the top of Spark and Kafka for real-time, large-scale machine learning (<a class="ulink" href="http://oryx.io/" target="_blank">http://oryx.io/</a>).</p></li><li style="list-style-type: disc"><p>The velox system for <a id="id71" class="indexterm"></a>serving machine learning prediction is based upon Spark and KeystoneML (<a class="ulink" href="https://github.com/amplab/velox-modelserver" target="_blank">https://github.com/amplab/velox-modelserver</a>).</p></li><li style="list-style-type: disc"><p>PredictionIO is an<a id="id72" class="indexterm"></a> open source machine learning service built on Spark, HBase, and Spray (<a class="ulink" href="https://prediction.io/" target="_blank">https://prediction.io/</a>).</p></li><li style="list-style-type: disc"><p>SeldonIO is an <a id="id73" class="indexterm"></a>open source predictive analytics platform, based upon Spark, Kafka, and Hadoop (<a class="ulink" href="http://www.seldon.io/" target="_blank">http://www.seldon.io/</a>).</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Summary</h2></div></div><hr /></div><p>In closing this chapter, I would invite you to work your way through each of the Scala code-based examples in the following chapters. I have been impressed by the rate at which Apache Spark has evolved, and I am also impressed at the frequency of the releases. So, even though at the time of writing, Spark has reached 1.4, I am sure that you will be using a later version. If you encounter problems, tackle them logically. Try approaching the Spark user group for assistance (<code class="email">&lt;<a class="email" href="mailto:user@spark.apache.org">user@spark.apache.org</a>&gt;</code>), or check the Spark website at <a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>.</p><p>I am always interested to hear from people, and connect with people on sites such as LinkedIn. I am keen to hear about the projects that people are involved with and new opportunities. I am interested to hear about Apache Spark, the ways that you use it and the systems that you build being used at scale. I can be contacted on LinkedIn at: <a class="ulink" href="http://linkedin.com/profile/view?id=73219349" target="_blank">linkedin.com/profile/view?id=73219349</a>.</p><p>Or, I can be contacted via my website at <a class="ulink" href="http://semtech-solutions.co.nz/" target="_blank">http://semtech-solutions.co.nz/</a>, or finally, by email at: <code class="email">&lt;<a class="email" href="mailto:info@semtech-solutions.co.nz">info@semtech-solutions.co.nz</a>&gt;</code>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. Apache Spark MLlib</h2></div></div></div><p>MLlib is the machine learning library that is provided with Apache Spark, the in memory cluster based open source data processing system. In this chapter, I will examine the functionality, provided within the <a id="id74" class="indexterm"></a>MLlib library in terms of areas such as regression, classification, and neural processing. I will examine the theory behind each algorithm before<a id="id75" class="indexterm"></a> providing working examples that tackle real problems. The example code and documentation on the web can be sparse and confusing. I will take a step-by-step approach in describing how the following algorithms can be used, and what they are capable of doing:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Classification with Naïve Bayes</p></li><li style="list-style-type: disc"><p>Clustering with K-Means</p></li><li style="list-style-type: disc"><p>Neural processing with ANN</p></li></ul></div><p>Having decided to learn about Apache Spark, I am assuming that you are familiar with Hadoop. Before I proceed, I will explain a little about my environment. My Hadoop cluster is installed on a set of Centos 6.5 Linux 64 bit servers. The following section will describe the architecture in detail.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec14"></a>The environment configuration</h2></div></div><hr /></div><p>Before<a id="id76" class="indexterm"></a> delving into the Apache Spark modules, I wanted to explain the structure and version of Hadoop and Spark clusters that I will use in this book. I will be using the Cloudera CDH 5.1.3 version of Hadoop for storage and I will be using two versions of Spark: 1.0 and 1.3 in this chapter.</p><p>The earlier version is compatible with Cloudera software, and has been tested and packaged by them. It is installed as a set of Linux services from the Cloudera repository using the yum command. Because I want to examine the Neural Net technology that has not been released yet, I will also download and run the development version of Spark 1.3 from GitHub. This will be explained later in the chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec22"></a>Architecture</h3></div></div></div><p>The following<a id="id77" class="indexterm"></a> diagram explains the structure of the small Hadoop cluster that I will use in this chapter:</p><div class="mediaobject"><img src="graphics/B01989_02_01.jpg" /></div><p>The previous diagram shows a five-node Hadoop cluster with a NameNode called <span class="strong"><strong>hc2nn</strong></span>, and DataNodes <span class="strong"><strong>hc2r1m1</strong></span> to <span class="strong"><strong>hc2r1m4</strong></span>. It also shows an Apache Spark cluster with a master node and four slave nodes. The Hadoop cluster provides the physical Centos 6 Linux machines while the Spark cluster runs on the same hosts. For instance, the Spark master server runs on the Hadoop Name Node machine <span class="strong"><strong>hc2nn</strong></span>, whereas the Spark <span class="strong"><strong>slave1</strong></span> worker runs on the host <span class="strong"><strong>hc2r1m1</strong></span>.</p><p>The Linux server naming standard used higher up should be explained. For instance the Hadoop NameNode server is called hc2nn. The <span class="strong"><strong>h</strong></span> in this server name means Hadoop, the <span class="strong"><strong>c</strong></span> means cluster, and the <span class="strong"><strong>nn</strong></span> means NameNode. So, hc2nn means Hadoop cluster 2 NameNode. Similarly, for the server hc2r1m1, the h means Hadoop the <span class="strong"><strong>c</strong></span> means cluster the <span class="strong"><strong>r</strong></span> means rack and the <span class="strong"><strong>m</strong></span> means machine. So, the name stands for Hadoop cluster 2 rack 1 machine 1. In a large Hadoop cluster, the machines will be organized into racks, so this naming standard means that the servers will be easy to locate.</p><p>You can arrange your Spark and Hadoop clusters as you see fit, they don't need to be on the same hosts. For the purpose of writing this book, I have limited machines available so it makes sense to co-locate the Hadoop and Spark clusters. You can use entirely separate machines for each cluster, as long as Spark is able to access Hadoop (if you want to use it for distributed storage).</p><p>Remember that although Spark is used for the speed of its in-memory distributed processing, it doesn't provide storage. You can use the Host file system to read and write your data, but if your data volumes are big enough to be described as big data, then it makes sense to use a distributed storage system like Hadoop.</p><p>Remember also that <a id="id78" class="indexterm"></a>Apache Spark may only be the processing step in your<a id="id79" class="indexterm"></a> <span class="strong"><strong>ETL</strong></span> (<span class="strong"><strong>Extract</strong></span>, <span class="strong"><strong>Transform</strong></span>, <span class="strong"><strong>Load</strong></span>) chain. It doesn't provide the rich tool set that the Hadoop ecosystem contains. You may still need Nutch/Gora/Solr for data acquisition; Sqoop and Flume for moving data; Oozie for scheduling; and HBase, or Hive for storage. The point that I am making is that although Apache Spark is a very powerful processing system, it should be considered a part of the wider Hadoop ecosystem.</p><p>Having described the environment that will be used in this chapter, I will move on to describe the functionality of the Apache Spark <span class="strong"><strong>MLlib</strong></span> (<span class="strong"><strong>Machine Learning library</strong></span>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec23"></a>The development environment</h3></div></div></div><p>The Scala language will be used for coding samples in this book. This is because as a scripting language, it produces less code than Java. It can also be used for the Spark shell, as well as compiled with Apache Spark applications. I will be using the sbt tool to compile the Scala code, which I have installed as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]# su -</strong></span>
<span class="strong"><strong>[root@hc2nn ~]# cd /tmp</strong></span>
<span class="strong"><strong>[root@hc2nn ~]#wget http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm</strong></span>
<span class="strong"><strong>[root@hc2nn ~]# rpm -ivh sbt.rpm</strong></span>
</pre></div><p>For convenience while <a id="id80" class="indexterm"></a>writing this book, I have used the generic Linux account called<a id="id81" class="indexterm"></a> <span class="strong"><strong>hadoop</strong></span> on the Hadoop NameNode server <code class="literal">hc2nn</code>. As the previous commands show that I need to install <code class="literal">sbt</code> as the root account, which I have accessed via <code class="literal">su</code> (switch user). I have then downloaded the <code class="literal">sbt.rpm</code> file, to the <code class="literal">/tmp</code> directory, from the web-based server called <code class="literal">repo.scala-sbt.org</code> using <code class="literal">wget</code>. Finally, I have installed the <code class="literal">rpm</code> file using the <code class="literal">rpm</code> command with the options <code class="literal">i</code> for install, <code class="literal">v</code> for verify, and <code class="literal">h</code> to print the hash marks while the package is being installed.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>You can download the example code files from your account at <a class="ulink" href="http://www.packtpub.com" target="_blank">http://www.packtpub.com</a> for all the Packt Publishing books you have purchased. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support" target="_blank">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div><p>I have developed all of the Scala code for Apache Spark, in this chapter, on the Linux server <code class="literal">hc2nn</code>, using the Linux hadoop account. I have placed each set of code within a sub directory under <code class="literal">/home/hadoop/spark</code>. For instance, the following sbt structure diagram shows that the MLlib Naïve Bayes code is stored within a subdirectory called <code class="literal">nbayes</code>, under the <code class="literal">spark</code> directory. What the diagram also shows is that the Scala code is developed within a subdirectory structure named <code class="literal">src/main/scala</code>, under the <code class="literal">nbayes</code> directory. The files called <code class="literal">bayes1.scala</code> and <code class="literal">convert.scala</code> contain the Naïve Bayes code that will be<a id="id82" class="indexterm"></a> used in the next section:</p><div class="mediaobject"><img src="graphics/B01989_02_02.jpg" /></div><p>The <code class="literal">bayes.sbt</code> file is a configuration file used by the sbt tool, which describes how to compile the Scala files within the <code class="literal">Scala</code> directory (also note that if you were developing in Java, you would use a path of the form <code class="literal">nbayes/src/main/java</code>). The contents of the <code class="literal">bayes.sbt</code> file are shown next. The <code class="literal">pwd</code> and <code class="literal">cat</code> Linux commands remind you of the file location, and they also remind you to dump the file contents.</p><p>The name, version, and <code class="literal">scalaVersion</code> options set the details of the project, and the version of Scala to be used. The <code class="literal">libraryDependencies</code> options define where the Hadoop and Spark libraries can be located. In this case, CDH5 has been installed using the Cloudera parcels, and the packages libraries can be located in the standard locations, that is, <code class="literal">/usr/lib/hadoop</code> for Hadoop and <code class="literal">/usr/lib/spark</code> for Spark. The resolver's option specifies the location for the Cloudera repository for other dependencies:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/nbayes</strong></span>
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ cat bayes.sbt</strong></span>

<span class="strong"><strong>name := "Naive Bayes"</strong></span>

<span class="strong"><strong>version := "1.0"</strong></span>

<span class="strong"><strong>scalaVersion := "2.10.4"</strong></span>


<span class="strong"><strong>libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.spark" %% "spark-core"  % "1.0.0"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.spark" %% "spark-mllib" % "1.0.0"</strong></span>

<span class="strong"><strong>// If using CDH, also add Cloudera repo</strong></span>
<span class="strong"><strong>resolvers += "Cloudera Repository" at https://repository.cloudera.com/artifactory/cloudera-repos/</strong></span>
</pre></div><p>The Scala nbayes<a id="id83" class="indexterm"></a> project code can be compiled from the <code class="literal">nbayes</code> sub directory using this command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ sbt compile</strong></span>
</pre></div><p>The <code class="literal">sbt compile</code> command is used to compile the code into classes. The classes are then placed in the <code class="literal">nbayes/target/scala-2.10/classes</code> directory. The compiled classes can be packaged into a JAR file with this command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ sbt package</strong></span>
</pre></div><p>The <code class="literal">sbt package</code> command will create a JAR file under the directory <code class="literal">nbayes/target/scala-2.10</code>. As the example in the <span class="emphasis"><em>sbt structure diagram</em></span> shows the JAR file named <code class="literal">naive-bayes_2.10-1.0.jar</code> has been created after a successful compile and package. This JAR file, and the classes that it contains, can then be used in a <code class="literal">spark-submit</code> command. This will be described later as the functionality in the Apache Spark MLlib module is explored.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec24"></a>Installing Spark</h3></div></div></div><p>Finally, when<a id="id84" class="indexterm"></a> describing the environment used for this book, I wanted to touch on the approach to installing and running Apache Spark. I won't elaborate on the Hadoop CDH5 install, except to say that I installed it using the Cloudera parcels. However, I manually installed version 1.0 of Apache Spark from the Cloudera repository, using the Linux <code class="literal">yum</code> commands. I installed the service-based packages, because I wanted the flexibility that would enable me to install multiple versions of Spark as services from Cloudera, as I needed.</p><p>When preparing a CDH Hadoop release, Cloudera takes the code that has been developed by the Apache Spark team, and the code released by the Apache Bigtop project. They perform an integration test so that it is guaranteed to work as a code stack. They also reorganize the code and binaries into services and parcels. This means that libraries, logs, and binaries can be located in defined locations under Linux, that is, <code class="literal">/var/log/spark</code>, <code class="literal">/usr/lib/spark</code>. It also means that, in the case of services, the components can be installed using the Linux <code class="literal">yum</code> command, and managed via the Linux <code class="literal">service</code> command.</p><p>Although, in the case of the Neural Network code described later in this chapter, a different approach was used. This is how Apache Spark 1.0 was installed for use with Hadoop CDH5:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# cd /etc/yum.repos.d</strong></span>
<span class="strong"><strong>[root@hc2nn yum.repos.d]# cat  cloudera-cdh5.repo</strong></span>

<span class="strong"><strong>[cloudera-cdh5]</strong></span>
<span class="strong"><strong># Packages for Cloudera's Distribution for Hadoop, Version 5, on RedHat or CentOS 6 x86_64</strong></span>
<span class="strong"><strong>name=Cloudera's Distribution for Hadoop, Version 5</strong></span>
<span class="strong"><strong>baseurl=http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5/</strong></span>
<span class="strong"><strong>gpgkey = http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera</strong></span>
<span class="strong"><strong>gpgcheck = 1</strong></span>
</pre></div><p>The first step is to ensure that a Cloudera repository file exists under the <code class="literal">/etc/yum.repos.d</code> directory, on the server <code class="literal">hc2nn</code> and all of the other Hadoop cluster servers. The file is called <code class="literal">cloudera-cdh5.repo</code>, and specifies where the yum command can locate software for the Hadoop CDH5 cluster. On all the Hadoop cluster nodes, I use the Linux yum command, as root, to install the Apache Spark components core, master, worker, history-server, and python:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# yum install spark-core spark-master spark-worker spark-history-server spark-python</strong></span>
</pre></div><p>This gives me the flexibility to configure Spark in any way that I want in the future. Note that I have installed the master component on all the nodes, even though I only plan to use it from the Name Node at this time. Now, the Spark install needs to be configured on all the nodes. The configuration files are stored under <code class="literal">/etc/spark/conf</code>. The first thing to do, will be to set <a id="id85" class="indexterm"></a>up a <code class="literal">slaves</code> file, which specifies on which hosts Spark will run it's worker components:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# cd /etc/spark/conf</strong></span>

<span class="strong"><strong>[root@hc2nn conf]# cat slaves</strong></span>
<span class="strong"><strong># A Spark Worker will be started on each of the machines listed below.</strong></span>
<span class="strong"><strong>hc2r1m1</strong></span>
<span class="strong"><strong>hc2r1m2</strong></span>
<span class="strong"><strong>hc2r1m3</strong></span>
<span class="strong"><strong>hc2r1m4</strong></span>
</pre></div><p>As you can see from the contents of the <code class="literal">slaves</code> file above Spark, it will run four workers on the Hadoop CDH5 cluster, Data Nodes, from <code class="literal">hc2r1m1</code> to <code class="literal">hc2r1m4</code>. Next, it will alter the contents of the <code class="literal">spark-env.sh</code> file to specify the Spark environment options. The <code class="literal">SPARK_MASTER_IP</code> values are defined as the full server name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export STANDALONE_SPARK_MASTER_HOST=hc2nn.semtech-solutions.co.nz</strong></span>
<span class="strong"><strong>export SPARK_MASTER_IP=$STANDALONE_SPARK_MASTER_HOST</strong></span>

<span class="strong"><strong>export SPARK_MASTER_WEBUI_PORT=18080</strong></span>
<span class="strong"><strong>export SPARK_MASTER_PORT=7077</strong></span>
<span class="strong"><strong>export SPARK_WORKER_PORT=7078</strong></span>
<span class="strong"><strong>export SPARK_WORKER_WEBUI_PORT=18081</strong></span>
</pre></div><p>The web user interface port numbers are specified for the master and worker processes, as well as the operational port numbers. The Spark service can then be started as root from the Name Node server. I use the following script:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo "hc2r1m1 - start worker"</strong></span>
<span class="strong"><strong>ssh   hc2r1m1 'service spark-worker start'</strong></span>

<span class="strong"><strong>echo "hc2r1m2 - start worker"</strong></span>
<span class="strong"><strong>ssh   hc2r1m2 'service spark-worker start'</strong></span>

<span class="strong"><strong>echo "hc2r1m3 - start worker"</strong></span>
<span class="strong"><strong>ssh   hc2r1m3 'service spark-worker start'</strong></span>

<span class="strong"><strong>echo "hc2r1m4 - start worker"</strong></span>
<span class="strong"><strong>ssh   hc2r1m4 'service spark-worker start'</strong></span>


<span class="strong"><strong>echo "hc2nn - start master server"</strong></span>
<span class="strong"><strong>service spark-master         start</strong></span>
<span class="strong"><strong>service spark-history-server start</strong></span>
</pre></div><p>This starts the Spark worker service on all of the slaves, and the master and history server on the Name Node <code class="literal">hc2nn</code>. So now, the Spark user interface can be accessed using the <code class="literal">http://hc2nn:18080</code> URL.</p><p>The following figure<a id="id86" class="indexterm"></a> shows an example of the Spark 1.0 master web user interface. It shows details about the Spark install, the workers, and the applications that are running or completed. The statuses of the master and workers are given. In this case, all are alive. Memory used and availability is given in total and by worker. Although, there are no applications running at the moment, each worker link can be selected to view the executor processes' running on each worker node, as the work volume for each application run is spread across the spark cluster.</p><p>Note also the Spark URL, <code class="literal">spark://hc2nn.semtech-solutions.co.nz:7077</code>, will be used when running the Spark applications like <code class="literal">spark-shell</code> and <code class="literal">spark-submit</code>. Using this URL, it is possible to ensure that the shell or application is run against this Spark cluster.</p><div class="mediaobject"><img src="graphics/B01989_02_03.jpg" /></div><p>This gives a quick overview of the Apache Spark installation using services, its configuration, how to start it, and <a id="id87" class="indexterm"></a>how to monitor it. Now, it is time to tackle the first of the MLlib functional areas, which is classification using the Naïve Bayes algorithm. The use of Spark will become clearer as Scala scripts are developed, and the resulting applications are monitored.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec15"></a>Classification with Naïve Bayes</h2></div></div><hr /></div><p>This section will provide a <a id="id88" class="indexterm"></a>working example of the Apache Spark<a id="id89" class="indexterm"></a> MLlib Naïve Bayes algorithm. It will describe the theory behind the algorithm, and will provide a step-by-step example in Scala to show how the algorithm may be used.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec25"></a>Theory</h3></div></div></div><p>In order to use the Naïve Bayes algorithm to classify a data set, the data must be linearly divisible, that is, the<a id="id90" class="indexterm"></a> classes within the data must be linearly divisible by class boundaries. The following figure visually explains this with three data sets, and two class boundaries shown via the dotted lines:</p><div class="mediaobject"><img src="graphics/B01989_02_04.jpg" /></div><p>Naïve Bayes assumes that the features (or dimensions) within a data set are independent of one another, that is, they have no effect on each other. An example for Naïve Bayes is supplied with the help of <a id="id91" class="indexterm"></a>Hernan Amiune at <a class="ulink" href="http://hernan.amiune.com/" target="_blank">http://hernan.amiune.com/</a>. The following example considers the classification of emails as spam. If you have 100 e-mails then perform the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>60% of emails are spam</strong></span>
<span class="strong"><strong>  80% of spam emails contain the word buy</strong></span>
<span class="strong"><strong>  20% of spam emails don't contain the word buy</strong></span>
<span class="strong"><strong>40% of emails are not spam</strong></span>
<span class="strong"><strong>  10% of non spam emails contain the word buy</strong></span>
<span class="strong"><strong>  90% of non spam emails don't contain the word buy</strong></span>
</pre></div><p>Thus, convert this example into probabilities, so that a Naïve Bayes equation can be created.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>P(Spam) = the probability that an email is spam = 0.6</strong></span>
<span class="strong"><strong>P(Not Spam) = the probability that an email is not spam = 0.4</strong></span>
<span class="strong"><strong>P(Buy|Spam) = the probability that an email that is spam has the word buy = 0.8</strong></span>
<span class="strong"><strong>P(Buy|Not Spam) = the probability that an email that is not spam has the word buy = 0.1</strong></span>
</pre></div><p>So, what is the<a id="id92" class="indexterm"></a> probability that an e-mail that contains the word buy is spam? Well, this would be written<a id="id93" class="indexterm"></a> as <span class="strong"><strong>P (Spam|Buy)</strong></span>. Naïve Bayes says that it is described by the equation in the following figure:</p><div class="mediaobject"><img src="graphics/B01989_02_05.jpg" /></div><p>So, using the previous percentage figures, we get the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>P(Spam|Buy) = ( 0.8 * 0.6 ) / (( 0.8 * 0.6 )  + ( 0.1 * 0.4 )  )  = ( .48 ) / ( .48 + .04 )</strong></span>
<span class="strong"><strong>= .48 / .52 = .923</strong></span>
</pre></div><p>This means that it is 92 percent more likely that an e-mail that contains the word buy is spam. That was a look at the theory; now, it's time to try a real world example using the Apache Spark MLlib Naïve Bayes algorithm.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec26"></a>Naïve Bayes in practice</h3></div></div></div><p>The first step is to choose <a id="id94" class="indexterm"></a>some data that will be used for classification. I have chosen some <a id="id95" class="indexterm"></a>data from the UK government data <a id="id96" class="indexterm"></a>web site, available at: <a class="ulink" href="http://data.gov.uk/dataset/road-accidents-safety-data" target="_blank">http://data.gov.uk/dataset/road-accidents-safety-data</a>.</p><p>The data set is called "Road Safety - Digital Breath Test Data 2013," which downloads a zipped text file called <code class="literal">DigitalBreathTestData2013.txt</code>. This file contains around half a million rows. The data looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Reason,Month,Year,WeekType,TimeBand,BreathAlcohol,AgeBand,Gender</strong></span>
<span class="strong"><strong>Suspicion of Alcohol,Jan,2013,Weekday,12am-4am,75,30-39,Male</strong></span>
<span class="strong"><strong>Moving Traffic Violation,Jan,2013,Weekday,12am-4am,0,20-24,Male</strong></span>
<span class="strong"><strong>Road Traffic Collision,Jan,2013,Weekend,12pm-4pm,0,20-24,Female</strong></span>
</pre></div><p>In order to classify the data, I have modified both the column layout, and the number of columns. I have<a id="id97" class="indexterm"></a> simply used Excel to give the data volume. However, if my data size had been in the big data range, I would have had to use Scala, or perhaps a tool like Apache Pig. As the following commands show, the data now resides on HDFS, in the directory named <code class="literal">/data/spark/nbayes</code>. The file name is called <code class="literal">DigitalBreathTestData2013- MALE2.csv</code>. Also, the line count from the Linux <code class="literal">wc</code> command shows that there are 467,000 rows. Finally, the following data sample shows that I have selected the columns: Gender, Reason, WeekType, TimeBand, BreathAlcohol, and AgeBand to classify. I will try and classify on the Gender column using the other columns as features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ hdfs dfs -cat /data/spark/nbayes/DigitalBreathTestData2013-MALE2.csv | wc -l</strong></span>
<span class="strong"><strong>467054</strong></span>

<span class="strong"><strong>[hadoop@hc2nn ~]$ hdfs dfs -cat /data/spark/nbayes/DigitalBreathTestData2013-MALE2.csv | head -5</strong></span>
<span class="strong"><strong>Male,Suspicion of Alcohol,Weekday,12am-4am,75,30-39</strong></span>
<span class="strong"><strong>Male,Moving Traffic Violation,Weekday,12am-4am,0,20-24</strong></span>
<span class="strong"><strong>Male,Suspicion of Alcohol,Weekend,4am-8am,12,40-49</strong></span>
<span class="strong"><strong>Male,Suspicion of Alcohol,Weekday,12am-4am,0,50-59</strong></span>
<span class="strong"><strong>Female,Road Traffic Collision,Weekend,12pm-4pm,0,20-24</strong></span>
</pre></div><p>The Apache Spark MLlib classification functions use a data structure called <code class="literal">LabeledPoint</code>, which is a<a id="id98" class="indexterm"></a> general purpose data representation defined at: <a class="ulink" href="http://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint" target="_blank">http://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint</a>.</p><p>This structure only accepts Double values, which means the text values in the previous data need to be classified numerically. Luckily, all of the columns in the data will convert to numeric categories, and I have provided two programs in the software package with this book, under the directory <code class="literal">chapter2\naive bayes</code> to do just that. The first is called <code class="literal">convTestData.pl</code>, and is a Perl script to convert the previous text file into Linux. The second file, which will be examined here is called <code class="literal">convert.scala</code>. It takes the contents of the <code class="literal">DigitalBreathTestData2013- MALE2.csv</code> file and converts each record into a Double vector.</p><p>The directory structure and files for an sbt Scala-based development environment have already been described earlier. I am developing my Scala code on the Linux server <code class="literal">hc2nn</code> using the Linux account hadoop. Next, the Linux <code class="literal">pwd</code> and <code class="literal">ls</code> commands show my top level <code class="literal">nbayes</code> <a id="id99" class="indexterm"></a>development directory with the <code class="literal">bayes.sbt</code> configuration file, whose contents have already been examined:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/nbayes</strong></span>
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ ls</strong></span>
<span class="strong"><strong>bayes.sbt     target   project   src</strong></span>
</pre></div><p>The Scala code to run the Naïve Bayes example is shown next, in the <code class="literal">src/main/scala</code> subdirectory, under the <code class="literal">nbayes</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn scala]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/nbayes/src/main/scala</strong></span>
<span class="strong"><strong>[hadoop@hc2nn scala]$ ls</strong></span>
<span class="strong"><strong>bayes1.scala  convert.scala</strong></span>
</pre></div><p>We will examine the <code class="literal">bayes1.scala</code> file later, but first, the text-based data on HDFS must be converted into the numeric Double values. This is where the <code class="literal">convert.scala</code> file is used. The code looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import org.apache.spark.SparkContext</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkConf</strong></span>
</pre></div><p>These lines import classes for Spark context, the connection to the Apache Spark cluster, and the Spark configuration. The object that is being created is called <code class="literal">convert1</code>. It is an application, as it extends the class <code class="literal">App</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>object convert1 extends App</strong></span>
<span class="strong"><strong>{</strong></span>
</pre></div><p>The next line creates a function called <code class="literal">enumerateCsvRecord</code>. It has a parameter called <code class="literal">colData</code>, which is an array of strings, and returns a string:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def enumerateCsvRecord( colData:Array[String]): String =</strong></span>
<span class="strong"><strong>{</strong></span>
</pre></div><p>The function then enumerates the text values in each column, so for an instance, <code class="literal">Male</code> becomes <code class="literal">0</code>. These numeric values are stored in values like <code class="literal">colVal1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val colVal1 =</strong></span>
<span class="strong"><strong>      colData(0) match</strong></span>
<span class="strong"><strong>      {</strong></span>
<span class="strong"><strong>        case "Male"                          =&gt; 0</strong></span>
<span class="strong"><strong>        case "Female"                        =&gt; 1</strong></span>
<span class="strong"><strong>        case "Unknown"                       =&gt; 2</strong></span>
<span class="strong"><strong>        case _                               =&gt; 99</strong></span>
<span class="strong"><strong>      }</strong></span>

<span class="strong"><strong>    val colVal2 =</strong></span>
<span class="strong"><strong>      colData(1) match</strong></span>
<span class="strong"><strong>      {</strong></span>
<span class="strong"><strong>        case "Moving Traffic Violation"      =&gt; 0</strong></span>
<span class="strong"><strong>        case "Other"                         =&gt; 1</strong></span>
<span class="strong"><strong>        case "Road Traffic Collision"        =&gt; 2</strong></span>
<span class="strong"><strong>        case "Suspicion of Alcohol"          =&gt; 3</strong></span>
<span class="strong"><strong>        case _                               =&gt; 99</strong></span>
<span class="strong"><strong>      }</strong></span>

<span class="strong"><strong>    val colVal3 =</strong></span>
<span class="strong"><strong>      colData(2) match</strong></span>
<span class="strong"><strong>      {</strong></span>
<span class="strong"><strong>        case "Weekday"                       =&gt; 0</strong></span>
<span class="strong"><strong>        case "Weekend"                       =&gt; 0</strong></span>
<span class="strong"><strong>        case _                               =&gt; 99</strong></span>
<span class="strong"><strong>      }</strong></span>

<span class="strong"><strong>    val colVal4 =</strong></span>
<span class="strong"><strong>      colData(3) match</strong></span>
<span class="strong"><strong>      {</strong></span>
<span class="strong"><strong>        case "12am-4am"                      =&gt; 0</strong></span>
<span class="strong"><strong>        case "4am-8am"                       =&gt; 1</strong></span>
<span class="strong"><strong>        case "8am-12pm"                      =&gt; 2</strong></span>
<span class="strong"><strong>        case "12pm-4pm"                      =&gt; 3</strong></span>
<span class="strong"><strong>        case "4pm-8pm"                       =&gt; 4</strong></span>
<span class="strong"><strong>        case "8pm-12pm"                      =&gt; 5</strong></span>
<span class="strong"><strong>        case _                               =&gt; 99</strong></span>
<span class="strong"><strong>      }</strong></span>

<span class="strong"><strong>    val colVal5 = colData(4)</strong></span>

<span class="strong"><strong>    val colVal6 =</strong></span>
<span class="strong"><strong>      colData(5) match</strong></span>
<span class="strong"><strong>      {</strong></span>
<span class="strong"><strong>        case "16-19"                         =&gt; 0</strong></span>
<span class="strong"><strong>        case "20-24"                         =&gt; 1</strong></span>
<span class="strong"><strong>        case "25-29"                         =&gt; 2</strong></span>
<span class="strong"><strong>        case "30-39"                         =&gt; 3</strong></span>
<span class="strong"><strong>        case "40-49"                         =&gt; 4</strong></span>
<span class="strong"><strong>        case "50-59"                         =&gt; 5</strong></span>
<span class="strong"><strong>        case "60-69"                         =&gt; 6</strong></span>
<span class="strong"><strong>        case "70-98"                         =&gt; 7</strong></span>
<span class="strong"><strong>        case "Other"                         =&gt; 8</strong></span>
<span class="strong"><strong>        case _                               =&gt; 99</strong></span>
<span class="strong"><strong>      }</strong></span>
</pre></div><p>A comma separated string called <code class="literal">lineString</code> is created from the numeric column values, and is then returned. The function closes with the final brace character<code class="literal">}</code>. Note that the data line created next starts with a label value at column one, and is followed by a vector, which<a id="id100" class="indexterm"></a> represents the data. The vector is space separated while the label is separated from the vector by a comma. Using these two separator types allows me to process both: the label and the vector in two simple steps later:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val lineString = colVal1+","+colVal2+" "+colVal3+" "+colVal4+" "+colVal5+" "+colVal6</strong></span>

<span class="strong"><strong>    return lineString</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>The main script defines the HDFS server name and path. It defines the input file, and the output path in terms of these values. It uses the Spark URL and application name to create a new configuration. It then creates a new context or connection to Spark using these details:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val hdfsServer = "hdfs://hc2nn.semtech-solutions.co.nz:8020"</strong></span>
<span class="strong"><strong>val hdfsPath   = "/data/spark/nbayes/"</strong></span>
<span class="strong"><strong>val inDataFile  = hdfsServer + hdfsPath + "DigitalBreathTestData2013-MALE2.csv"</strong></span>
<span class="strong"><strong>val outDataFile = hdfsServer + hdfsPath + "result"</strong></span>

<span class="strong"><strong>val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"</strong></span>
<span class="strong"><strong>val appName = "Convert 1"</strong></span>
<span class="strong"><strong>val sparkConf = new SparkConf()</strong></span>

<span class="strong"><strong>sparkConf.setMaster(sparkMaster)</strong></span>
<span class="strong"><strong>sparkConf.setAppName(appName)</strong></span>

<span class="strong"><strong>val sparkCxt = new SparkContext(sparkConf)</strong></span>
</pre></div><p>The CSV-based<a id="id101" class="indexterm"></a> raw data file is loaded from HDFS using the Spark context <code class="literal">textFile</code> method. Then, a data row count is printed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val csvData = sparkCxt.textFile(inDataFile)</strong></span>
<span class="strong"><strong>println("Records in  : "+ csvData.count() )</strong></span>
</pre></div><p>The CSV raw data is passed line by line to the <code class="literal">enumerateCsvRecord</code> function. The returned string-based numeric data is stored in the <code class="literal">enumRddData</code> variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val enumRddData = csvData.map</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    csvLine =&gt;</strong></span>
<span class="strong"><strong>      val colData = csvLine.split(',')</strong></span>

<span class="strong"><strong>      enumerateCsvRecord(colData)</strong></span>

<span class="strong"><strong>  }</strong></span>
</pre></div><p>Finally, the number of records in the <code class="literal">enumRddData</code> variable is printed, and the enumerated data is saved to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  println("Records out : "+ enumRddData.count() )</strong></span>

<span class="strong"><strong>  enumRddData.saveAsTextFile(outDataFile)</strong></span>

<span class="strong"><strong>} // end object</strong></span>
</pre></div><p>In order to run this script as an application against Spark, it must be compiled. This is carried out with the <code class="literal">sbt</code> package command, which also compiles the code. The following command was run from the <code class="literal">nbayes</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ sbt package</strong></span>
<span class="strong"><strong>Loading /usr/share/sbt/bin/sbt-launch-lib.bash</strong></span>
<span class="strong"><strong>....</strong></span>
<span class="strong"><strong>[info] Done packaging.</strong></span>
<span class="strong"><strong>[success] Total time: 37 s, completed Feb 19, 2015 1:23:55 PM</strong></span>
</pre></div><p>This causes the compiled classes that are created to be packaged into a JAR library, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/nbayes</strong></span>
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ ls -l target/scala-2.10</strong></span>
<span class="strong"><strong>total 24</strong></span>
<span class="strong"><strong>drwxrwxr-x 2 hadoop hadoop  4096 Feb 19 13:23 classes</strong></span>
<span class="strong"><strong>-rw-rw-r-- 1 hadoop hadoop 17609 Feb 19 13:23 naive-bayes_2.10-1.0.jar</strong></span>
</pre></div><p>The application <a id="id102" class="indexterm"></a>
<code class="literal">convert1</code> can now be run against Spark using the application name, the Spark URL, and the full path to the JAR file that was created. Some extra parameters specify memory and maximum cores that are supposed to be used:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>spark-submit \</strong></span>
<span class="strong"><strong>  --class convert1 \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 700M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 100 \</strong></span>
<span class="strong"><strong>  /home/hadoop/spark/nbayes/target/scala-2.10/naive-bayes_2.10-1.0.jar</strong></span>
</pre></div><p>This creates a data directory on HDFS called the <code class="literal">/data/spark/nbayes/</code> followed by the result, which contains part files, containing the processed data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$  hdfs dfs -ls /data/spark/nbayes</strong></span>
<span class="strong"><strong>Found 2 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup   24645166 2015-01-29 21:27 /data/spark/nbayes/DigitalBreathTestData2013-MALE2.csv</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hadoop supergroup          0 2015-02-19 13:36 /data/spark/nbayes/result</strong></span>

<span class="strong"><strong>[hadoop@hc2nn nbayes]$ hdfs dfs -ls /data/spark/nbayes/result</strong></span>
<span class="strong"><strong>Found 3 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup          0 2015-02-19 13:36 /data/spark/nbayes/result/_SUCCESS</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup    2828727 2015-02-19 13:36 /data/spark/nbayes/result/part-00000</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup    2865499 2015-02-19 13:36 /data/spark/nbayes/result/part-00001</strong></span>
</pre></div><p>In the following HDFS <code class="literal">cat</code> command, I have concatenated the part file data into a file called <code class="literal">DigitalBreathTestData2013-MALE2a.csv</code>. I have then examined the top five lines of the file using the <code class="literal">head</code> command to show that it is numeric. Finally, I have loaded it into HDFS with<a id="id103" class="indexterm"></a> the <code class="literal">put</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ hdfs dfs -cat /data/spark/nbayes/result/part* &gt; ./DigitalBreathTestData2013-MALE2a.csv</strong></span>

<span class="strong"><strong>[hadoop@hc2nn nbayes]$ head -5 DigitalBreathTestData2013-MALE2a.csv</strong></span>
<span class="strong"><strong>0,3 0 0 75 3</strong></span>
<span class="strong"><strong>0,0 0 0 0 1</strong></span>
<span class="strong"><strong>0,3 0 1 12 4</strong></span>
<span class="strong"><strong>0,3 0 0 0 5</strong></span>
<span class="strong"><strong>1,2 0 3 0 1</strong></span>

<span class="strong"><strong>[hadoop@hc2nn nbayes]$ hdfs dfs -put ./DigitalBreathTestData2013-MALE2a.csv /data/spark/nbayes</strong></span>
</pre></div><p>The following HDFS <code class="literal">ls</code> command now shows the numeric data file stored on HDFS, in the <code class="literal">nbayes</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ hdfs dfs -ls /data/spark/nbayes</strong></span>
<span class="strong"><strong>Found 3 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup   24645166 2015-01-29 21:27 /data/spark/nbayes/DigitalBreathTestData2013-MALE2.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup    5694226 2015-02-19 13:39 /data/spark/nbayes/DigitalBreathTestData2013-MALE2a.csv</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hadoop supergroup          0 2015-02-19 13:36 /data/spark/nbayes/result</strong></span>
</pre></div><p>Now that the data has been converted into a numeric form, it can be processed with the MLlib Naïve Bayes algorithm; this is what the Scala file <code class="literal">bayes1.scala</code> does. This file imports the same configuration and context classes as before. It also imports MLlib classes for Naïve Bayes, vectors, and the LabeledPoint structure. The application class that is created this time is called <code class="literal">bayes1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import org.apache.spark.SparkContext</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkConf</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.classification.NaiveBayes</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.regression.LabeledPoint</strong></span>

<span class="strong"><strong>object bayes1 extends App</strong></span>
<span class="strong"><strong>{</strong></span>
</pre></div><p>Again, the HDFS data file is defined, and a Spark context is created as before:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val hdfsServer = "hdfs://hc2nn.semtech-solutions.co.nz:8020"</strong></span>
<span class="strong"><strong>  val hdfsPath   = "/data/spark/nbayes/"</strong></span>

<span class="strong"><strong>  val dataFile = hdfsServer+hdfsPath+"DigitalBreathTestData2013-MALE2a.csv"</strong></span>

<span class="strong"><strong>  val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"</strong></span>
<span class="strong"><strong>  val appName = "Naive Bayes 1"</strong></span>
<span class="strong"><strong>  val conf = new SparkConf()</strong></span>
<span class="strong"><strong>  conf.setMaster(sparkMaster)</strong></span>
<span class="strong"><strong>  conf.setAppName(appName)</strong></span>

<span class="strong"><strong>  val sparkCxt = new SparkContext(conf)</strong></span>
</pre></div><p>The raw CSV data is<a id="id104" class="indexterm"></a> loaded and split by the separator characters. The first column becomes the label (<code class="literal">Male/Female</code>) that the data will be classified upon. The final columns separated by spaces become the classification features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val csvData = sparkCxt.textFile(dataFile)</strong></span>

<span class="strong"><strong>  val ArrayData = csvData.map</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    csvLine =&gt;</strong></span>
<span class="strong"><strong>      val colData = csvLine.split(',')</strong></span>
<span class="strong"><strong>      LabeledPoint(colData(0).toDouble, Vectors.dense(colData(1).split(' ').map(_.toDouble)))</strong></span>
<span class="strong"><strong>  }</strong></span>
</pre></div><p>The data is then randomly divided into training (70%) and testing (30%) data sets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val divData = ArrayData.randomSplit(Array(0.7, 0.3), seed = 13L)</strong></span>

<span class="strong"><strong>  val trainDataSet = divData(0)</strong></span>
<span class="strong"><strong>  val testDataSet  = divData(1)</strong></span>
</pre></div><p>The Naïve Bayes MLlib function can now be trained using the previous training set. The trained Naïve Bayes model, held in the variable <code class="literal">nbTrained</code>, can then be used to predict the <code class="literal">Male/Female</code> result labels against the testing data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val nbTrained = NaiveBayes.train(trainDataSet)</strong></span>
<span class="strong"><strong>  val nbPredict = nbTrained.predict(testDataSet.map(_.features))</strong></span>
</pre></div><p>Given that all of the data already contained labels, the original and predicted labels for the test data can be compared. An accuracy figure can then be computed to determine how accurate<a id="id105" class="indexterm"></a> the predictions were, by comparing the original labels with the prediction values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val predictionAndLabel = nbPredict.zip(testDataSet.map(_.label))</strong></span>
<span class="strong"><strong>  val accuracy = 100.0 * predictionAndLabel.filter(x =&gt; x._1 == x._2).count() / testDataSet.count()</strong></span>
<span class="strong"><strong>  println( "Accuracy : " + accuracy );</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>So this explains the Scala Naïve Bayes code example. It's now time to run the compiled <code class="literal">bayes1</code> application using <code class="literal">spark-submit</code>, and to determine the classification accuracy. The parameters are the same. It's just the class name that has changed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>spark-submit \</strong></span>
<span class="strong"><strong>  --class bayes1 \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 700M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 100 \</strong></span>
<span class="strong"><strong>  /home/hadoop/spark/nbayes/target/scala-2.10/naive-bayes_2.10-1.0.jar</strong></span>
</pre></div><p>The resulting accuracy given by the Spark cluster is just <code class="literal">43</code> percent, which seems to imply that this data is not suitable for Naïve Bayes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Accuracy: 43.30</strong></span>
</pre></div><p>In the next example, I will use K-Means to try and determine what clusters exist within the data. Remember, Naïve Bayes needs the data classes to be linearly divisible along the class boundaries. With K-Means, it will be possible to determine both: the membership and centroid location of the clusters within the data.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec16"></a>Clustering with K-Means</h2></div></div><hr /></div><p>This example <a id="id106" class="indexterm"></a>will use the same test data from the previous example, but will attempt to<a id="id107" class="indexterm"></a> find clusters in the data using the MLlib K-Means algorithm.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec27"></a>Theory</h3></div></div></div><p>The K-Means algorithm iteratively attempts to determine clusters within the test data by minimizing the distance <a id="id108" class="indexterm"></a>between the mean value of cluster center vectors, and the new candidate cluster member vectors. The following equation assumes data set members that range from <span class="strong"><strong>X1</strong></span> to <span class="strong"><strong>Xn</strong></span>; it also assumes <span class="strong"><strong>K</strong></span> cluster sets that range from <span class="strong"><strong>S1</strong></span> to <span class="strong"><strong>Sk</strong></span> where <span class="strong"><strong>K &lt;= n</strong></span>.</p><div class="mediaobject"><img src="graphics/B01989_02_06.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec28"></a>K-Means in practice</h3></div></div></div><p>Again, the K-Means MLlib functionality uses the LabeledPoint structure to process its data and so, it needs<a id="id109" class="indexterm"></a> numeric input data. As the same data from the last section is being reused, I will not re-explain the data conversion. The only change that has been made in data terms, in this section, is that processing under HDFS will now take place under the <code class="literal">/data/spark/kmeans/</code> directory<span class="strong"><strong>.</strong></span> Also, the conversion Scala script for the K-Means example produces a record that is all comma separated.</p><p>The development and processing for the K-Means example has taken place under the <code class="literal">/home/hadoop/spark/kmeans</code> directory, to separate the work from other development. The sbt configuration file is now called <code class="literal">kmeans.sbt</code>, and is identical to the last example, except for the project name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>name := "K-Means"</strong></span>
</pre></div><p>The code for this section can be found in the software package under <code class="literal">chapter2\K-Means</code>. So, looking at the code for <code class="literal">kmeans1.scala</code>, which is stored under <code class="literal">kmeans/src/main/scala</code>, some similar actions occur. The import statements refer to Spark context and configuration. This time, however, the K-Means functionality is also being imported from MLlib. Also, the application class name has been changed for this example to <code class="literal">kmeans1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import org.apache.spark.SparkContext</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkConf</strong></span>

<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.clustering.{KMeans,KMeansModel}</strong></span>

<span class="strong"><strong>object kmeans1 extends App</strong></span>
<span class="strong"><strong>{</strong></span>
</pre></div><p>The same actions <a id="id110" class="indexterm"></a>are being taken as the last example to define the data file—define the Spark configuration and create a Spark context:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val hdfsServer = "hdfs://hc2nn.semtech-solutions.co.nz:8020"</strong></span>
<span class="strong"><strong>  val hdfsPath   = "/data/spark/kmeans/"</strong></span>

<span class="strong"><strong>  val dataFile   = hdfsServer + hdfsPath + "DigitalBreathTestData2013-MALE2a.csv"</strong></span>

<span class="strong"><strong>  val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"</strong></span>
<span class="strong"><strong>  val appName = "K-Means 1"</strong></span>
<span class="strong"><strong>  val conf = new SparkConf()</strong></span>

<span class="strong"><strong>  conf.setMaster(sparkMaster)</strong></span>
<span class="strong"><strong>  conf.setAppName(appName)</strong></span>

<span class="strong"><strong>  val sparkCxt = new SparkContext(conf)</strong></span>
</pre></div><p>Next, the CSV data is loaded from the data file, and is split by comma characters into the variable <code class="literal">VectorData</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val csvData = sparkCxt.textFile(dataFile)</strong></span>
<span class="strong"><strong>  val VectorData = csvData.map</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    csvLine =&gt;</strong></span>
<span class="strong"><strong>      Vectors.dense( csvLine.split(',').map(_.toDouble))</strong></span>
<span class="strong"><strong>  }</strong></span>
</pre></div><p>A K-Means object is initialized, and the parameters are set to define the number of clusters, and the maximum number of iterations to determine them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val kMeans = new KMeans</strong></span>
<span class="strong"><strong>  val numClusters         = 3</strong></span>
<span class="strong"><strong>  val maxIterations       = 50</strong></span>
</pre></div><p>Some default values are defined for initialization mode, the number of runs, and Epsilon, which I needed for the K-Means call, but did not vary for processing. Finally, these parameters were set against the K-Means object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val initializationMode  = KMeans.K_MEANS_PARALLEL</strong></span>
<span class="strong"><strong>  val numRuns             = 1</strong></span>
<span class="strong"><strong>  val numEpsilon          = 1e-4</strong></span>

<span class="strong"><strong>  kMeans.setK( numClusters )</strong></span>
<span class="strong"><strong>  kMeans.setMaxIterations( maxIterations )</strong></span>
<span class="strong"><strong>  kMeans.setInitializationMode( initializationMode )</strong></span>
<span class="strong"><strong>  kMeans.setRuns( numRuns )</strong></span>
<span class="strong"><strong>  kMeans.setEpsilon( numEpsilon )</strong></span>
</pre></div><p>I cached the training vector data to improve the performance, and trained the K-Means object using the Vector Data to create a trained K-Means model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  VectorData.cache</strong></span>
<span class="strong"><strong>  val kMeansModel = kMeans.run( VectorData )</strong></span>
</pre></div><p>I have computed the K-Means <a id="id111" class="indexterm"></a>cost, the number of input data rows, and output the results via print line statements. The cost value indicates how tightly the clusters are packed, and how separated clusters are:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val kMeansCost = kMeansModel.computeCost( VectorData )</strong></span>

<span class="strong"><strong>  println( "Input data rows : " + VectorData.count() )</strong></span>
<span class="strong"><strong>  println( "K-Means Cost    : " + kMeansCost )</strong></span>
</pre></div><p>Next, I have used the K-Means Model to print the cluster centers as vectors for each of the three clusters that were computed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  kMeansModel.clusterCenters.foreach{ println }</strong></span>
</pre></div><p>Finally, I have used the K-Means Model <code class="literal">predict</code> function to create a list of cluster membership predictions. I have then counted these predictions by value to give a count of the data points in each cluster. This shows which clusters are bigger, and if there really are three clusters:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val clusterRddInt = kMeansModel.predict( VectorData )</strong></span>

<span class="strong"><strong>  val clusterCount = clusterRddInt.countByValue</strong></span>

<span class="strong"><strong>  clusterCount.toList.foreach{ println }</strong></span>

<span class="strong"><strong>} // end object kmeans1</strong></span>
</pre></div><p>So, in order to run this application, it must be compiled and packaged from the <code class="literal">kmeans</code> subdirectory as the<a id="id112" class="indexterm"></a> Linux <code class="literal">pwd</code> command shows here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn kmeans]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/kmeans</strong></span>
<span class="strong"><strong>[hadoop@hc2nn kmeans]$ sbt package</strong></span>

<span class="strong"><strong>Loading /usr/share/sbt/bin/sbt-launch-lib.bash</strong></span>
<span class="strong"><strong>[info] Set current project to K-Means (in build file:/home/hadoop/spark/kmeans/)</strong></span>
<span class="strong"><strong>[info] Compiling 2 Scala sources to /home/hadoop/spark/kmeans/target/scala-2.10/classes...</strong></span>
<span class="strong"><strong>[info] Packaging /home/hadoop/spark/kmeans/target/scala-2.10/k-means_2.10-1.0.jar ...</strong></span>
<span class="strong"><strong>[info] Done packaging.</strong></span>
<span class="strong"><strong>[success] Total time: 20 s, completed Feb 19, 2015 5:02:07 PM</strong></span>
</pre></div><p>Once this packaging is successful, I check HDFS to ensure that the test data is ready. As in the last example, I converted my data to numeric form using the <code class="literal">convert.scala</code> file, provided in the software package. I will process the data file <code class="literal">DigitalBreathTestData2013-MALE2a.csv</code> in the HDFS directory /<code class="literal">data/spark/kmeans</code> shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn nbayes]$ hdfs dfs -ls /data/spark/kmeans</strong></span>
<span class="strong"><strong>Found 3 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup   24645166 2015-02-05 21:11 /data/spark/kmeans/DigitalBreathTestData2013-MALE2.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup    5694226 2015-02-05 21:48 /data/spark/kmeans/DigitalBreathTestData2013-MALE2a.csv</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hadoop supergroup          0 2015-02-05 21:46 /data/spark/kmeans/result</strong></span>
</pre></div><p>The <code class="literal">spark-submit</code> tool is used to run the K-Means application. The only change in this command, as shown here, is that the class is now <code class="literal">kmeans1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>spark-submit \</strong></span>
<span class="strong"><strong>  --class kmeans1 \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 700M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 100 \</strong></span>
<span class="strong"><strong>  /home/hadoop/spark/kmeans/target/scala-2.10/k-means_2.10-1.0.jar</strong></span>
</pre></div><p>The output from the Spark cluster run is shown to be as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Input data rows : 467054</strong></span>
<span class="strong"><strong>K-Means Cost    : 5.40312223450789E7</strong></span>
</pre></div><p>The previous output shows the input data volume, which looks correct, plus it also shows the K-Means cost value. Next comes the three vectors, which describe the data cluster centers with the correct number of dimensions. Remember that these cluster centroid vectors will have the same number of columns as the original vector data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.24698249738061878,1.3015883142472253,0.005830116872250263,2.9173747788555207,1.156645130895448,3.4400290524342454]</strong></span>

<span class="strong"><strong>[0.3321793984152627,1.784137241326256,0.007615970459266097,2.5831987075928917,119.58366028156011,3.8379106085083468]</strong></span>

<span class="strong"><strong>[0.25247226760684494,1.702510963969387,0.006384899819416975,2.231404248000688,52.202897927594805,3.551509158139135]</strong></span>
</pre></div><p>Finally, cluster membership is given for clusters 1 to 3 with cluster 1 (index 0) having the largest membership at <code class="literal">407,539</code> member vectors.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(0,407539)</strong></span>
<span class="strong"><strong>(1,12999)</strong></span>
<span class="strong"><strong>(2,46516)</strong></span>
</pre></div><p>So, these two examples show<a id="id113" class="indexterm"></a> how data can be classified and clustered using Naïve Bayes and K-Means. But what if I want to classify images or more complex patterns, and use a black box approach to classification? The next section examines Spark-based classification using <span class="strong"><strong>ANN's</strong></span>, or <a id="id114" class="indexterm"></a>
<span class="strong"><strong>Artificial Neural Network's</strong></span>. In order to do this, I need to download the latest Spark code, and build a server for Spark 1.3, as it has not yet been formally released (at the time of writing).</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>ANN – Artificial Neural Networks</h2></div></div><hr /></div><p>In order to examine the <span class="strong"><strong>ANN</strong></span> (Artificial Neural Network) functionality in Apache Spark, I will need to obtain the latest source code from the GitHub website. The ANN functionality has been developed by Bert Greevenbosch (<a class="ulink" href="http://www.bertgreevenbosch.nl/" target="_blank">http://www.bertgreevenbosch.nl/</a>), and is set to be released in Apache Spark 1.3. At the<a id="id115" class="indexterm"></a> time of writing the current Spark release is 1.2.1, and CDH 5.x ships with Spark 1.0. So, in order to examine this unreleased ANN functionality, the source code will need to be sourced and built into a Spark server. This is what I will do after explaining a little on the theory behind ANN.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec29"></a>Theory</h3></div></div></div><p>The following figure <a id="id116" class="indexterm"></a>shows a simple biological neuron to the left. The neuron has dendrites that receive signals from other neurons. A cell body controls activation, and an axon carries an electrical impulse to the dendrites of other neurons. The artificial neuron to the right has a series of weighted inputs: a summing function that groups the inputs, and a firing mechanism (<span class="strong"><strong>F(Net)</strong></span>), which decides whether the inputs have reached a threshold, and if so, the neuron will fire:</p><div class="mediaobject"><img src="graphics/B01989_02_07.jpg" /></div><p>Neural networks are tolerant of noisy images and distortion, and so are useful when a black box classification method is needed for potentially degraded images. The next area to consider is the summation function for the neuron inputs. The following diagram shows the <a id="id117" class="indexterm"></a>summation function called <span class="strong"><strong>Net</strong></span> for neuron <span class="strong"><strong>i</strong></span>. The connections between the neurons that have the weighting values, contain the stored knowledge of the network. Generally, a network will have an input layer, an output layer, and a number of hidden layers. A neuron will fire if the sum of its inputs exceeds a threshold.</p><div class="mediaobject"><img src="graphics/B01989_02_08.jpg" /></div><p>In the previous equation, the diagram and the key show that the input values from a pattern <span class="strong"><strong>P</strong></span> are passed to neurons in the input layer of a network. These values become the input layer neuron activation values; they are a special case. The inputs to neuron <span class="strong"><strong>i</strong></span> are the sum of the weighting value for neuron connection <span class="strong"><strong>i-j</strong></span>, multiplied by the activation from neuron <span class="strong"><strong>j</strong></span>. The<a id="id118" class="indexterm"></a> activation at neuron <span class="strong"><strong>j</strong></span> (if it is not an input layer neuron) is given by <span class="strong"><strong>F(Net)</strong></span>, the squashing function, which will be described next.</p><p>A simulated neuron needs a firing mechanism, which decides whether the inputs to the neuron have reached a threshold. And then, it fires to create the activation value for that neuron. This firing or squashing function can be described by the generalized sigmoid function shown in the following figure:</p><div class="mediaobject"><img src="graphics/B01989_02_09.jpg" /></div><p>This function has two constants: <span class="strong"><strong>A</strong></span> and <span class="strong"><strong>B</strong></span>; <span class="strong"><strong>B</strong></span> affects the shape of the activation curve as shown in the previous graph. The bigger the value, the more similar a function becomes to an on/off step. The <a id="id119" class="indexterm"></a>value of <span class="strong"><strong>A</strong></span> sets a minimum for the returned activation. In the previous graph it is zero.</p><p>So, this provides a mechanism for simulating a neuron, creating weighting matrices as the neuron connections, and managing the neuron activation. But how are the networks organized? The next diagram shows a suggested neuron architecture—the neural network has an input layer of neurons, an output layer, and one or more hidden layers. All neurons in each layer are connected to each neuron in the adjacent layers.</p><div class="mediaobject"><img src="graphics/B01989_02_10.jpg" /></div><p>During the training, activation passes from the input layer through the network to the output layer. Then, the error or difference between the expected or actual output causes error deltas to be passed back through the network, altering the weighting matrix values. Once the<a id="id120" class="indexterm"></a> desired output layer vector is achieved, then the knowledge is stored in the weighting matrices, and the network can be further trained or used for classification.</p><p>So, the theory behind neural networks has been described in terms of back propagation. Now is the time to obtain the development version of the Apache Spark code, and build the Spark server, so that the ANN Scala code can be run.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec30"></a>Building the Spark server</h3></div></div></div><p>I would not normally advise that Apache Spark code be downloaded and used before it has been released by Spark, or packaged by Cloudera (for use with CDH), but the desire to examine ANN <a id="id121" class="indexterm"></a>functionality, along with the time scale allowed for this book, mean that I need to do so. I extracted the full Spark code tree from this path:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>https://github.com/apache/spark/pull/1290.</strong></span>
</pre></div><p>I stored this code on the Linux server <code class="literal">hc2nn</code>, under the directory <code class="literal">/home/hadoop/spark/spark</code>. I then obtained the ANN code from Bert Greevenbosch's GitHub development area:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>https://github.com/bgreeven/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/ann/ArtificialNeuralNetwork.scala</strong></span>
<span class="strong"><strong>https://github.com/bgreeven/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/ANNClassifier.scala</strong></span>
</pre></div><p>The <code class="literal">ANNClassifier.scala</code> file contains the public functions that will be called. The <code class="literal">ArtificialNeuralNetwork.scala</code> file contains the private MLlib ANN functions that <code class="literal">ANNClassifier.scala</code> calls. I already have Java open JDK installed on my server, so the next step is to set up the <code class="literal">spark-env.sh</code> environment configuration file under <code class="literal">/home/hadoop/spark/spark/conf</code>. My file looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export STANDALONE_SPARK_MASTER_HOST=hc2nn.semtech-solutions.co.nz</strong></span>
<span class="strong"><strong>export SPARK_MASTER_IP=$STANDALONE_SPARK_MASTER_HOST</strong></span>
<span class="strong"><strong>export SPARK_HOME=/home/hadoop/spark/spark</strong></span>
<span class="strong"><strong>export SPARK_LAUNCH_WITH_SCALA=0</strong></span>
<span class="strong"><strong>export SPARK_MASTER_WEBUI_PORT=19080</strong></span>
<span class="strong"><strong>export SPARK_MASTER_PORT=8077</strong></span>
<span class="strong"><strong>export SPARK_WORKER_PORT=8078</strong></span>
<span class="strong"><strong>export SPARK_WORKER_WEBUI_PORT=19081</strong></span>
<span class="strong"><strong>export SPARK_WORKER_DIR=/var/run/spark/work</strong></span>
<span class="strong"><strong>export SPARK_LOG_DIR=/var/log/spark</strong></span>
<span class="strong"><strong>export SPARK_HISTORY_SERVER_LOG_DIR=/var/log/spark</strong></span>
<span class="strong"><strong>export SPARK_PID_DIR=/var/run/spark/</strong></span>
<span class="strong"><strong>export HADOOP_CONF_DIR=/etc/hadoop/conf</strong></span>
<span class="strong"><strong>export SPARK_JAR_PATH=${SPARK_HOME}/assembly/target/scala-2.10/</strong></span>
<span class="strong"><strong>export SPARK_JAR=${SPARK_JAR_PATH}/spark-assembly-1.3.0-SNAPSHOT-hadoop2.3.0-cdh5.1.2.jar</strong></span>
<span class="strong"><strong>export JAVA_HOME=/usr/lib/jvm/java-1.7.0</strong></span>
<span class="strong"><strong>export SPARK_LOCAL_IP=192.168.1.103</strong></span>
</pre></div><p>The <code class="literal">SPARK_MASTER_IP</code> variable tells the cluster which server is the master. The port variables define the master, the worker web, and the operating port values. There are some log and JAR file paths defined, as well as <code class="literal">JAVA_HOME</code> and the local server IP address. Details for building Spark with Apache Maven can be found at:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>http://spark.apache.org/docs/latest/building-spark.html</strong></span>
</pre></div><p>The slaves file in the same directory will be set up as before with the names of the four workers servers<a id="id122" class="indexterm"></a> from <code class="literal">hc2r1m1</code> to <code class="literal">hc2r1m4</code>.</p><p>In order to build using Apache Maven, I had to install <code class="literal">mvn</code> on to my Linux server <code class="literal">hc2nn</code>, where I will run the Spark build. I did this as the root user, obtaining a Maven repository file by first using <code class="literal">wget</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo</strong></span>
</pre></div><p>Then, checking that the new repository file is in place with <code class="literal">ls</code> long listing.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# ls -l /etc/yum.repos.d/epel-apache-maven.repo</strong></span>
<span class="strong"><strong>-rw-r--r-- 1 root root 445 Mar  4  2014 /etc/yum.repos.d/epel-apache-maven.repo</strong></span>
</pre></div><p>Then Maven can be installed using the Linux <code class="literal">yum</code> command, the examples below show the install command and a check via <code class="literal">ls</code> that the <code class="literal">mvn</code> command exists.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# yum install apache-maven</strong></span>
<span class="strong"><strong>[root@hc2nn ~]# ls -l /usr/share/apache-maven/bin/mvn</strong></span>
<span class="strong"><strong>-rwxr-xr-x 1 root root 6185 Dec 15 06:30 /usr/share/apache-maven/bin/mvn</strong></span>
</pre></div><p>The commands that I have used to build the Spark source tree are shown here along with the successful output. First, the environment is set up, and then the build is started with the <code class="literal">mvn</code> command. Options are added to build for Hadoop 2.3/yarn, and the tests are skipped. The build uses the <code class="literal">clean</code> and <code class="literal">package</code> options to remove the old build files each time, and then create JAR files. Finally, the build output is copied via the <code class="literal">tee</code> command to a file named <code class="literal">build.log</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd /home/hadoop/spark/spark/conf ; . ./spark-env.sh ; cd ..</strong></span>

<span class="strong"><strong>mvn  -Pyarn -Phadoop-2.3  -Dhadoop.version=2.3.0-cdh5.1.2 -DskipTests clean package | tee build.log 2&gt;&amp;1</strong></span>

<span class="strong"><strong>[INFO] ----------------------------------------------------------</strong></span>
<span class="strong"><strong>[INFO] BUILD SUCCESS</strong></span>
<span class="strong"><strong>[INFO] ----------------------------------------------------------</strong></span>
<span class="strong"><strong>[INFO] Total time: 44:20 min</strong></span>
<span class="strong"><strong>[INFO] Finished at: 2015-02-16T12:20:28+13:00</strong></span>
<span class="strong"><strong>[INFO] Final Memory: 76M/925M</strong></span>
<span class="strong"><strong>[INFO] ----------------------------------------------------------</strong></span>
</pre></div><p>The actual build command that you use will depend upon whether you have Hadoop, and the version of it. Check the previous <span class="emphasis"><em>building spark</em></span> for details, the build takes around 40 minutes on my servers.</p><p>Given that this<a id="id123" class="indexterm"></a> build will be packaged and copied to the other servers in the Spark cluster, it is important that all the servers use the same version of Java, else errors such as these will occur:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>15/02/15 12:41:41 ERROR executor.Executor: Exception in task 0.1 in stage 0.0 (TID 2)</strong></span>
<span class="strong"><strong>java.lang.VerifyError: class org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;</strong></span>
<span class="strong"><strong>        at java.lang.ClassLoader.defineClass1(Native Method)</strong></span>
</pre></div><p>Given that the source tree has been built, it now needs to be bundled up and released to each of the servers in the Spark cluster. Given that these servers are also the members of the CDH cluster, and have password-less SSH access set up, I can use the <code class="literal">scp</code> command to release the built software. The following commands show the spark directory under the <code class="literal">/home/hadoop/spark</code> path being packaged into a tar file called <code class="literal">spark_bld.tar</code>. The Linux <code class="literal">scp</code> command is then used to copy the tar file to each slave server; the following example shows <code class="literal">hc2r1m1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn spark]$ cd /home/hadoop/spark</strong></span>
<span class="strong"><strong>[hadoop@hc2nn spark]$ tar cvf spark_bld.tar spark</strong></span>
<span class="strong"><strong>[hadoop@hc2nn spark]$ scp ./spark_bld.tar hadoop@hc2r1m1:/home/hadoop/spark/spark_bld.tar</strong></span>
</pre></div><p>Now that the tarred Spark build is on the slave node, it needs to be unpacked. The following command shows the process for the server <code class="literal">hc2r1m1</code>. The tar file is unpacked to the same directory as the build server <code class="literal">hc2nn</code>, that is, <code class="literal">/home/hadoop/spark</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 ~]$ mkdir spark ; mv spark_bld.tar spark</strong></span>
<span class="strong"><strong>[hadoop@hc2r1m1 ~]$ cd spark ; ls</strong></span>
<span class="strong"><strong>spark_bld.tar</strong></span>
<span class="strong"><strong>[hadoop@hc2r1m1 spark]$ tar xvf spark_bld.tar</strong></span>
</pre></div><p>Once the build has been run successfully, and the built code has been released to the slave servers, the built version of Spark can be started from the master server <span class="strong"><strong>hc2nn</strong></span>. Note that I have chosen different port numbers from the Spark version 1.0, installed on these servers. Also note that I will start Spark as root, because the Spark 1.0 install is managed as Linux services under the root account. As the two installs will share facilities like logging and <code class="literal">.pid</code> file locations, root user will ensure access. This is the script that I have used to start Apache Spark 1.3:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd /home/hadoop/spark/spark/conf ;  . ./spark-env.sh ; cd ../sbin</strong></span>
<span class="strong"><strong>echo "hc2nn - start master server"</strong></span>
<span class="strong"><strong>./start-master.sh</strong></span>
<span class="strong"><strong>echo "sleep 5000 ms"</strong></span>
<span class="strong"><strong>sleep 5</strong></span>
<span class="strong"><strong>echo "hc2nn - start history server"</strong></span>
<span class="strong"><strong>./start-history-server.sh</strong></span>
<span class="strong"><strong>echo "Start Spark slaves workers"</strong></span>
<span class="strong"><strong>./start-slaves.sh</strong></span>
</pre></div><p>It executes the <a id="id124" class="indexterm"></a>
<code class="literal">spark-env.sh</code> file to set up the environment, and then uses the scripts in the Spark <code class="literal">sbin</code> directory to start the services. It starts the master and the history server first on <code class="literal">hc2nn</code>, and then it starts the slaves. I added a delay before starting the slaves, as I found that they were trying to connect to the master before it was ready. The Spark 1.3 web user interface can now be accessed via this URL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>http://hc2nn.semtech-solutions.co.nz:19080/</strong></span>
</pre></div><p>The Spark URL, which allows applications to connect to Spark is this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Spark Master at spark://hc2nn.semtech-solutions.co.nz:8077</strong></span>
</pre></div><p>As defined by the port numbers in the spark environment configuration file, Spark is now available to be used with ANN functionality. The next section will present the ANN Scala scripts and data to show how this Spark-based functionality can be used.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec31"></a>ANN in practice</h3></div></div></div><p>In order to begin<a id="id125" class="indexterm"></a> ANN training, test data is needed. Given that this type of classification method is supposed to be good at classifying distorted or noisy images, I have decided to attempt to classify the images here:</p><div class="mediaobject"><img src="graphics/B01989_02_11.jpg" /></div><p>They are hand-crafted text files that contain shaped blocks, created from the characters 1 and 0. When they are stored on HDFS, the carriage return characters are removed, so that the image is presented <a id="id126" class="indexterm"></a>as a single line vector. So, the ANN will be classifying a series of shape images, and then it will be tested against the same images with noise added to determine whether the classification will still work. There are six training images, and they will each be given an arbitrary training label from 0.1 to 0.6. So, if the ANN is presented with a closed square, it should return a label of 0.1. The following image shows an example of a testing image with noise added. The noise, created by adding extra zero (0) characters within the image, has been highlighted:</p><div class="mediaobject"><img src="graphics/B01989_02_12.jpg" /></div><p>Because the Apache Spark server has changed from the previous examples, and the Spark library locations have<a id="id127" class="indexterm"></a> also changed, the <code class="literal">sbt</code> configuration file used for compiling the example ANN Scala code must also be changed. As before, the ANN code is being developed using the Linux hadoop account in a subdirectory called <code class="literal">spark/ann</code>. The <code class="literal">ann.sbt</code> file exists within the <code class="literal">ann</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> [hadoop@hc2nn ann]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/ann</strong></span>

<span class="strong"><strong> [hadoop@hc2nn ann]$ ls</strong></span>
<span class="strong"><strong>ann.sbt    project  src  target</strong></span>
</pre></div><p>The contents of the <code class="literal">ann.sbt</code> file have been changed to use full paths of JAR library files for the Spark dependencies. This is because the new Apache Spark code for build 1.3 now resides under <code class="literal">/home/hadoop/spark/spark</code>. Also, the project name has been changed to <code class="literal">A N N</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>name := "A N N"</strong></span>
<span class="strong"><strong>version := "1.0"</strong></span>
<span class="strong"><strong>scalaVersion := "2.10.4"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "spark-core"  % "1.3.0" from "file:///home/hadoop/spark/spark/core/target/spark-core_2.10-1.3.0-SNAPSHOT.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "spark-mllib" % "1.3.0" from "file:///home/hadoop/spark/spark/mllib/target/spark-mllib_2.10-1.3.0-SNAPSHOT.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "akka" % "1.3.0" from "file:///home/hadoop/spark/spark/assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop2.3.0-cdh5.1.2.jar"</strong></span>
</pre></div><p>As in the previous <a id="id128" class="indexterm"></a>examples, the actual Scala code to be compiled exists in a subdirectory named <code class="literal">src/main/scala</code> as shown next. I have created two Scala programs. The first trains using the input data, and then tests the ANN model with the same input data. The second tests the trained model with noisy data, to the test distorted data classification:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn scala]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/ann/src/main/scala</strong></span>

<span class="strong"><strong>[hadoop@hc2nn scala]$ ls</strong></span>
<span class="strong"><strong>test_ann1.scala  test_ann2.scala</strong></span>
</pre></div><p>I will examine the first Scala file entirely, and then I will just show the extra features of the second file, as the two examples are very similar up to the point of training the ANN. The code examples shown here can be found in the software package provided with this book, under the path <code class="literal">chapter2\ANN</code>. So, to examine the first Scala example, the import statements are similar to the previous examples. The Spark context, configuration, vectors, and <code class="literal">LabeledPoint</code> are being imported. The RDD class for RDD processing is being imported this time, along with the new ANN class <code class="literal">ANNClassifier</code>. Note that the <code class="literal">MLlib/classification</code> routines widely use the <code class="literal">LabeledPoint</code> structure for input data, which will contain the features and labels that are supposed to be trained against:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import org.apache.spark.SparkContext</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkConf</strong></span>

<span class="strong"><strong>import org.apache.spark.mllib.classification.ANNClassifier</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg._</strong></span>
<span class="strong"><strong>import org.apache.spark.rdd.RDD</strong></span>


<span class="strong"><strong>object testann1 extends App</strong></span>
<span class="strong"><strong>{</strong></span>
</pre></div><p>The application class in this example has been called <code class="literal">testann1</code>. The HDFS files to be processed have been<a id="id129" class="indexterm"></a> defined in terms of the HDFS server, path, and file name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val server = "hdfs://hc2nn.semtech-solutions.co.nz:8020"</strong></span>
<span class="strong"><strong>  val path   = "/data/spark/ann/"</strong></span>

<span class="strong"><strong>  val data1 = server + path + "close_square.img"</strong></span>
<span class="strong"><strong>  val data2 = server + path + "close_triangle.img"</strong></span>
<span class="strong"><strong>  val data3 = server + path + "lines.img"</strong></span>
<span class="strong"><strong>  val data4 = server + path + "open_square.img"</strong></span>
<span class="strong"><strong>  val data5 = server + path + "open_triangle.img"</strong></span>
<span class="strong"><strong>  val data6 = server + path + "plus.img"</strong></span>
</pre></div><p>The Spark context has been created with the URL for the Spark instance, which now has a different port number—<code class="literal">8077</code>. The application name is <code class="literal">ANN 1</code>. This will appear on the Spark web UI when the application is run:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:8077"</strong></span>
<span class="strong"><strong>  val appName = "ANN 1"</strong></span>
<span class="strong"><strong>  val conf = new SparkConf()</strong></span>

<span class="strong"><strong>  conf.setMaster(sparkMaster)</strong></span>
<span class="strong"><strong>  conf.setAppName(appName)</strong></span>

<span class="strong"><strong>  val sparkCxt = new SparkContext(conf)</strong></span>
</pre></div><p>The HDFS-based input training and test data files are loaded. The values on each line are split by space characters, and the numeric values have been converted into Doubles. The variables that contain this data are then stored in an array called inputs. At the same time, an array called outputs is created, containing the labels from 0.1 to 0.6. These values will be used to classify the input patterns:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val rData1 = sparkCxt.textFile(data1).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rData2 = sparkCxt.textFile(data2).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rData3 = sparkCxt.textFile(data3).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rData4 = sparkCxt.textFile(data4).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rData5 = sparkCxt.textFile(data5).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rData6 = sparkCxt.textFile(data6).map(_.split(" ").map(_.toDouble)).collect</strong></span>

<span class="strong"><strong>  val inputs = Array[Array[Double]] (</strong></span>
<span class="strong"><strong>     rData1(0), rData2(0), rData3(0), rData4(0), rData5(0), rData6(0) )</strong></span>

<span class="strong"><strong>  val outputs = Array[Double]( 0.1, 0.2, 0.3, 0.4, 0.5, 0.6 )</strong></span>
</pre></div><p>The input and output<a id="id130" class="indexterm"></a> data, representing the input data features and labels, are then combined and converted into a <code class="literal">LabeledPoint</code> structure. Finally, the data is parallelized in order to partition it for the optimal parallel processing:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val ioData = inputs.zip( outputs )</strong></span>
<span class="strong"><strong>  val lpData = ioData.map{ case(features,label) =&gt;</strong></span>

<span class="strong"><strong>    LabeledPoint( label, Vectors.dense(features) )</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>  val rddData = sparkCxt.parallelize( lpData )</strong></span>
</pre></div><p>Variables are created to define the hidden layer topology of the ANN. In this case, I have chosen to have two hidden layers, each with 100 neurons. The maximum numbers of iterations are defined, as well as a batch size (six patterns) and convergence tolerance. The tolerance refers to how big the training error can get before we can consider training to have worked. Then, an ANN model is created using these configuration parameters and the input data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val hiddenTopology : Array[Int] = Array( 100, 100 )</strong></span>
<span class="strong"><strong>  val maxNumIterations = 1000</strong></span>
<span class="strong"><strong>  val convTolerance    = 1e-4</strong></span>
<span class="strong"><strong>  val batchSize        = 6</strong></span>

<span class="strong"><strong>  val annModel = ANNClassifier.train(rddData,</strong></span>
<span class="strong"><strong>                                     batchSize,</strong></span>
<span class="strong"><strong>                                     hiddenTopology,</strong></span>
<span class="strong"><strong>                                     maxNumIterations,</strong></span>
<span class="strong"><strong>                                     convTolerance)</strong></span>
</pre></div><p>In order to test the trained ANN model, the same input training data is used as testing data used to obtain prediction labels. First, an input data variable is created called <code class="literal">rPredictData</code>. Then, the data is partitioned and finally, the predictions are obtained using the trained ANN model. For this model to work, it must output the labels 0.1 to 0.6:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val rPredictData = inputs.map{ case(features) =&gt;</strong></span>

<span class="strong"><strong>    ( Vectors.dense(features) )</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>  val rddPredictData = sparkCxt.parallelize( rPredictData )</strong></span>
<span class="strong"><strong>  val predictions = annModel.predict( rddPredictData )</strong></span>
</pre></div><p>The label predictions are printed, and the script closes with a closing bracket:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  predictions.toArray().foreach( value =&gt; println( "prediction &gt; " + value ) )</strong></span>
<span class="strong"><strong>} // end ann1</strong></span>
</pre></div><p>So, in order to run this code sample, it must first be compiled and packaged. By now, you must be familiar with the <code class="literal">sbt</code> command, executed from the <code class="literal">ann</code> sub directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ann]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/ann</strong></span>
<span class="strong"><strong>[hadoop@hc2nn ann]$ sbt package</strong></span>
</pre></div><p>The <code class="literal">spark-submit</code> command is<a id="id131" class="indexterm"></a> then used from within the new <code class="literal">spark/spark</code> path using the new Spark-based URL at port 8077 to run the application <code class="literal">testann1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/home/hadoop/spark/spark/bin/spark-submit \</strong></span>
<span class="strong"><strong>  --class testann1 \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:8077  \</strong></span>
<span class="strong"><strong>  --executor-memory 700M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 100 \</strong></span>
<span class="strong"><strong>  /home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar</strong></span>
</pre></div><p>By checking the Apache Spark web URL at <code class="literal">http://hc2nn.semtech-solutions.co.nz:19080/</code>, it is now possible to see the application running. The following figure shows the application <span class="strong"><strong>ANN 1</strong></span> running, as well as the previous completed executions:</p><div class="mediaobject"><img src="graphics/B01989_02_13.jpg" /></div><p>By selecting one <a id="id132" class="indexterm"></a>of the cluster host worker instances, it is possible to see a list of executors that actually carry out cluster processing for that worker:</p><p> </p><div class="mediaobject"><img src="graphics/B01989_02_14.jpg" /></div><p>
</p><p>Finally, by selecting one of the executors, it is possible to see its history and configuration, as<a id="id133" class="indexterm"></a> well as the links to the log file, and error information. At this level, with the log information provided, debugging is possible. These log files can be checked for processing error messages.</p><div class="mediaobject"><img src="graphics/B01989_02_15.jpg" /></div><p>The <span class="strong"><strong>ANN 1</strong></span> application provides the following output to show that it has reclassified the same input data correctly. The reclassification has been successful, as each of the input patterns has been given the same label as it was trained with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>prediction &gt; 0.1</strong></span>
<span class="strong"><strong>prediction &gt; 0.2</strong></span>
<span class="strong"><strong>prediction &gt; 0.3</strong></span>
<span class="strong"><strong>prediction &gt; 0.4</strong></span>
<span class="strong"><strong>prediction &gt; 0.5</strong></span>
<span class="strong"><strong>prediction &gt; 0.6</strong></span>
</pre></div><p>So, this shows that <a id="id134" class="indexterm"></a>ANN training and test prediction will work with the same data. Now, I will train with the same data, but test with distorted or noisy data, an example of which I already demonstrated. This example can be found in the file called <code class="literal">test_ann2.scala</code>, in your software package. It is very similar to the first example, so I will just demonstrate the changed code. The application is now called <code class="literal">testann2</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>object testann2 extends App</strong></span>
</pre></div><p>An extra set of testing data is created, after the ANN model has been created using the training data. This testing data contains noise:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val tData1 = server + path + "close_square_test.img"</strong></span>
<span class="strong"><strong>  val tData2 = server + path + "close_triangle_test.img"</strong></span>
<span class="strong"><strong>  val tData3 = server + path + "lines_test.img"</strong></span>
<span class="strong"><strong>  val tData4 = server + path + "open_square_test.img"</strong></span>
<span class="strong"><strong>  val tData5 = server + path + "open_triangle_test.img"</strong></span>
<span class="strong"><strong>  val tData6 = server + path + "plus_test.img"</strong></span>
</pre></div><p>This data is processed into input arrays, and is partitioned for cluster processing:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val rtData1 = sparkCxt.textFile(tData1).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rtData2 = sparkCxt.textFile(tData2).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rtData3 = sparkCxt.textFile(tData3).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rtData4 = sparkCxt.textFile(tData4).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rtData5 = sparkCxt.textFile(tData5).map(_.split(" ").map(_.toDouble)).collect</strong></span>
<span class="strong"><strong>  val rtData6 = sparkCxt.textFile(tData6).map(_.split(" ").map(_.toDouble)).collect</strong></span>

<span class="strong"><strong>  val tInputs = Array[Array[Double]] (</strong></span>
<span class="strong"><strong>     rtData1(0), rtData2(0), rtData3(0), rtData4(0), rtData5(0), rtData6(0) )</strong></span>

<span class="strong"><strong>  val rTestPredictData = tInputs.map{ case(features) =&gt; ( Vectors.dense(features) ) }</strong></span>
<span class="strong"><strong>  val rddTestPredictData = sparkCxt.parallelize( rTestPredictData )</strong></span>
</pre></div><p>It is then used to<a id="id135" class="indexterm"></a> generate label predictions in the same way as the first example. If the model classifies the data correctly, then the same label values should be printed from 0.1 to 0.6:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  val testPredictions = annModel.predict( rddTestPredictData )</strong></span>
<span class="strong"><strong>  testPredictions.toArray().foreach( value =&gt; println( "test prediction &gt; " + value ) )</strong></span>
</pre></div><p>The code has already been compiled, so it can be run using the <code class="literal">spark-submit</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/home/hadoop/spark/spark/bin/spark-submit \</strong></span>
<span class="strong"><strong>  --class testann2 \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:8077  \</strong></span>
<span class="strong"><strong>  --executor-memory 700M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 100 \</strong></span>
<span class="strong"><strong>  /home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar</strong></span>
</pre></div><p>Here is the cluster output from this script, which shows a successful classification using a trained ANN model, and some noisy test data. The noisy data has been classified correctly. For instance, if the trained model had become confused, it might have given a value of <code class="literal">0.15</code> for the noisy <code class="literal">close_square_test.img</code> test image in position one, instead of returning <code class="literal">0.1</code> as it did:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>test prediction &gt; 0.1</strong></span>
<span class="strong"><strong>test prediction &gt; 0.2</strong></span>
<span class="strong"><strong>test prediction &gt; 0.3</strong></span>
<span class="strong"><strong>test prediction &gt; 0.4</strong></span>
<span class="strong"><strong>test prediction &gt; 0.5</strong></span>
<span class="strong"><strong>test prediction &gt; 0.6</strong></span>
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Summary</h2></div></div><hr /></div><p>This chapter has attempted to provide you with an overview of some of the functionality available within the Apache Spark MLlib module. It has also shown the functionality that will soon be available in terms of ANN, or artificial neural networks, which is intended for release in Spark 1.3. It has not been possible to cover all the areas of MLlib, due to the time and space allowed for this chapter.</p><p>You have been shown how to develop Scala-based examples for Naïve Bayes classification, K-Means clustering, and ANN or artificial neural networks. You have been shown how to prepare test data for these Spark MLlib routines. You have also been shown that they all accept the LabeledPoint structure, which contains features and labels. Also, each approach takes a training and prediction approach to training and testing a model using different data sets. Using the approach shown in this chapter, you can now investigate the remaining functionality in the MLlib library. You should refer to the <a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a> website, and ensure that when checking documentation that you refer to the correct version, that is, <a class="ulink" href="http://spark.apache.org/docs/1.0.0/" target="_blank">http://spark.apache.org/docs/1.0.0/</a> for version 1.0.0.</p><p>Having examined the Apache Spark MLlib machine learning library, in this chapter, it is now time to consider Apache Spark's stream processing capability. The next chapter will examine stream processing using the Spark and Scala-based example code.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>Chapter 3. Apache Spark Streaming</h2></div></div></div><p>The Apache Streaming module is a stream processing-based module within Apache Spark. It uses the Spark cluster to offer the ability to scale to a high degree. Being based on Spark, it is also highly fault tolerant, having the ability to rerun failed tasks by checkpointing the data stream that is being processed. The following areas will be covered in this chapter after an initial section, which will provide a practical overview of how Apache Spark processes stream-based data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Error recovery and checkpointing</p></li><li style="list-style-type: disc"><p>TCP-based Stream Processing</p></li><li style="list-style-type: disc"><p>File Streams</p></li><li style="list-style-type: disc"><p>Flume Stream source</p></li><li style="list-style-type: disc"><p>Kafka Stream source</p></li></ul></div><p>For each topic, I will provide a worked example in Scala, and will show how the stream-based architecture can be set up and tested.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec19"></a>Overview</h2></div></div><hr /></div><p>When giving an<a id="id136" class="indexterm"></a> overview of the Apache Spark streaming module, I would advise you to check the <a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a> website for up-to-date information, as <a id="id137" class="indexterm"></a>well as the Spark-based user groups such as <code class="email">&lt;<a class="email" href="mailto:user@spark.apache.org">user@spark.apache.org</a>&gt;</code>. My reason for saying this is because these are the primary places where Spark information is available. Also the extremely fast (and increasing) pace of change means that by the time you read this new Spark functionality and versions, will be available. So, in the light of this, when giving an overview, I will try to generalize.</p><div class="mediaobject"><img src="graphics/B01989_03_01.jpg" /></div><p>The previous figure <a id="id138" class="indexterm"></a>shows potential data sources for Apache Streaming, such<a id="id139" class="indexterm"></a> as <span class="strong"><strong>Kafka</strong></span>, <span class="strong"><strong>Flume</strong></span>, and <span class="strong"><strong>HDFS</strong></span>. These<a id="id140" class="indexterm"></a> feed into the Spark Streaming module, and <a id="id141" class="indexterm"></a>are processed as discrete streams. The diagram also shows that other Spark module functionality, such as machine learning, can be used to process the stream-based data. The <a id="id142" class="indexterm"></a>fully processed data can then be an output for <span class="strong"><strong>HDFS</strong></span>, <span class="strong"><strong>databases</strong></span>, or <a id="id143" class="indexterm"></a>
<span class="strong"><strong>dashboards</strong></span>. This diagram is based on the one at the Spark streaming website, but I wanted to extend it for both—expressing the Spark module functionality, and for dashboarding options. The previous diagram shows a MetricSystems feed being fed from Spark to Graphite. Also, it is possible to feed Solr-based data to Lucidworks banana (a port of kabana). It is also worth mentioning here that Databricks (see <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark Databricks</em></span> and <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Databricks Visualization</em></span>) can also present the Spark stream data as a dashboard.</p><div class="mediaobject"><img src="graphics/B01989_03_02.jpg" /></div><p>When discussing Spark discrete streams, the previous figure, again taken from the Spark website<a id="id144" class="indexterm"></a> at <a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>, is the diagram I like to use. The green boxes in the previous figure show the continuous data stream sent to Spark, being broken <a id="id145" class="indexterm"></a>down into a <span class="strong"><strong>discrete stream</strong></span> (<span class="strong"><strong>DStream</strong></span>). The size of each element in the stream is then based on a batch time, which might be two seconds. It is also possible to create a window, expressed as the previous red box, over the DStream. For instance, when carrying out trend analysis in real time, it might be necessary to determine the top ten Twitter-based Hashtags over a ten minute window.</p><p>So, given that Spark<a id="id146" class="indexterm"></a> can be used for Stream processing, how is a Stream created? The following Scala-based code shows how a Twitter stream can be created. This example is simplified because Twitter authorization has not been included, but you get the idea (the full example code is in the <span class="emphasis"><em>Checkpointing</em></span> section). The Spark stream context, called <code class="literal">ssc</code>, is created using the spark context <code class="literal">sc</code>. A batch time is specified when it is created; in this case, five seconds. A Twitter-based DStream, called <code class="literal">stream</code>, is then created from the <code class="literal">Streamingcontext</code> using a window of 60 seconds:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val ssc    = new StreamingContext(sc, Seconds(5) )</strong></span>
<span class="strong"><strong>    val stream = TwitterUtils.createStream(ssc,None).window( Seconds(60) )</strong></span>
</pre></div><p>The stream processing can be started with the stream context start method (shown next), and the <code class="literal">awaitTermination</code> method indicates that it should process until stopped. So, if this code is embedded in a library-based application, it will run until the session is terminated, perhaps with a <span class="emphasis"><em>Crtl</em></span> + <span class="emphasis"><em>C</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    ssc.start()</strong></span>
<span class="strong"><strong>    ssc.awaitTermination()</strong></span>
</pre></div><p>This explains what Spark streaming is, and what it does, but it does not explain error handling, or what to do<a id="id147" class="indexterm"></a> if your stream-based application fails. The next section will examine Spark streaming error management and recovery.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec20"></a>Errors and recovery</h2></div></div><hr /></div><p>Generally, the question that needs to be asked for your application is; is it critical that you receive and process <a id="id148" class="indexterm"></a>all the data? If not, then on failure you might just<a id="id149" class="indexterm"></a> be able to restart the application and discard the missing or lost data. If this is not the case, then you will need to use checkpointing, which will be described in the next section.</p><p>It is also worth noting that your application's error management should be robust and self-sufficient. What I mean by this is that; if an exception is non-critical, then manage the exception, perhaps log it, and continue processing. For instance, when a task reaches the maximum number of failures (specified by <code class="literal">spark.task.maxFailures</code>), it will terminate processing.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec32"></a>Checkpointing</h3></div></div></div><p>It is possible to set<a id="id150" class="indexterm"></a> up an HDFS-based checkpoint directory to store Apache Spark-based streaming information. In this Scala example, data will be stored in HDFS, under <code class="literal">/data/spark/checkpoint</code>. The following HDFS file system <code class="literal">ls</code> command shows that before starting, the directory does not exist:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn stream]$ hdfs dfs -ls /data/spark/checkpoint</strong></span>
<span class="strong"><strong>ls: `/data/spark/checkpoint': No such file or directory</strong></span>
</pre></div><p>The Twitter-based Scala code sample given next, starts by defining a package name for the application, and by importing Spark, streaming, context, and Twitter-based functionality. It then defines an application object named <code class="literal">stream1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>package nz.co.semtechsolutions</strong></span>

<span class="strong"><strong>import org.apache.spark._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.twitter._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.StreamingContext._</strong></span>

<span class="strong"><strong>object stream1 {</strong></span>
</pre></div><p>Next, a method is defined called <code class="literal">createContext</code>, which will be used to create both the spark, and streaming contexts. It will also checkpoint the stream to the HDFS-based directory using<a id="id151" class="indexterm"></a> the streaming context checkpoint method, which takes a directory path as a parameter. The directory path being the value (<code class="literal">cpDir</code>) that was passed into the <code class="literal">createContext</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  def createContext( cpDir : String ) : StreamingContext = {</strong></span>

<span class="strong"><strong>    val appName = "Stream example 1"</strong></span>
<span class="strong"><strong>    val conf    = new SparkConf()</strong></span>

<span class="strong"><strong>    conf.setAppName(appName)</strong></span>

<span class="strong"><strong>    val sc = new SparkContext(conf)</strong></span>

<span class="strong"><strong>    val ssc    = new StreamingContext(sc, Seconds(5) )</strong></span>

<span class="strong"><strong>    ssc.checkpoint( cpDir )</strong></span>

<span class="strong"><strong>               ssc</strong></span>
<span class="strong"><strong>  }</strong></span>
</pre></div><p>Now, the main method is defined, as is the <code class="literal">HDFS</code> directory, as well as Twitter access authority and parameters. The Spark streaming context <code class="literal">ssc</code> is either retrieved or created using the HDFS <code class="literal">checkpoint</code> directory via the <code class="literal">StreamingContext</code> method—<code class="literal">getOrCreate</code>. If the directory doesn't exist, then the previous method called <code class="literal">createContext</code> is called, which will create the context and checkpoint. Obviously, I have truncated my own Twitter auth. keys in this example for security reasons:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  def main(args: Array[String]) {</strong></span>

<span class="strong"><strong>    val hdfsDir = "/data/spark/checkpoint"</strong></span>

<span class="strong"><strong>    val consumerKey       = "QQpxx"</strong></span>
<span class="strong"><strong>    val consumerSecret    = "0HFzxx"</strong></span>
<span class="strong"><strong>    val accessToken       = "323xx"</strong></span>
<span class="strong"><strong>    val accessTokenSecret = "IlQxx"</strong></span>

<span class="strong"><strong>    System.setProperty("twitter4j.oauth.consumerKey", consumerKey)</strong></span>
<span class="strong"><strong>    System.setProperty("twitter4j.oauth.consumerSecret", consumerSecret)</strong></span>
<span class="strong"><strong>    System.setProperty("twitter4j.oauth.accessToken", accessToken)</strong></span>
<span class="strong"><strong>    System.setProperty("twitter4j.oauth.accessTokenSecret", accessTokenSecret)</strong></span>

<span class="strong"><strong>    val ssc = StreamingContext.getOrCreate(hdfsDir,</strong></span>
<span class="strong"><strong>      () =&gt; { createContext( hdfsDir ) })</strong></span>

<span class="strong"><strong>    val stream = TwitterUtils.createStream(ssc,None).window( Seconds(60) )</strong></span>

<span class="strong"><strong>    // do some processing</strong></span>

<span class="strong"><strong>    ssc.start()</strong></span>
<span class="strong"><strong>    ssc.awaitTermination()</strong></span>

<span class="strong"><strong>  } // end main</strong></span>
</pre></div><p>Having run this code, which has no actual processing, the HDFS <code class="literal">checkpoint</code> directory can be checked<a id="id152" class="indexterm"></a> again. This time it is apparent that the <code class="literal">checkpoint</code> directory has been created, and the data has been stored:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn stream]$ hdfs dfs -ls /data/spark/checkpoint</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hadoop supergroup          0 2015-07-02 13:41 /data/spark/checkpoint/0fc3d94e-6f53-40fb-910d-1eef044b12e9</strong></span>
</pre></div><p>This example, taken from the Apache Spark website, shows how checkpoint storage can be set up and used. But how often is checkpointing carried out? The Meta data is stored during each stream batch. The actual data is stored with a period, which is the maximum of the batch interval, or ten seconds. This might not be ideal for you, so you can reset the value using the method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>DStream.checkpoint( newRequiredInterval )</strong></span>
</pre></div><p>Where <code class="literal">newRequiredInterval</code> is the new checkpoint interval value that you require, generally you should aim for a value which is five to ten times your batch interval.</p><p>Checkpointing saves both the stream batch and metadata (data about the data). If the application fails, then when it restarts, the checkpointed data is used when processing is started. The batch data that was being processed at the time of failure is reprocessed, along with the batched data since the failure.</p><p>Remember to<a id="id153" class="indexterm"></a> monitor the HDFS disk space being used for check pointing. In the next section, I will begin to examine the streaming sources, and will provide some examples of each type.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec21"></a>Streaming sources</h2></div></div><hr /></div><p>I will not be able to <a id="id154" class="indexterm"></a>cover all the stream types with practical examples in this section, but where this chapter is too small to include code, I will at least<a id="id155" class="indexterm"></a> provide a description. In this chapter, I will cover the TCP and file streams, and the Flume, Kafka, and Twitter streams. I will start with a practical TCP-based example.</p><p>This chapter examines stream processing architecture. For instance, what happens in cases where the stream data delivery rate exceeds the potential data processing rate? Systems like Kafka provide the possibility of solving this issue by providing the ability to use multiple data topics and consumers.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec33"></a>TCP stream</h3></div></div></div><p>There is a possibility of using the Spark <a id="id156" class="indexterm"></a>streaming context method called <code class="literal">socketTextStream</code> to stream data via TCP/IP, by specifying a hostname and a port number. The Scala-based code example in this section will receive data on port <code class="literal">10777</code> that was supplied using the <code class="literal">netcat</code> Linux command. The code sample starts by defining the package name, and importing Spark, the context, and the streaming classes. The object class named <code class="literal">stream2</code> is defined, as it is the main method with arguments:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>package nz.co.semtechsolutions</strong></span>

<span class="strong"><strong>import org.apache.spark._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.StreamingContext._</strong></span>

<span class="strong"><strong>object stream2 {</strong></span>

<span class="strong"><strong>  def main(args: Array[String]) {</strong></span>
</pre></div><p>The number of arguments passed to the class is checked to ensure that it is the hostname and the port number. A<a id="id157" class="indexterm"></a> Spark configuration object is created with an application name defined. The Spark and streaming contexts are then created. Then, a streaming batch time of 10 seconds is set:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    if ( args.length &lt; 2 )</strong></span>
<span class="strong"><strong>    {</strong></span>
<span class="strong"><strong>      System.err.println("Usage: stream2 &lt;host&gt; &lt;port&gt;")</strong></span>
<span class="strong"><strong>      System.exit(1)</strong></span>
<span class="strong"><strong>    }</strong></span>

<span class="strong"><strong>    val hostname = args(0).trim</strong></span>
<span class="strong"><strong>    val portnum  = args(1).toInt</strong></span>

<span class="strong"><strong>    val appName = "Stream example 2"</strong></span>
<span class="strong"><strong>    val conf    = new SparkConf()</strong></span>

<span class="strong"><strong>    conf.setAppName(appName)</strong></span>

<span class="strong"><strong>    val sc  = new SparkContext(conf)</strong></span>
<span class="strong"><strong>    val ssc = new StreamingContext(sc, Seconds(10) )</strong></span>
</pre></div><p>A DStream called <code class="literal">rawDstream</code> is created by calling the <code class="literal">socketTextStream</code> method of the streaming context using the host and port name parameters.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val rawDstream = ssc.socketTextStream( hostname, portnum )</strong></span>
</pre></div><p>A top-ten word count is created from the raw stream data by splitting words by spacing. Then a (key,value) pair is created as <code class="literal">(word,1)</code>, which is reduced by the key value, this being the word. So now, there is a list of words and their associated counts. Now, the key and value are swapped, so the list becomes (<code class="literal">count</code> and <code class="literal">word</code>). Then, a sort is done on the key, which is now the count. Finally, the top 10 items in the <code class="literal">rdd</code>, within the DStream, are taken and printed out:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val wordCount = rawDstream</strong></span>
<span class="strong"><strong>                     .flatMap(line =&gt; line.split(" "))</strong></span>
<span class="strong"><strong>                     .map(word =&gt; (word,1))</strong></span>
<span class="strong"><strong>                     .reduceByKey(_+_)</strong></span>
<span class="strong"><strong>                     .map(item =&gt; item.swap)</strong></span>
<span class="strong"><strong>                     .transform(rdd =&gt; rdd.sortByKey(false))</strong></span>
<span class="strong"><strong>                     .foreachRDD( rdd =&gt;</strong></span>
<span class="strong"><strong>                       { rdd.take(10).foreach(x=&gt;println("List : " + x)) })</strong></span>
</pre></div><p>The code closes with the Spark Streaming start, and <code class="literal">awaitTermination</code> methods being called to start the<a id="id158" class="indexterm"></a> stream processing and await process termination:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    ssc.start()</strong></span>
<span class="strong"><strong>    ssc.awaitTermination()</strong></span>

<span class="strong"><strong>  } // end main</strong></span>

<span class="strong"><strong>} // end stream2</strong></span>
</pre></div><p>The data for this application is provided, as I stated previously, by the Linux <code class="literal">netcat</code> (<code class="literal">nc</code>) command. The Linux <code class="literal">cat</code> command dumps the contents of a log file, which is piped to <code class="literal">nc</code>. The <code class="literal">lk</code> options force <code class="literal">netcat</code> to listen for connections, and keep on listening if the connection is lost. This example shows that the port being used is <code class="literal">10777</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn log]# pwd</strong></span>
<span class="strong"><strong>/var/log</strong></span>
<span class="strong"><strong>[root@hc2nn log]# cat ./anaconda.storage.log | nc -lk 10777</strong></span>
</pre></div><p>The output from this TCP-based stream processing is shown here. The actual output is not as important as the method demonstrated. However, the data shows, as expected, a list of 10 log file words in descending count order. Note that the top word is empty because the stream was not filtered for empty words:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>List : (17104,)</strong></span>
<span class="strong"><strong>List : (2333,=)</strong></span>
<span class="strong"><strong>List : (1656,:)</strong></span>
<span class="strong"><strong>List : (1603,;)</strong></span>
<span class="strong"><strong>List : (1557,DEBUG)</strong></span>
<span class="strong"><strong>List : (564,True)</strong></span>
<span class="strong"><strong>List : (495,False)</strong></span>
<span class="strong"><strong>List : (411,None)</strong></span>
<span class="strong"><strong>List : (356,at)</strong></span>
<span class="strong"><strong>List : (335,object)</strong></span>
</pre></div><p>This is interesting if you want to stream data using Apache Spark streaming, based upon TCP/IP from a host and port. But what about more exotic methods? What if you wish to stream data from a messaging system, or via memory-based channels? What if you want to use some of the big data tools available today like Flume and Kafka? The next sections will examine these<a id="id159" class="indexterm"></a> options, but first I will demonstrate how streams can be based upon files.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec34"></a>File streams</h3></div></div></div><p>I have modified the Scala-based code example in the last section, to monitor an HDFS-based directory, by calling the Spark streaming context method called <code class="literal">textFileStream</code>. I will not display all of<a id="id160" class="indexterm"></a> the code, given this small change. The application class is now called <code class="literal">stream3</code>, which takes a single parameter—the <code class="literal">HDFS</code> directory. The directory path could be on NFS or AWS S3 (all the code samples will be available with this book):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val rawDstream = ssc.textFileStream( directory )</strong></span>
</pre></div><p>The stream processing is the same as before. The stream is split into words, and the top-ten word list is printed. The only difference this time is that the data must be put into the <code class="literal">HDFS</code> directory while the application is running. This is achieved with the HDFS file system <code class="literal">put</code> command here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn log]# hdfs dfs -put ./anaconda.storage.log /data/spark/stream</strong></span>
</pre></div><p>As you can see, the <code class="literal">HDFS</code> directory used is <code class="literal">/data/spark/stream/</code>, and the text-based source log file is <code class="literal">anaconda.storage.log</code> (under <code class="literal">/var/log/</code>). As expected, the same word list and count is printed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>List : (17104,)</strong></span>
<span class="strong"><strong>List : (2333,=)</strong></span>
<span class="strong"><strong>……..</strong></span>
<span class="strong"><strong>List : (564,True)</strong></span>
<span class="strong"><strong>List : (495,False)</strong></span>
<span class="strong"><strong>List : (411,None)</strong></span>
<span class="strong"><strong>List : (356,at)</strong></span>
<span class="strong"><strong>List : (335,object)</strong></span>
</pre></div><p>These are simple streaming methods based on TCP, and file system data. But what if I want to use some of the built-in streaming functionality within Spark streaming? This will be examined next. The Spark streaming Flume library will be used as an example.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec35"></a>Flume</h3></div></div></div><p>Flume is<a id="id161" class="indexterm"></a> an Apache <a id="id162" class="indexterm"></a>open source project and product, which is designed to move large amounts of data at a big data scale. It is highly scalable, distributed, and reliable, working on the basis of data source, data sink, and<a id="id163" class="indexterm"></a> data channels, as the diagram here, taken<a id="id164" class="indexterm"></a> from the <a class="ulink" href="http://flume.apache.org/" target="_blank">http://flume.apache.org/</a> website, shows:</p><div class="mediaobject"><img src="graphics/B01989_03_03.jpg" /></div><p>Flume uses agents to <a id="id165" class="indexterm"></a>process data streams. As can be seen in the previous figure, an agent has a data source, a data processing channel, and a data sink. A clearer way to describe this is via the following figure. The channel acts as a queue for the sourced data and the sink passes the data to the next link in the chain.</p><div class="mediaobject"><img src="graphics/B01989_03_04.jpg" /></div><p>Flume agents can form Flume architectures; the output of one agent's sink can be the input to a second agent. Apache Spark allows two approaches to using Apache Flume. The first is an Avro push-based in-memory approach, whereas the second one, still based on Avro, is a pull-based system, using a custom Spark sink library.</p><p>I installed Flume via<a id="id166" class="indexterm"></a> the Cloudera CDH 5.3 cluster manager, which installs a single agent. Checking the Linux command line, I can see that Flume version 1.5 is now available:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# flume-ng version</strong></span>
<span class="strong"><strong>Flume 1.5.0-cdh5.3.3</strong></span>
<span class="strong"><strong>Source code repository: https://git-wip-us.apache.org/repos/asf/flume.git</strong></span>
<span class="strong"><strong>Revision: b88ce1fd016bc873d817343779dfff6aeea07706</strong></span>
<span class="strong"><strong>Compiled by jenkins on Wed Apr  8 14:57:43 PDT 2015</strong></span>
<span class="strong"><strong>From source with checksum 389d91c718e03341a2367bf4ef12428e</strong></span>
</pre></div><p>The Flume-based <a id="id167" class="indexterm"></a>Spark example that I will initially implement here, is the Flume-based push approach, where Spark acts as a receiver, and Flume pushes the data to Spark. The following figure represents the structure that I will implement on a single node:</p><div class="mediaobject"><img src="graphics/B01989_03_05.jpg" /></div><p>The message data will be sent to port <code class="literal">10777</code> on a host called <code class="literal">hc2r1m1</code> using the Linux <code class="literal">netcat</code> (<code class="literal">nc</code>) command. This will act as a source (<code class="literal">source1</code>) for the Flume agent (<code class="literal">agent1</code>), which will have an in-memory channel called <code class="literal">channel1</code>. The sink used by <code class="literal">agent1</code> will be Apache Avro based, again on a host called <code class="literal">hc2r1m1</code>, but this time, the port number will be <code class="literal">11777</code>. The Apache Spark Flume application <code class="literal">stream4</code> (which I will describe shortly) will listen for Flume stream data on this port.</p><p>I start the streaming process by executing the <code class="literal">netcat</code> (<code class="literal">nc</code>) command next, against the <code class="literal">10777</code> port. Now, when I type text into this window, it will be used as a Flume source, and the data will be sent to the Spark application:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ nc  hc2r1m1.semtech-solutions.co.nz  10777</strong></span>
</pre></div><p>In order to run my Flume<a id="id168" class="indexterm"></a> agent, <code class="literal">agent1</code>, I have created a Flume configuration file called <code class="literal">agent1.flume.cfg</code>, which describes the agent's source, channel, and sink. The contents of the file are as follows. The first section defines the <code class="literal">agent1</code> source, channel, and sink names.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>agent1.sources  = source1</strong></span>
<span class="strong"><strong>agent1.channels = channel1</strong></span>
<span class="strong"><strong>agent1.sinks    = sink1</strong></span>
</pre></div><p>The next section defines <code class="literal">source1</code> to be netcat based, running on the host called <code class="literal">hc2r1m1</code>, and <code class="literal">10777</code> port:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>agent1.sources.source1.channels=channel1</strong></span>
<span class="strong"><strong>agent1.sources.source1.type=netcat</strong></span>
<span class="strong"><strong>agent1.sources.source1.bind=hc2r1m1.semtech-solutions.co.nz</strong></span>
<span class="strong"><strong>agent1.sources.source1.port=10777</strong></span>
</pre></div><p>The <code class="literal">agent1</code> <a id="id169" class="indexterm"></a>channel, <code class="literal">channel1</code>, is defined as a memory-based channel with a maximum event capacity of 1000 events:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>agent1.channels.channel1.type=memory</strong></span>
<span class="strong"><strong>agent1.channels.channel1.capacity=1000</strong></span>
</pre></div><p>Finally, the <code class="literal">agent1</code> sink, <code class="literal">sink1</code>, is defined as an Apache Avro sink on the host called <code class="literal">hc2r1m1</code>, and <code class="literal">11777</code> port:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>agent1.sinks.sink1.type=avro</strong></span>
<span class="strong"><strong>agent1.sinks.sink1.hostname=hc2r1m1.semtech-solutions.co.nz</strong></span>
<span class="strong"><strong>agent1.sinks.sink1.port=11777</strong></span>
<span class="strong"><strong>agent1.sinks.sink1.channel=channel1</strong></span>
</pre></div><p>I have created a Bash script called <code class="literal">flume.bash</code> to run the Flume agent, <code class="literal">agent1</code>. It looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ more flume.bash</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong># run the bash agent</strong></span>

<span class="strong"><strong>flume-ng agent \</strong></span>
<span class="strong"><strong>  --conf /etc/flume-ng/conf \</strong></span>
<span class="strong"><strong>  --conf-file ./agent1.flume.cfg \</strong></span>
<span class="strong"><strong>  -Dflume.root.logger=DEBUG,INFO,console  \</strong></span>
<span class="strong"><strong>  -name agent1</strong></span>
</pre></div><p>The script calls<a id="id170" class="indexterm"></a> the Flume executable <code class="literal">flume-ng</code>, passing the <code class="literal">agent1</code> configuration file. The call specifies the agent named <code class="literal">agent1</code>. It also specifies the Flume configuration directory to be <code class="literal">/etc/flume-ng/conf/</code>, the default value. Initially, I will use a <code class="literal">netcat</code> Flume source with a Scala-based example to show how data can be sent to an Apache Spark application. Then, I will show how an RSS-based <a id="id171" class="indexterm"></a>data feed can be processed in a similar way. So initially, the Scala code that will receive the <code class="literal">netcat</code> data looks like this. The class package name and the application class name are defined. The necessary classes for Spark and Flume are imported. Finally, the main method is defined:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>package nz.co.semtechsolutions</strong></span>

<span class="strong"><strong>import org.apache.spark._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.StreamingContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.flume._</strong></span>

<span class="strong"><strong>object stream4 {</strong></span>

<span class="strong"><strong>  def main(args: Array[String]) {</strong></span>
</pre></div><p>The host and port name arguments for the data stream are checked and extracted:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    if ( args.length &lt; 2 )</strong></span>
<span class="strong"><strong>    {</strong></span>
<span class="strong"><strong>      System.err.println("Usage: stream4 &lt;host&gt; &lt;port&gt;")</strong></span>
<span class="strong"><strong>      System.exit(1)</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>    val hostname = args(0).trim</strong></span>
<span class="strong"><strong>    val portnum  = args(1).toInt</strong></span>

<span class="strong"><strong>    println("hostname : " + hostname)</strong></span>
<span class="strong"><strong>    println("portnum  : " + portnum)</strong></span>
</pre></div><p>The Spark and streaming contexts are created. Then, the Flume-based data stream is created using the stream context host and port number. The Flume-based class <code class="literal">FlumeUtils</code> has been used to do this by calling it's <code class="literal">createStream</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val appName = "Stream example 4"</strong></span>
<span class="strong"><strong>    val conf    = new SparkConf()</strong></span>

<span class="strong"><strong>    conf.setAppName(appName)</strong></span>

<span class="strong"><strong>    val sc  = new SparkContext(conf)</strong></span>
<span class="strong"><strong>    val ssc = new StreamingContext(sc, Seconds(10) )</strong></span>

<span class="strong"><strong>    val rawDstream = FlumeUtils.createStream(ssc,hostname,portnum)</strong></span>
</pre></div><p>Finally, a stream event <a id="id172" class="indexterm"></a>count is printed, and (for debug purposes while we test the stream) the stream content is dumped. After this, the stream context is started and configured to run until terminated via the application:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    rawDstream.count()</strong></span>
<span class="strong"><strong>         .map(cnt =&gt; "&gt;&gt;&gt;&gt; Received events : " + cnt )</strong></span>
<span class="strong"><strong>         .print()</strong></span>

<span class="strong"><strong>    rawDstream.map(e =&gt; new String(e.event.getBody.array() ))</strong></span>
<span class="strong"><strong>              .print</strong></span>

<span class="strong"><strong>    ssc.start()</strong></span>
<span class="strong"><strong>    ssc.awaitTermination()</strong></span>

<span class="strong"><strong>  } // end main</strong></span>
<span class="strong"><strong>} // end stream4</strong></span>
</pre></div><p>Having compiled it, I will<a id="id173" class="indexterm"></a> run this application using <code class="literal">spark-submit</code>. In the other chapters of this book, I will use a Bash-based script called <code class="literal">run_stream.bash</code> to execute the job. The script looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ more run_stream.bash</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>SPARK_HOME=/usr/local/spark</strong></span>
<span class="strong"><strong>SPARK_BIN=$SPARK_HOME/bin</strong></span>
<span class="strong"><strong>SPARK_SBIN=$SPARK_HOME/sbin</strong></span>

<span class="strong"><strong>JAR_PATH=/home/hadoop/spark/stream/target/scala-2.10/streaming_2.10-1.0.jar</strong></span>
<span class="strong"><strong>CLASS_VAL=$1</strong></span>
<span class="strong"><strong>CLASS_PARAMS="${*:2}"</strong></span>

<span class="strong"><strong>STREAM_JAR=/usr/local/spark/lib/spark-examples-1.3.1-hadoop2.3.0.jar</strong></span>

<span class="strong"><strong>cd $SPARK_BIN</strong></span>

<span class="strong"><strong>./spark-submit \</strong></span>
<span class="strong"><strong>  --class $CLASS_VAL \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 100M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 50 \</strong></span>
<span class="strong"><strong>  --jars $STREAM_JAR \</strong></span>
<span class="strong"><strong>  $JAR_PATH \</strong></span>
<span class="strong"><strong>  $CLASS_PARAMS</strong></span>
</pre></div><p>So, this script sets some Spark-based variables, and a JAR library path for this job. It takes which Spark class to run, as its first parameter. It passes all the other variables, as parameters, to the Spark application class job. So, the execution of the application looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ ./run_stream.bash  \</strong></span>
<span class="strong"><strong>                     nz.co.semtechsolutions.stream4 \</strong></span>
<span class="strong"><strong>                     hc2r1m1.semtech-solutions.co.nz  \</strong></span>
<span class="strong"><strong>                     11777</strong></span>
</pre></div><p>This means that the Spark application is ready, and is running as a Flume sink on port <code class="literal">11777</code>. The Flume input<a id="id174" class="indexterm"></a> is ready, running as a netcat task on port <code class="literal">10777</code>. Now, the Flume agent, <code class="literal">agent1</code>, can be started using the Flume script called <code class="literal">flume.bash</code> to send the netcat source-based data to the Apache Spark Flume-based sink:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ ./flume.bash</strong></span>
</pre></div><p>Now, when the text is passed to the netcat session, it should flow through Flume, and be processed as a stream by Spark. Let's try it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ nc  hc2r1m1.semtech-solutions.co.nz 10777</strong></span>
<span class="strong"><strong>I hope that Apache Spark will print this</strong></span>
<span class="strong"><strong>OK</strong></span>
<span class="strong"><strong>I hope that Apache Spark will print this</strong></span>
<span class="strong"><strong>OK</strong></span>
<span class="strong"><strong>I hope that Apache Spark will print this</strong></span>
<span class="strong"><strong>OK</strong></span>
</pre></div><p>Three simple pieces of text have been added to the netcat session, and have been acknowledged with an <code class="literal">OK</code>, so that they can be passed to Flume. The debug output in the Flume session shows<a id="id175" class="indexterm"></a> that the events (one per line ) have been received and processed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>2015-07-06 18:13:18,699 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41</strong></span>
<span class="strong"><strong>2015-07-06 18:13:18,700 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1</strong></span>
<span class="strong"><strong>2015-07-06 18:13:18,990 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41</strong></span>
<span class="strong"><strong>2015-07-06 18:13:18,991 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1</strong></span>
<span class="strong"><strong>2015-07-06 18:13:19,270 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41</strong></span>
<span class="strong"><strong>2015-07-06 18:13:19,271 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1</strong></span>
</pre></div><p>Finally, in the Spark <code class="literal">stream4</code> application session, three events have been received and processed. In this case, dumped to the session to prove the point that the data arrived. Of course, this is not what you <a id="id176" class="indexterm"></a>would normally do, but I wanted to prove data transit through this configuration:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1436163210000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; Received events : 3</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1436163210000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>I hope that Apache Spark will print this</strong></span>
<span class="strong"><strong>I hope that Apache Spark will print this</strong></span>
<span class="strong"><strong>I hope that Apache Spark will print this</strong></span>
</pre></div><p>This is interesting, but it is not really a production-worthy example of Spark Flume data processing. So, in order to demonstrate a potentially real data processing approach, I will change the Flume<a id="id177" class="indexterm"></a> configuration file source details so that it uses a Perl script, which is executable as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>agent1.sources.source1.type=exec</strong></span>
<span class="strong"><strong>agent1.sources.source.command=./rss.perl</strong></span>
</pre></div><p>The Perl script, which is referenced previously, <code class="literal">rss.perl</code>, just acts as a source of Reuters science news. It receives the news as XML, and converts it into JSON format. It also cleans the data<a id="id178" class="indexterm"></a> of unwanted noise. First, it imports packages like LWP and <code class="literal">XML::XPath</code> to enable XML processing. Then, it specifies a science-based Reuters news data source, and creates a new LWP agent to process the data, similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/usr/bin/perl</strong></span>

<span class="strong"><strong>use strict;</strong></span>
<span class="strong"><strong>use LWP::UserAgent;</strong></span>
<span class="strong"><strong>use XML::XPath;</strong></span>

<span class="strong"><strong>my $urlsource="http://feeds.reuters.com/reuters/scienceNews" ;</strong></span>

<span class="strong"><strong>my  $agent = LWP::UserAgent-&gt;new;</strong></span>
</pre></div><p>Then an infinite while loop is opened, and an HTTP <code class="literal">GET</code> request is carried out against the URL. The request is configured, and the agent makes the request via a call to the request method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>while()</strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  my  $req = HTTP::Request-&gt;new(GET =&gt; ($urlsource));</strong></span>

<span class="strong"><strong>  $req-&gt;header('content-type' =&gt; 'application/json');</strong></span>
<span class="strong"><strong>  $req-&gt;header('Accept'       =&gt; 'application/json');</strong></span>

<span class="strong"><strong>  my $resp = $agent-&gt;request($req);</strong></span>
</pre></div><p>If the request is successful, then the XML data returned, is defined as the decoded content of the request. Title information is extracted from the XML, via an XPath call using the path called <code class="literal">/rss/channel/item/title</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  if ( $resp-&gt;is_success )</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    my $xmlpage = $resp -&gt; decoded_content;</strong></span>

<span class="strong"><strong>    my $xp = XML::XPath-&gt;new( xml =&gt; $xmlpage );</strong></span>
<span class="strong"><strong>    my $nodeset = $xp-&gt;find( '/rss/channel/item/title' );</strong></span>

<span class="strong"><strong>    my @titles = () ;</strong></span>
<span class="strong"><strong>    my $index = 0 ;</strong></span>
</pre></div><p>For each node in the extracted title data title XML string, data is extracted. It is cleaned of unwanted XML tags, and added to a Perl-based array called <code class="literal">titles</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    foreach my $node ($nodeset-&gt;get_nodelist)</strong></span>
<span class="strong"><strong>    {</strong></span>
<span class="strong"><strong>      my $xmlstring = XML::XPath::XMLParser::as_string($node) ;</strong></span>

<span class="strong"><strong>       $xmlstring =~ s/&lt;title&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;\/title&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/"//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/,//g;</strong></span>

<span class="strong"><strong>       $titles[$index] = $xmlstring ;</strong></span>
<span class="strong"><strong>       $index = $index + 1 ;</strong></span>

<span class="strong"><strong>    } # foreach find node</strong></span>
</pre></div><p>The same process is<a id="id179" class="indexterm"></a> carried out for description-based <a id="id180" class="indexterm"></a>data in the request response XML. The XPath value used this time is <code class="literal">/rss/channel/item/description/</code>. There are many more tags to be cleaned from the description data, so there are many more Perl searches, and line replacements that act on this data (<code class="literal">s///g</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    my $nodeset = $xp-&gt;find( '/rss/channel/item/description' );</strong></span>

<span class="strong"><strong>    my @desc = () ;</strong></span>
<span class="strong"><strong>    $index = 0 ;</strong></span>

<span class="strong"><strong>    foreach my $node ($nodeset-&gt;get_nodelist)</strong></span>
<span class="strong"><strong>    {</strong></span>
<span class="strong"><strong>       my $xmlstring = XML::XPath::XMLParser::as_string($node) ;</strong></span>

<span class="strong"><strong>       $xmlstring =~ s/&lt;img.+\/img&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/href=".+"//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/src=".+"//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/src='.+'//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;br.+\/&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;\/div&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;\/a&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;a &gt;\n//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;img &gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;img \/&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;div.+&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;title&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;\/title&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;description&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&lt;\/description&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/&amp;lt;.+&gt;//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/"//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/,//g;</strong></span>
<span class="strong"><strong>       $xmlstring =~ s/\r|\n//g;</strong></span>

<span class="strong"><strong>       $desc[$index] = $xmlstring ;</strong></span>
<span class="strong"><strong>       $index = $index + 1 ;</strong></span>

<span class="strong"><strong>    } # foreach find node</strong></span>
</pre></div><p>Finally, the XML-based<a id="id181" class="indexterm"></a> title and description data<a id="id182" class="indexterm"></a> is output in the RSS JSON format using a <code class="literal">print</code> command. The script then sleeps for 30 seconds, and requests more RSS news information to process:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    my $newsitems = $index ;</strong></span>
<span class="strong"><strong>    $index = 0 ;</strong></span>

<span class="strong"><strong>    for ($index=0; $index &lt; $newsitems; $index++) {</strong></span>

<span class="strong"><strong>      print "{\"category\": \"science\","</strong></span>
<span class="strong"><strong>            . " \"title\": \"" .  $titles[$index] . "\","</strong></span>
<span class="strong"><strong>            . " \"summary\": \"" .  $desc[$index] . "\""</strong></span>
<span class="strong"><strong>             . "}\n";</strong></span>

<span class="strong"><strong>    } # for rss items</strong></span>

<span class="strong"><strong>  } # success ?</strong></span>

<span class="strong"><strong>  sleep(30) ;</strong></span>

<span class="strong"><strong>} # while</strong></span>
</pre></div><p>I have created a second Scala-based stream processing code example called <code class="literal">stream5</code>. It is similar to the <code class="literal">stream4</code> example, but it now processes the <code class="literal">rss</code> item data from the stream. A case class is defined next to process the category, title, and summary from the XML <code class="literal">rss</code> information. An html location is defined to store the resulting data that comes from the Flume channel:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    case class RSSItem(category : String, title : String, summary : String)</strong></span>

<span class="strong"><strong>    val now: Long = System.currentTimeMillis</strong></span>

<span class="strong"><strong>    val hdfsdir = "hdfs://hc2nn:8020/data/spark/flume/rss/"</strong></span>
</pre></div><p>The <code class="literal">rss</code> stream data<a id="id183" class="indexterm"></a> from the Flume-based event is<a id="id184" class="indexterm"></a> converted into a string. It is then formatted using the case class called <code class="literal">RSSItem</code>. If there is event data, it is then written to an HDFS directory using the previous <code class="literal">hdfsdir</code> path:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>        rawDstream.map(record =&gt; {</strong></span>
<span class="strong"><strong>        implicit val formats = DefaultFormats</strong></span>
<span class="strong"><strong>        read[RSSItem](new String(record.event.getBody().array()))</strong></span>
<span class="strong"><strong>    })</strong></span>
<span class="strong"><strong>         .foreachRDD(rdd =&gt; {</strong></span>
<span class="strong"><strong>            if (rdd.count() &gt; 0) {</strong></span>
<span class="strong"><strong>              rdd.map(item =&gt; {</strong></span>
<span class="strong"><strong>                implicit val formats = DefaultFormats</strong></span>
<span class="strong"><strong>                write(item)</strong></span>
<span class="strong"><strong>                  }).saveAsTextFile(hdfsdir+"file_"+now.toString())</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>    })</strong></span>
</pre></div><p>Running this code sample, it is possible to see that the Perl <code class="literal">rss</code> script is producing data, because the Flume script output indicates that 80 events have been accepted and received:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>2015-07-07 14:14:24,017 (agent-shutdown-hook) [DEBUG - org.apache.flume.source.ExecSource.stop(ExecSource.java:219)] Exec source with command:./news_rss_collector.py stopped. Metrics:SOURCE:source1{src.events.accepted=80, src.events.received=80, src.append.accepted=0, src.append-batch.accepted=0, src.open-connection.count=0, src.append-batch.received=0, src.append.received=0}</strong></span>
</pre></div><p>The Scala Spark application <code class="literal">stream5</code> has processed 80 events in two batches:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;&gt; Received events : 73</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;&gt; Received events : 7</strong></span>
</pre></div><p>And the events have been stored on HDFS, under the expected directory, as the Hadoop file system <code class="literal">ls</code> command shows here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ hdfs dfs -ls /data/spark/flume/rss/</strong></span>
<span class="strong"><strong>Found 2 items</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hadoop supergroup          0 2015-07-07 14:09 /data/spark/flume/rss/file_1436234439794</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hadoop supergroup          0 2015-07-07 14:14 /data/spark/flume/rss/file_1436235208370</strong></span>
</pre></div><p>Also, using the <a id="id185" class="indexterm"></a>Hadoop file system <code class="literal">cat</code> command, it is possible to prove that the files on HDFS contain rss feed news-based data as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$  hdfs dfs -cat /data/spark/flume/rss/file_1436235208370/part-00000 | head -1</strong></span>

<span class="strong"><strong>{"category":"healthcare","title":"BRIEF-Aetna CEO says has not had specific conversations with DOJ on Humana - CNBC","summary":"* Aetna CEO Says Has Not Had Specific Conversations With Doj About Humana Acquisition - CNBC"}</strong></span>
</pre></div><p>This Spark stream-based<a id="id186" class="indexterm"></a> example has used Apache Flume to transmit data from an rss source, through Flume, to HDFS via a Spark consumer. This is a good example, but what if you want to publish data to a group of consumers? In the next section, I will examine Apache Kafka—a publish subscribe messaging system, and determine how it can be used with Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec36"></a>Kafka</h3></div></div></div><p>Apache Kafka (<a class="ulink" href="http://kafka.apache.org/" target="_blank">http://kafka.apache.org/</a>) is a top<a id="id187" class="indexterm"></a> level open-source project in Apache. It is a big <a id="id188" class="indexterm"></a>data publish/subscribe messaging system that is fast and highly scalable. It uses<a id="id189" class="indexterm"></a> message brokers for data management, and ZooKeeper for configuration, so that data can be organized into consumer groups and topics. Data in Kafka is split into partitions. In this example, I will demonstrate a receiver-less Spark-based Kafka consumer, so that I don't need to worry about configuring Spark data partitions when compared to my Kafka data.</p><p>In order to demonstrate Kafka-based message production and consumption, I will use the Perl RSS script from the last section as a data source. The data passing into Kafka and onto Spark will be Reuters RSS news data in the JSON format.</p><p>As topic messages are created by message producers, they are then placed in partitions in message order sequence. The messages in the partitions are retained for a configurable time period. Kafka then stores the offset value for each consumer, which is that consumer's position (in terms of message consumption) in that partition.</p><p>I am currently using<a id="id190" class="indexterm"></a> Cloudera's CDH 5.3 Hadoop cluster. In order to install Kafka, I need to download a Kafka JAR<a id="id191" class="indexterm"></a> library file from: <a class="ulink" href="http://archive.cloudera.com/csds/kafka/" target="_blank">http://archive.cloudera.com/csds/kafka/</a>.</p><p>Having downloaded the file, and given that I am using CDH cluster manager, I then need to copy the file to the <code class="literal">/opt/cloudera/csd/</code> directory on my NameNode CentOS server, so that it will be visible to install:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn csd]# pwd</strong></span>
<span class="strong"><strong>/opt/cloudera/csd</strong></span>

<span class="strong"><strong>[root@hc2nn csd]# ls -l KAFKA-1.2.0.jar</strong></span>
<span class="strong"><strong>-rw-r--r-- 1 hadoop hadoop 5670 Jul 11 14:56 KAFKA-1.2.0.jar</strong></span>
</pre></div><p>I then need to restart the Cloudera cluster manager server on my NameNode, or master server, so that the change will be recognized. This was done as root using the service command, which is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn hadoop]# service cloudera-scm-server restart</strong></span>
<span class="strong"><strong>Stopping cloudera-scm-server:                              [  OK  ]</strong></span>
<span class="strong"><strong>Starting cloudera-scm-server:                              [  OK  ]</strong></span>
</pre></div><p>Now, the Kafka parcel should be visible within the CDH manager under <span class="strong"><strong>Hosts</strong></span> | <span class="strong"><strong>Parcels</strong></span>, as shown in the following figure. You can follow the usual download, distribution, and activate cycle for the CDH parcel installation:</p><div class="mediaobject"><img src="graphics/B01989_03_06.jpg" /></div><p>I have installed Kafka message brokers on each Data Node, or Spark Slave machine in my cluster. I then set the Kafka broker ID values for each Kafka broker server, giving them a <code class="literal">broker.id</code> number of 1 through 4. As Kafka uses ZooKeeper for cluster data configuration, I wanted to keep all the Kafka data in a top level node called <code class="literal">kafka</code> in ZooKeeper. In order to<a id="id192" class="indexterm"></a> do this, I set the Kafka ZooKeeper root value, called <code class="literal">zookeeper.chroot</code>, to <code class="literal">/kafka</code>. After making these changes, I restarted the CDH Kafka servers for the changes to take effect.</p><p>With Kafka installed, I can check the scripts available for testing. The following listing shows Kafka-based scripts for message producers and consumers, as well as scripts for managing topics, and checking consumer offsets. These scripts will be used in this section in order to demonstrate Kafka functionality:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ ls /usr/bin/kafka*</strong></span>

<span class="strong"><strong>/usr/bin/kafka-console-consumer         /usr/bin/kafka-run-class</strong></span>
<span class="strong"><strong>/usr/bin/kafka-console-producer         /usr/bin/kafka-topics</strong></span>
<span class="strong"><strong>/usr/bin/kafka-consumer-offset-checker</strong></span>
</pre></div><p>In order to run the installed<a id="id193" class="indexterm"></a> Kafka servers, I need to have the broker server ID's (<code class="literal">broker.id</code>) values set, else an error will occur. Once Kafka is installed and running, I will need to prepare a message producer script. The simple Bash script given next, called <code class="literal">kafka.bash</code>, defines a comma-separated broker list of hosts and ports. It also defines a topic called <code class="literal">rss</code>. It then calls the Perl script <code class="literal">rss.perl</code> to generate the RSS-based data. This data is then piped into the Kafka producer script called <code class="literal">kafka-console-producer</code> to be sent to Kafka.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ more kafka.bash</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>BROKER_LIST="hc2r1m1:9092,hc2r1m2:9092,hc2r1m3:9092,hc2r1m4:9092"</strong></span>
<span class="strong"><strong>TOPIC="rss"</strong></span>

<span class="strong"><strong>./rss.perl | /usr/bin/kafka-console-producer --broker-list $BROKER_LIST --topic $TOPIC</strong></span>
</pre></div><p>Notice that I have not mentioned Kafka topics at this point. When a topic is created in Kafka, the number of <a id="id194" class="indexterm"></a>partitions can be specified. In the following example, the <code class="literal">kafka-topics</code> script has been called with the <code class="literal">create</code> option. The number of partitions have been set to <code class="literal">5</code>, and the data replication factor has been set to <code class="literal">3</code>. The ZooKeeper server string has been defined as <code class="literal">hc2r1m2-4</code> with a port number of <code class="literal">2181</code>. Also note that the top level ZooKeeper Kafka node has been defined as <code class="literal">/kafka</code> in the ZooKeeper string:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/usr/bin/kafka-topics \</strong></span>
<span class="strong"><strong>  --create  \</strong></span>
<span class="strong"><strong>  --zookeeper hc2r1m2:2181,hc2r1m3:2181,hc2r1m4:2181/kafka \</strong></span>
<span class="strong"><strong>  --replication-factor 3  \</strong></span>
<span class="strong"><strong>  --partitions 5  \</strong></span>
<span class="strong"><strong>  --topic rss</strong></span>
</pre></div><p>I have also created a Bash script called <code class="literal">kafka_list.bash</code> for use during testing, which checks all the Kafka<a id="id195" class="indexterm"></a> topics that have been created, and also the Kafka consumer offsets. It calls the <code class="literal">kafka-topics</code> commands with a <code class="literal">list</code> option, and a <code class="literal">ZooKeeper</code> string to get a list of created topics. It then calls the Kafka script called <code class="literal">kafka-consumer-offset-checker</code> with a <code class="literal">ZooKeeper</code> string—the topic name and a group name to get a list of consumer offset values. Using this script, I can check that my topics are created, and the topic data is being consumed correctly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ cat kafka_list.bash</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>ZOOKEEPER="hc2r1m2:2181,hc2r1m3:2181,hc2r1m4:2181/kafka"</strong></span>
<span class="strong"><strong>TOPIC="rss"</strong></span>
<span class="strong"><strong>GROUP="group1"</strong></span>

<span class="strong"><strong>echo ""</strong></span>
<span class="strong"><strong>echo "================================"</strong></span>
<span class="strong"><strong>echo " Kafka Topics "</strong></span>
<span class="strong"><strong>echo "================================"</strong></span>

<span class="strong"><strong>/usr/bin/kafka-topics --list --zookeeper $ZOOKEEPER</strong></span>

<span class="strong"><strong>echo ""</strong></span>
<span class="strong"><strong>echo "================================"</strong></span>
<span class="strong"><strong>echo " Kafka Offsets "</strong></span>
<span class="strong"><strong>echo "================================"</strong></span>
</pre></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/usr/bin/kafka-consumer-offset-checker \</strong></span>
<span class="strong"><strong>  --group $GROUP \</strong></span>
<span class="strong"><strong>  --topic $TOPIC \</strong></span>
<span class="strong"><strong>  --zookeeper $ZOOKEEPER</strong></span>
</pre></div><p>Next, I need to create the Apache Spark Scala-based Kafka consumer code. As I said, I will create a receiver-less<a id="id196" class="indexterm"></a> example, so that the Kafka data partitions match in both, Kafka and Spark. The example is<a id="id197" class="indexterm"></a> called <code class="literal">stream6</code>. First, the package is defined, and the classes are imported for Kafka, spark, context, and streaming. Then, the object class called <code class="literal">stream6</code>, and the main method are defined. The code looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>package nz.co.semtechsolutions</strong></span>

<span class="strong"><strong>import kafka.serializer.StringDecoder</strong></span>

<span class="strong"><strong>import org.apache.spark._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.StreamingContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.kafka._</strong></span>

<span class="strong"><strong>object stream6 {</strong></span>

<span class="strong"><strong>  def main(args: Array[String]) {</strong></span>
</pre></div><p>Next, the class parameters (broker's string, group ID, and topic) are checked and processed. If the class parameters are incorrect, then an error is printed, and execution stops, else the parameter variables are defined:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    if ( args.length &lt; 3 )</strong></span>
<span class="strong"><strong>    {</strong></span>
<span class="strong"><strong>      System.err.println("Usage: stream6 &lt;brokers&gt; &lt;groupid&gt; &lt;topics&gt;\n")</strong></span>
<span class="strong"><strong>      System.err.println("&lt;brokers&gt; = host1:port1,host2:port2\n")</strong></span>
<span class="strong"><strong>      System.err.println("&lt;groupid&gt; = group1\n")</strong></span>
<span class="strong"><strong>      System.err.println("&lt;topics&gt;  = topic1,topic2\n")</strong></span>
<span class="strong"><strong>      System.exit(1)</strong></span>
<span class="strong"><strong>    }</strong></span>

<span class="strong"><strong>    val brokers = args(0).trim</strong></span>
<span class="strong"><strong>    val groupid = args(1).trim</strong></span>
<span class="strong"><strong>    val topics  = args(2).trim</strong></span>

<span class="strong"><strong>    println("brokers : " + brokers)</strong></span>
<span class="strong"><strong>    println("groupid : " + groupid)</strong></span>
<span class="strong"><strong>    println("topics  : " + topics)</strong></span>
</pre></div><p>The Spark context is defined<a id="id198" class="indexterm"></a> in terms of an application name. Again the Spark URL has been left as the default. The streaming context has been created using the Spark context. I have left the stream batch interval at 10 seconds, which is the same as the last example. However, you can set it using a parameter of your choice:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val appName = "Stream example 6"</strong></span>
<span class="strong"><strong>    val conf    = new SparkConf()</strong></span>

<span class="strong"><strong>    conf.setAppName(appName)</strong></span>

<span class="strong"><strong>    val sc  = new SparkContext(conf)</strong></span>
<span class="strong"><strong>    val ssc = new StreamingContext(sc, Seconds(10) )</strong></span>
</pre></div><p>Next, the broker list and <a id="id199" class="indexterm"></a>group ID are set up as parameters. These values are then used to create a Kafka-based Spark stream called <code class="literal">rawDStream</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val topicsSet = topics.split(",").toSet</strong></span>
<span class="strong"><strong>    val kafkaParams : Map[String, String] =</strong></span>
<span class="strong"><strong>        Map("metadata.broker.list" -&gt; brokers,</strong></span>
<span class="strong"><strong>            "group.id" -&gt; groupid )</strong></span>

<span class="strong"><strong>    val rawDstream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)</strong></span>
</pre></div><p>I have again printed the stream event count for debug purposes, so that I know when the application is receiving<a id="id200" class="indexterm"></a> and processing the data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    rawDstream.count().map(cnt =&gt; "&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Received events : " + cnt ).print()</strong></span>
</pre></div><p>The HDSF location for the Kafka data has been defined as <code class="literal">/data/spark/kafka/rss/</code>. It has been mapped from the DStream into the variable lines. Using the <code class="literal">foreachRDD</code> method, a check on the data count is carried out on the <code class="literal">lines</code> variable, before saving the data into HDFS using the <code class="literal">saveAsTextFile</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val now: Long = System.currentTimeMillis</strong></span>

<span class="strong"><strong>    val hdfsdir = "hdfs://hc2nn:8020/data/spark/kafka/rss/"</strong></span>

<span class="strong"><strong>    val lines = rawDstream.map(record =&gt; record._2)</strong></span>

<span class="strong"><strong>    lines.foreachRDD(rdd =&gt; {</strong></span>
<span class="strong"><strong>            if (rdd.count() &gt; 0) {</strong></span>
<span class="strong"><strong>              rdd.saveAsTextFile(hdfsdir+"file_"+now.toString())</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>    })</strong></span>
</pre></div><p>Finally, the Scala script closes by starting the stream processing, and setting the application class to run until terminated with <code class="literal">awaitTermination</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    ssc.start()</strong></span>
<span class="strong"><strong>    ssc.awaitTermination()</strong></span>

<span class="strong"><strong>  } // end main</strong></span>

<span class="strong"><strong>} // end stream6</strong></span>
</pre></div><p>With all of the scripts explained and the Kafka CDH brokers running, it is time to examine the Kafka configuration, which if you remember is maintained by Apache ZooKeeper (all of the code <a id="id201" class="indexterm"></a>samples that have been described so far will be released with the book). I will use the <code class="literal">zookeeper-client</code> tool, and connect to the <code class="literal">zookeeper</code> server on the host called <code class="literal">hc2r1m2</code> on the <code class="literal">2181</code> port. As you can see here, I have received a connected message from the <code class="literal">client</code> session:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ /usr/bin/zookeeper-client -server hc2r1m2:2181</strong></span>


<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 0]</strong></span>
</pre></div><p>If you remember, I specified the top level ZooKeeper directory for Kafka to be <code class="literal">/kafka</code>. If I examine this now<a id="id202" class="indexterm"></a> via a client session, I can see the Kafka ZooKeeper structure. I will be interested in <code class="literal">brokers</code> (the CDH Kafka broker servers), and <code class="literal">consumers</code> (the previous Spark Scala code). The ZooKeeper <code class="literal">ls</code> commands show that the four Kafka servers have registered with ZooKeeper, and are listed by their <code class="literal">broker.id</code> configuration values one to four:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 2] ls /kafka</strong></span>
<span class="strong"><strong>[consumers, config, controller, admin, brokers, controller_epoch]</strong></span>

<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 3] ls /kafka/brokers</strong></span>
<span class="strong"><strong>[topics, ids]</strong></span>

<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 4] ls /kafka/brokers/ids</strong></span>
<span class="strong"><strong>[3, 2, 1, 4]</strong></span>
</pre></div><p>I will create the topic that I want to use for this test using the Kafka script <code class="literal">kafka-topics</code> with a <code class="literal">create</code> flag. I do<a id="id203" class="indexterm"></a> this manually, because I can demonstrate the definition of the data partitions while I do it. Note that I have set the partitions in the Kafka <code class="literal">topic rss</code> to five as shown in the following piece of code. Note also that the ZooKeeper connection string for the command has a comma-separated list of ZooKeeper servers, terminated by the top level ZooKeeper Kafka directory called <code class="literal">/kafka</code>. This means that the command puts the new topic in the proper place:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ /usr/bin/kafka-topics \</strong></span>
<span class="strong"><strong>&gt;   --create  \</strong></span>
<span class="strong"><strong>&gt;   --zookeeper hc2r1m2:2181,hc2r1m3:2181,hc2r1m4:2181/kafka \</strong></span>
<span class="strong"><strong>&gt;   --replication-factor 3  \</strong></span>
<span class="strong"><strong>&gt;   --partitions 5  \</strong></span>
<span class="strong"><strong>&gt;   --topic rss</strong></span>

<span class="strong"><strong>Created topic "rss".</strong></span>
</pre></div><p>Now, when I use the ZooKeeper client to check the Kafka topic configuration, I can see the correct topic name, and the expected number of the partitions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 5] ls /kafka/brokers/topics</strong></span>
<span class="strong"><strong>[rss]</strong></span>

<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 6] ls /kafka/brokers/topics/rss</strong></span>
<span class="strong"><strong>[partitions]</strong></span>

<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 7] ls /kafka/brokers/topics/rss/partitions</strong></span>
<span class="strong"><strong>[3, 2, 1, 0, 4]</strong></span>
</pre></div><p>This describes the configuration for the Kafka broker servers in ZooKeeper, but what about the data consumers? Well, the following listing shows where the data will be held. Remember though, at this time, there is no consumer running, so it is not represented in ZooKeeper:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 9]  ls /kafka/consumers</strong></span>
<span class="strong"><strong>[]</strong></span>
<span class="strong"><strong>[zk: hc2r1m2:2181(CONNECTED) 10] quit</strong></span>
</pre></div><p>In order to start this test, I will run my Kafka data producer, and consumer scripts. I will also check the output of the Spark <a id="id204" class="indexterm"></a>application class and need to check the Kafka partition offsets and HDFS to make sure that the data has arrived. This is quite complicated, so I will add a diagram here in the following figure to explain the test architecture.</p><p>The Perl script called <code class="literal">rss.perl</code> will be used to provide a data source for a Kafka data producer, which will<a id="id205" class="indexterm"></a> feed data into the CDH Kafka broker servers. The data will be stored in ZooKeeper, in the structure that has just been examined, under the top level node called <code class="literal">/kafka</code>. The Apache Spark Scala-based application will then act as a Kafka consumer, and read the data that it will store under HDFS.</p><div class="mediaobject"><img src="graphics/B01989_03_07.jpg" /></div><p>In order to try and explain the complexity here, I will also examine my method of running the Apache Spark class. It will be started via the <code class="literal">spark-submit</code> command. Remember again that <a id="id206" class="indexterm"></a>all of these scripts will be released with this book, so that you can examine them in your own time. I always use scripts for server test management, so that I encapsulate complexity, and command execution is quickly repeatable. The script, <code class="literal">run_stream.bash</code>, is like many example scripts that have already been used in this chapter, and this book. It accepts a class name and the class parameters, and runs the class via spark-submit:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ more run_stream.bash</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>SPARK_HOME=/usr/local/spark</strong></span>
<span class="strong"><strong>SPARK_BIN=$SPARK_HOME/bin</strong></span>
<span class="strong"><strong>SPARK_SBIN=$SPARK_HOME/sbin</strong></span>

<span class="strong"><strong>JAR_PATH=/home/hadoop/spark/stream/target/scala-2.10/streaming_2.10-1.0.jar</strong></span>
<span class="strong"><strong>CLASS_VAL=$1</strong></span>
<span class="strong"><strong>CLASS_PARAMS="${*:2}"</strong></span>

<span class="strong"><strong>STREAM_JAR=/usr/local/spark/lib/spark-examples-1.3.1-hadoop2.3.0.jar</strong></span>
<span class="strong"><strong>cd $SPARK_BIN</strong></span>

<span class="strong"><strong>./spark-submit \</strong></span>
<span class="strong"><strong>  --class $CLASS_VAL \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 100M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 50 \</strong></span>
<span class="strong"><strong>  --jars $STREAM_JAR \</strong></span>
<span class="strong"><strong>  $JAR_PATH \</strong></span>
<span class="strong"><strong>  $CLASS_PARAMS</strong></span>
</pre></div><p>I then used a second script, which calls the <code class="literal">run_kafka_example.bash</code> script to execute the Kafka consumer <a id="id207" class="indexterm"></a>code in the previous <code class="literal">stream6</code> application class. Note that this script sets up the full application class name—the broker server list. It also sets up the topic name, called <code class="literal">rss</code>, to use for data consumption. Finally, it defines a consumer group called <code class="literal">group1</code>. Remember that Kafka is a publish/subscribe message brokering system. There may be many producers and consumers organized by topic, group, and partition:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ more run_kafka_example.bash</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>RUN_CLASS=nz.co.semtechsolutions.stream6</strong></span>
<span class="strong"><strong>BROKERS="hc2r1m1:9092,hc2r1m2:9092,hc2r1m3:9092,hc2r1m4:9092"</strong></span>
<span class="strong"><strong>GROUPID=group1</strong></span>
<span class="strong"><strong>TOPICS=rss</strong></span>

<span class="strong"><strong># run the Apache Spark Kafka example</strong></span>

<span class="strong"><strong>./run_stream.bash $RUN_CLASS \</strong></span>
<span class="strong"><strong>                  $BROKERS \</strong></span>
<span class="strong"><strong>                  $GROUPID \</strong></span>
<span class="strong"><strong>                  $TOPICS</strong></span>
</pre></div><p>So, I will start the Kafka consumer by running the <code class="literal">run_kafka_example.bash</code> script, which in turn will run the previous <code class="literal">stream6</code> Scala code using spark-submit. While monitoring Kafka data <a id="id208" class="indexterm"></a>consumption using the script called <code class="literal">kafka_list.bash</code>, I was able to get the <code class="literal">kafka-consumer-offset-checker</code> script to list the Kafka-based topics, but for some reason, it will not check the correct path (under <code class="literal">/kafka</code> in ZooKeeper) when checking the offsets as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ ./kafka_list.bash</strong></span>

<span class="strong"><strong>================================</strong></span>
<span class="strong"><strong> Kafka Topics</strong></span>
<span class="strong"><strong>================================</strong></span>
<span class="strong"><strong>__consumer_offsets</strong></span>
<span class="strong"><strong>rss</strong></span>

<span class="strong"><strong>================================</strong></span>
<span class="strong"><strong> Kafka Offsets</strong></span>
<span class="strong"><strong>================================</strong></span>
<span class="strong"><strong>Exiting due to: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /consumers/group1/offsets/rss/4.</strong></span>
</pre></div><p>By starting the Kafka producer rss feed using the script <code class="literal">kafka.bash</code>, I can now start feeding the rss-based <a id="id209" class="indexterm"></a>data through Kafka into Spark, and then into HDFS. Periodically checking the <code class="literal">spark-submit</code> session output it can be seen that events are passing through the Spark-based Kafka DStream. The following output comes from the stream count in the Scala code, and shows that at that point, 28 events were processed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1436834440000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Received events : 28</strong></span>
</pre></div><p>By checking HDFS under the <code class="literal">/data/spark/kafka/rss/</code> directory, via the Hadoop file system <code class="literal">ls</code> command, it can be seen that there is now data stored on HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ hdfs dfs -ls /data/spark/kafka/rss</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hadoop supergroup          0 2015-07-14 12:40 /data/spark/kafka/rss/file_1436833769907</strong></span>
</pre></div><p>By checking the contents of this directory, it can be seen that an HDFS part data file exists, which should contain the RSS-based data from Reuters:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ hdfs dfs -ls /data/spark/kafka/rss/file_1436833769907</strong></span>
<span class="strong"><strong>Found 2 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup          0 2015-07-14 12:40 /data/spark/kafka/rss/file_1436833769907/_SUCCESS</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup       8205 2015-07-14 12:40 /data/spark/kafka/rss/file_1436833769907/part-00001</strong></span>
</pre></div><p>Using the Hadoop file <a id="id210" class="indexterm"></a>system <code class="literal">cat</code> command below, I can dump the contents of this HDFS-based file to check its contents. I have used<a id="id211" class="indexterm"></a> the Linux <code class="literal">head</code> command to limit the data to save space. Clearly this is RSS Reuters science based information that the Perl script <code class="literal">rss.perl</code> has converted from XML to RSS JSON format.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m1 stream]$ hdfs dfs -cat /data/spark/kafka/rss/file_1436833769907/part-00001 | head -2</strong></span>

<span class="strong"><strong>{"category": "science", "title": "Bear necessities: low metabolism lets pandas survive on bamboo", "summary": "WASHINGTON (Reuters) - Giant pandas eat vegetables even though their bodies are better equipped to eat meat. So how do these black-and-white bears from the remote misty mountains of central China survive on a diet almost exclusively of a low-nutrient food like bamboo?"}</strong></span>

<span class="strong"><strong>{"category": "science", "title": "PlanetiQ tests sensor for commercial weather satellites", "summary": "CAPE CANAVERAL (Reuters) - PlanetiQ a privately owned company is beginning a key test intended to pave the way for the first commercial weather satellites."}</strong></span>
</pre></div><p>This ends this Kafka example. It can be seen that Kafka brokers have been installed and configured. It shows that an RSS data-based Kafka producer has fed data into the brokers. It has been proved, using the ZooKeeper client, that the Kafka architecture, matching the brokers, topics, and partitions has been set up in ZooKeeper. Finally, it has been shown using the Apache Spark-based Scala code, in the <code class="literal">stream6</code> application, that the Kafka data has been consumed and saved to HDFS.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Summary</h2></div></div><hr /></div><p>I could have provided streaming examples for systems like Kinesis, as well as queuing systems, but there was not room in this chapter. Twitter streaming has been examined by example in the checkpointing section.</p><p>This chapter has provided practical examples of data recovery via checkpointing in Spark streaming. It has also touched on the performance limitations of checkpointing and shown that that the checkpointing interval should be set at five to ten times the Spark stream batch interval. Checkpointing provides a stream-based recovery mechanism in the case of Spark application failure.</p><p>This chapter has provided some stream-based worked examples for TCP, File, Flume, and Kafka-based Spark stream coding. All the examples here are based on Scala, and are compiled with <code class="literal">sbt</code>. All of the code will be released with this book. Where the example architecture has become over-complicated, I have provided an architecture diagram (I'm thinking of the Kafka example here).</p><p>It is clear to me that the Apache Spark streaming module contains a rich source of functionality that should meet most of your needs, and will grow as future releases of Spark are delivered. Remember to check the Apache Spark website (<a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>), and join the Spark user list via <code class="email">&lt;<a class="email" href="mailto:user@spark.apache.org">user@spark.apache.org</a>&gt;</code>. Don't be afraid to ask questions, or make mistakes, as it seems to me that mistakes teach more than success.</p><p>The next chapter will examine the Spark SQL module, and will provide worked examples of SQL, data frames, and accessing Hive among other topics.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>Chapter 4. Apache Spark SQL</h2></div></div></div><p>In this chapter, I would like to examine Apache Spark SQL, the use of Apache Hive with Spark, and DataFrames. DataFrames<a id="id212" class="indexterm"></a> have been introduced in Spark 1.3, and are columnar data storage structures, roughly equivalent to relational database tables. The chapters in this book have not been developed in sequence, so the earlier chapters might use older versions of Spark than the later ones. I also want to examine user-defined functions for Spark SQL. A good place to find information about the Spark class API is: <code class="literal">spark.apache.org/docs/&lt;version&gt;/api/scala/index.html</code>.</p><p>I prefer to use Scala, but the API information is also available in Java and Python formats. The <code class="literal">&lt;version&gt;</code> value refers to the release of Spark that you will be using—1.3.1. This chapter will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>SQL context</p></li><li style="list-style-type: disc"><p>Importing and saving data</p></li><li style="list-style-type: disc"><p>DataFrames</p></li><li style="list-style-type: disc"><p>Using SQL</p></li><li style="list-style-type: disc"><p>User-defined functions</p></li><li style="list-style-type: disc"><p>Using Hive</p></li></ul></div><p>Before moving straight into SQL and DataFrames, I will give an overview of the SQL context.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec23"></a>The SQL context</h2></div></div><hr /></div><p>The SQL context is the starting<a id="id213" class="indexterm"></a> point for working with columnar data in Apache Spark. It is<a id="id214" class="indexterm"></a> created from the Spark context, and provides the means for loading and saving data files of different types, using DataFrames, and manipulating columnar data with SQL, among other things. It can be used for the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Executing SQL via the SQL method</p></li><li style="list-style-type: disc"><p>Registering user-defined functions via the UDF method</p></li><li style="list-style-type: disc"><p>Caching</p></li><li style="list-style-type: disc"><p>Configuration</p></li><li style="list-style-type: disc"><p>DataFrames</p></li><li style="list-style-type: disc"><p>Data source access</p></li><li style="list-style-type: disc"><p>DDL operations</p></li></ul></div><p>I am sure that there are<a id="id215" class="indexterm"></a> other areas, but you get the idea. The examples in this chapter are written in Scala, just because I prefer the language, but you can develop in Python and Java as well. As shown previously, the SQL context is created from the Spark context. Importing the SQL context implicitly allows you to implicitly convert RDDs into DataFrames:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val sqlContext = new org.apache.spark.sql.SQLContext(sc)</strong></span>
<span class="strong"><strong>import sqlContext.implicits._</strong></span>
</pre></div><p>For instance, using the previous <code class="literal">implicits</code> call, allows you to import a CSV file and split it by separator characters. It can then convert the RDD that contains the data into a data frame using the <code class="literal">toDF</code> method.</p><p>It is also possible to define a Hive context for the access and manipulation of Apache Hive database table data (Hive is the Apache data warehouse that is part of the Hadoop eco-system, and it uses HDFS for storage). The Hive context allows a superset of SQL functionality when compared to the Spark context. The use of Hive with Spark will be covered in a later section in this chapter.</p><p>Next, I will examine some of the supported file formats available for importing and saving data.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec24"></a>Importing and saving data</h2></div></div><hr /></div><p>I wanted to add this section<a id="id216" class="indexterm"></a> about importing and saving data here, even though it is not purely<a id="id217" class="indexterm"></a> about Spark SQL, so I could introduce concepts<a id="id218" class="indexterm"></a> such as <span class="strong"><strong>Parquet</strong></span> and <span class="strong"><strong>JSON</strong></span> file formats. This section also allows me to cover how to access and save data in loose text; as well as the CSV, Parquet and JSON formats, conveniently, in one place.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec37"></a>Processing the Text files</h3></div></div></div><p>Using the Spark<a id="id219" class="indexterm"></a> context, it is possible to load a text file into an RDD using the <a id="id220" class="indexterm"></a>
<code class="literal">textFile</code> method. Also, the <code class="literal">wholeTextFile</code> method can read the contents of a <a id="id221" class="indexterm"></a>directory into an RDD. The following examples show how a file, based on the local file system (<code class="literal">file://</code>), or HDFS (<code class="literal">hdfs://</code>) can be read into a Spark RDD. These examples show that the data will be partitioned into six parts for increased performance. The first two examples are the same, as they both manipulate a file on the Linux file system:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sc.textFile("/data/spark/tweets.txt",6)</strong></span>
<span class="strong"><strong>sc.textFile("file:///data/spark/tweets.txt",6)</strong></span>
<span class="strong"><strong>sc.textFile("hdfs://server1:4014/data/spark/tweets.txt",6)</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec38"></a>Processing the JSON files</h3></div></div></div><p>JSON is a<a id="id222" class="indexterm"></a> data interchange format, developed from Javascript. <span class="strong"><strong>JSON</strong></span> actually stands for <span class="strong"><strong>JavaScript</strong></span> <span class="strong"><strong>Object</strong></span> <span class="strong"><strong>Notation</strong></span>. It is a text-based format, and can be expressed, for instance, as XML. The following example<a id="id223" class="indexterm"></a> uses the SQL context method called <code class="literal">jsonFile</code> to load the HDFS-based JSON data file named <code class="literal">device.json</code>. The resulting data is created as a data frame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val dframe = sqlContext.jsonFile("hdfs:///data/spark/device.json")</strong></span>
</pre></div><p>Data can be saved in JSON format using the data frame <code class="literal">toJSON</code> method, as shown by the following example. First, the Apache Spark and Spark SQL classes are imported:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import org.apache.spark._</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext._</strong></span>
<span class="strong"><strong>import org.apache.spark.sql.Row;</strong></span>
<span class="strong"><strong>import org.apache.spark.sql.types.{StructType,StructField,StringType};</strong></span>
</pre></div><p>Next, the object class called <code class="literal">sql1</code> is defined as is a main method with parameters. A configuration object is defined that is used to create a spark context. The master Spark URL is left as the default value, so Spark expects local mode, the local host, and the <code class="literal">7077</code> port:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>object sql1 {</strong></span>

<span class="strong"><strong>  def main(args: Array[String]) {</strong></span>

<span class="strong"><strong>    val appName = "sql example 1"</strong></span>
<span class="strong"><strong>    val conf    = new SparkConf()</strong></span>

<span class="strong"><strong>    conf.setAppName(appName)</strong></span>

<span class="strong"><strong>    val sc = new SparkContext(conf)</strong></span>
</pre></div><p>An SQL context is created from the Spark context, and a raw text file is loaded in CSV format called <code class="literal">adult.test.data_1x</code>, using the <code class="literal">textFile</code> method. A schema string is then created, which<a id="id224" class="indexterm"></a> contains the data column names and the schema created from it by splitting the string by<a id="id225" class="indexterm"></a> its spacing, and using the <code class="literal">StructType</code> and <code class="literal">StructField</code> methods to define each schema column as a string value:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val sqlContext = new org.apache.spark.sql.SQLContext(sc)</strong></span>

<span class="strong"><strong>    val rawRdd = sc.textFile("hdfs:///data/spark/sql/adult.test.data_1x")</strong></span>

<span class="strong"><strong>    val schemaString = "age workclass fnlwgt education " +   "educational-num  marital-status occupation relationship " +</strong></span>
<span class="strong"><strong>"race gender capital-gain capital-loss hours-per-week " +</strong></span>
<span class="strong"><strong>"native-country income"</strong></span>

<span class="strong"><strong>    val schema =</strong></span>
<span class="strong"><strong>      StructType(</strong></span>
<span class="strong"><strong>    schemaString.split(" ").map(fieldName =&gt; StructField(fieldName, StringType, true)))</strong></span>
</pre></div><p>Each data row is then created from the raw CSV data by splitting it with the help of a comma as a line divider, and then the elements are added to a <code class="literal">Row()</code> structure. A data frame is created from the schema, and the row data which is then converted into JSON format using the <code class="literal">toJSON</code> method. Finally, the data is saved to HDFS using the <code class="literal">saveAsTextFile</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val rowRDD = rawRdd.map(_.split(","))</strong></span>
<span class="strong"><strong>      .map(p =&gt; Row( p(0),p(1),p(2),p(3),p(4),p(5),p(6),p(7),p(8),</strong></span>
<span class="strong"><strong>                      p(9),p(10),p(11),p(12),p(13),p(14) ))</strong></span>

<span class="strong"><strong>    val adultDataFrame = sqlContext.createDataFrame(rowRDD, schema)</strong></span>

<span class="strong"><strong>    val jsonData = adultDataFrame.toJSON</strong></span>

<span class="strong"><strong>    jsonData.saveAsTextFile("hdfs:///data/spark/sql/adult.json")</strong></span>

<span class="strong"><strong>  } // end main</strong></span>

<span class="strong"><strong>} // end sql1</strong></span>
</pre></div><p>So the resulting data can be seen on <a id="id226" class="indexterm"></a>HDFS, the Hadoop file system <code class="literal">ls</code> command below shows that the data resides in the <code class="literal">target</code> directory as a success file and two part files.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn sql]$ hdfs dfs -ls /data/spark/sql/adult.json</strong></span>

<span class="strong"><strong>Found 3 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup          0 2015-06-20 17:17 /data/spark/sql/adult.json/_SUCCESS</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup       1731 2015-06-20 17:17 /data/spark/sql/adult.json/part-00000</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup       1724 2015-06-20 17:17 /data/spark/sql/adult.json/part-00001</strong></span>
</pre></div><p>Using the Hadoop file system's <code class="literal">cat</code> command, it is possible to display the contents of the JSON data. I will just show a sample to save space:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn sql]$ hdfs dfs -cat /data/spark/sql/adult.json/part-00000 | more</strong></span>

<span class="strong"><strong>{"age":"25","workclass":" Private","fnlwgt":" 226802","education":" 11th","educational-num":"</strong></span>
<span class="strong"><strong> 7","marital-status":" Never-married","occupation":" Machine-op-inspct","relationship":" Own-</strong></span>
<span class="strong"><strong>child","race":" Black","gender":" Male","capital-gain":" 0","capital-loss":" 0","hours-per-we</strong></span>
<span class="strong"><strong>ek":" 40","native-country":" United-States","income":" &lt;=50K"}</strong></span>
</pre></div><p>Processing the Parquet data is very similar, as I will show next.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec39"></a>Processing the Parquet files</h3></div></div></div><p>Apache Parquet is another<a id="id227" class="indexterm"></a> columnar-based data format used by many tools in the Hadoop tool set for file I/O, such as Hive, Pig, and Impala. It increases<a id="id228" class="indexterm"></a> performance by using efficient compression and encoding routines.</p><p>The Parquet processing example is very similar to the JSON Scala code. The DataFrame is created, and then saved in a Parquet format using the save method with a type of Parquet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val adultDataFrame = sqlContext.createDataFrame(rowRDD, schema)</strong></span>
<span class="strong"><strong>    adultDataFrame.save("hdfs:///data/spark/sql/adult.parquet","parquet")</strong></span>

<span class="strong"><strong>  } // end main</strong></span>

<span class="strong"><strong>} // end sql2</strong></span>
</pre></div><p>This results in an HDFS-based directory, which contains three Parquet-based files: a common Metadata file, a Metadata file, and a temporary file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn sql]$ hdfs dfs -ls /data/spark/sql/adult.parquet</strong></span>
<span class="strong"><strong>Found 3 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup       1412 2015-06-21 13:17 /data/spark/sql/adult.parquet/_common_metadata</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup       1412 2015-06-21 13:17 /data/spark/sql/adult.parquet/_metadata</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hadoop supergroup          0 2015-06-21 13:17 /data/spark/sql/adult.parquet/_temporary</strong></span>
</pre></div><p>Listing the contents<a id="id229" class="indexterm"></a> of the metadata file, using the Hadoop file system's <code class="literal">cat</code> command, gives an idea of the data format. However the Parquet header is<a id="id230" class="indexterm"></a> binary, and so, it does not display with <code class="literal">more</code> and <code class="literal">cat</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn sql]$ hdfs dfs -cat /data/spark/sql/adult.parquet/_metadata | more</strong></span>
<span class="strong"><strong>s%</strong></span>
<span class="strong"><strong>ct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"workclass</strong></span>
<span class="strong"><strong>","type":"string","nullable":true,"metadata":{}},{"name":"fnlwgt","type":"string","nullable":</strong></span>
<span class="strong"><strong>true,"metadata":{}},</strong></span>
</pre></div><p>For more information about possible Spark and SQL context methods, check the contents of the classes called <code class="literal">org.apache.spark.SparkContext</code>, and <code class="literal">org.apache.spark.sql.SQLContext</code>, using the Apache Spark API path here for the specific <code class="literal">&lt;version&gt;</code> of Spark that you are interested in:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>spark.apache.org/docs/&lt;version&gt;/api/scala/index.html</strong></span>
</pre></div><p>In the next section, I will examine Apache Spark DataFrames, introduced in Spark 1.3.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec25"></a>DataFrames</h2></div></div><hr /></div><p>I have already mentioned that a DataFrame is based on a columnar format. Temporary tables can be created from it, but I will expand on this in the next section. There are many methods available to<a id="id231" class="indexterm"></a> the data frame that allow data manipulation, and processing. I have based the Scala code used here, on the code in the last section, so I will just show you the working lines and the output. It is possible to display a data frame schema as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>adultDataFrame.printSchema()</strong></span>

<span class="strong"><strong>root</strong></span>
<span class="strong"><strong> |-- age: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- workclass: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- fnlwgt: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- education: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- educational-num: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- marital-status: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- occupation: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- relationship: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- race: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- gender: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- capital-gain: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- capital-loss: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- hours-per-week: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- native-country: string (nullable = true)</strong></span>
<span class="strong"><strong> |-- income: string (nullable = true)</strong></span>
</pre></div><p>It is possible to use the <code class="literal">select</code> method to filter columns from the data. I have limited the output here, in terms of rows, but you get the idea:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>adultDataFrame.select("workclass","age","education","income").show()</strong></span>

<span class="strong"><strong>workclass         age education     income</strong></span>
<span class="strong"><strong> Private          25   11th          &lt;=50K</strong></span>
<span class="strong"><strong> Private          38   HS-grad       &lt;=50K</strong></span>
<span class="strong"><strong> Local-gov        28   Assoc-acdm    &gt;50K</strong></span>
<span class="strong"><strong> Private          44   Some-college  &gt;50K</strong></span>
<span class="strong"><strong> none             18   Some-college  &lt;=50K</strong></span>
<span class="strong"><strong> Private          34   10th          &lt;=50K</strong></span>
<span class="strong"><strong> none             29   HS-grad       &lt;=50K</strong></span>
<span class="strong"><strong> Self-emp-not-inc 63   Prof-school   &gt;50K</strong></span>
<span class="strong"><strong> Private          24   Some-college  &lt;=50K</strong></span>
<span class="strong"><strong> Private          55   7th-8th       &lt;=50K</strong></span>
</pre></div><p>It is possible to filter the data returned from the DataFrame using the <code class="literal">filter</code> method. Here, I have added the occupation column to the output, and filtered on the worker age:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    adultDataFrame</strong></span>
<span class="strong"><strong>      .select("workclass","age","education","occupation","income")</strong></span>
<span class="strong"><strong>      .filter( adultDataFrame("age") &gt; 30 )</strong></span>
<span class="strong"><strong>      .show()</strong></span>

<span class="strong"><strong>workclass         age education     occupation         income</strong></span>
<span class="strong"><strong> Private          38   HS-grad       Farming-fishing    &lt;=50K</strong></span>
<span class="strong"><strong> Private          44   Some-college  Machine-op-inspct  &gt;50K</strong></span>
<span class="strong"><strong> Private          34   10th          Other-service      &lt;=50K</strong></span>
<span class="strong"><strong> Self-emp-not-inc 63   Prof-school   Prof-specialty     &gt;50K</strong></span>
<span class="strong"><strong> Private          55   7th-8th       Craft-repair       &lt;=50K</strong></span>
</pre></div><p>There is also a <code class="literal">group by</code> method for determining volume counts within a data set. As this is an income-based<a id="id232" class="indexterm"></a> dataset, I think that volumes within the wage brackets would be interesting. I have also used a bigger dataset to give more meaningful results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    adultDataFrame</strong></span>
<span class="strong"><strong>      .groupBy("income")</strong></span>
<span class="strong"><strong>      .count()</strong></span>
<span class="strong"><strong>      .show()</strong></span>

<span class="strong"><strong>income count</strong></span>
<span class="strong"><strong> &lt;=50K 24720</strong></span>
<span class="strong"><strong> &gt;50K  7841</strong></span>
</pre></div><p>This is interesting, but what if I want to compare <code class="literal">income</code> brackets with <code class="literal">occupation</code>, and sort the results for a better understanding? The following example shows how this can be done, and gives the example data volumes. It shows that there is a high volume of managerial roles compared to other occupations. This example also sorts the output by the occupation column:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    adultDataFrame</strong></span>
<span class="strong"><strong>      .groupBy("income","occupation")</strong></span>
<span class="strong"><strong>      .count()</strong></span>
<span class="strong"><strong>      .sort("occupation")</strong></span>
<span class="strong"><strong>      .show()</strong></span>

<span class="strong"><strong>income occupation         count</strong></span>
<span class="strong"><strong> &gt;50K   Adm-clerical      507</strong></span>
<span class="strong"><strong> &lt;=50K  Adm-clerical      3263</strong></span>
<span class="strong"><strong> &lt;=50K  Armed-Forces      8</strong></span>
<span class="strong"><strong> &gt;50K   Armed-Forces      1</strong></span>
<span class="strong"><strong> &lt;=50K  Craft-repair      3170</strong></span>
<span class="strong"><strong> &gt;50K   Craft-repair      929</strong></span>
<span class="strong"><strong> &lt;=50K  Exec-managerial   2098</strong></span>
<span class="strong"><strong> &gt;50K   Exec-managerial   1968</strong></span>
<span class="strong"><strong> &lt;=50K  Farming-fishing   879</strong></span>
<span class="strong"><strong> &gt;50K   Farming-fishing   115</strong></span>
<span class="strong"><strong> &lt;=50K  Handlers-cleaners 1284</strong></span>
<span class="strong"><strong> &gt;50K   Handlers-cleaners 86</strong></span>
<span class="strong"><strong> &gt;50K   Machine-op-inspct 250</strong></span>
<span class="strong"><strong> &lt;=50K  Machine-op-inspct 1752</strong></span>
<span class="strong"><strong> &gt;50K   Other-service     137</strong></span>
<span class="strong"><strong> &lt;=50K  Other-service     3158</strong></span>
<span class="strong"><strong> &gt;50K   Priv-house-serv   1</strong></span>
<span class="strong"><strong> &lt;=50K  Priv-house-serv   148</strong></span>
<span class="strong"><strong> &gt;50K   Prof-specialty    1859</strong></span>
<span class="strong"><strong> &lt;=50K  Prof-specialty    2281</strong></span>
</pre></div><p>So, SQL-like actions can be carried out against DataFrames, including <code class="literal">select</code>, <code class="literal">filter</code>, sort <code class="literal">group by</code>, and <code class="literal">print</code>. The next section shows how tables can be created from the DataFrames, and how the SQL-based actions are carried out against them.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec26"></a>Using SQL</h2></div></div><hr /></div><p>After using the<a id="id233" class="indexterm"></a> previous Scala example to create a data frame, from a CSV based-data input file on HDFS, I can now define a temporary table, based on the data frame, and run SQL against it. The following example shows the temporary table called <code class="literal">adult</code> being defined, and a row count being created using <code class="literal">COUNT(*)</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    adultDataFrame.registerTempTable("adult")</strong></span>

<span class="strong"><strong>    val resRDD = sqlContext.sql("SELECT COUNT(*) FROM adult")</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; "Count - " + t(0)).collect().foreach(println)</strong></span>
</pre></div><p>This gives a row count of over 32,000 rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Count – 32561</strong></span>
</pre></div><p>It is also possible to limit the volume of the data selected from the table using the <code class="literal">LIMIT</code> SQL option, which is shown in the following example. The first 10 rows have been selected from the data, this is useful if I just want to check data types and quality:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val resRDD = sqlContext.sql("SELECT * FROM adult LIMIT 10")</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; t(0)  + " " + t(1)  + " " + t(2)  + " " + t(3)  + " " +</strong></span>
<span class="strong"><strong>                    t(4)  + " " + t(5)  + " " + t(6)  + " " + t(7)  + " " +</strong></span>
<span class="strong"><strong>                    t(8)  + " " + t(9)  + " " + t(10) + " " + t(11) + " " +</strong></span>
<span class="strong"><strong>                    t(12) + " " + t(13) + " " + t(14)</strong></span>
<span class="strong"><strong>              )</strong></span>
<span class="strong"><strong>      .collect().foreach(println)</strong></span>
</pre></div><p>A sample of the data looks like the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>50  Private  283676  Some-college  10  Married-civ-spouse  Craft-repair  Husband  White  Male  0  0  40  United-States  &gt;50K</strong></span>
</pre></div><p>When the schema<a id="id234" class="indexterm"></a> for this data was created in the Scala-based data frame example in the last section, all the columns were created as strings. However, if I want to filter the data in SQL using <code class="literal">WHERE</code> clauses, it would be useful to have proper data types. For instance, if an age column stores integer values, it should be stored as an integer so that I can execute numeric comparisons against it. I have changed my Scala code to include all the possible types:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import org.apache.spark.sql.types._</strong></span>
</pre></div><p>I have also now defined my schema using different types, to better match the data, and I have defined the row data in terms of the actual data types, converting raw data string values into integer values, where necessary:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val schema =</strong></span>
<span class="strong"><strong>      StructType(</strong></span>
<span class="strong"><strong>        StructField("age",                IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("workclass",          StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("fnlwgt",             IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("education",          StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("educational-num",    IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("marital-status",     StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("occupation",         StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("relationship",       StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("race",               StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("gender",             StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("capital-gain",       IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("capital-loss",       IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("hours-per-week",     IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("native-country",     StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("income",             StringType,  false) ::</strong></span>
<span class="strong"><strong>        Nil)</strong></span>

<span class="strong"><strong>    val rowRDD = rawRdd.map(_.split(","))</strong></span>
<span class="strong"><strong>      .map(p =&gt; Row( p(0).trim.toInt,p(1),p(2).trim.toInt,p(3),</strong></span>
<span class="strong"><strong>                     p(4).trim.toInt,p(5),p(6),p(7),p(8),</strong></span>
<span class="strong"><strong>                     p(9),p(10).trim.toInt,p(11).trim.toInt,</strong></span>
<span class="strong"><strong>                     p(12).trim.toInt,p(13),p(14) ))</strong></span>
</pre></div><p>The SQL can now <a id="id235" class="indexterm"></a>use numeric filters in the <code class="literal">WHERE</code> clause correctly. If the <code class="literal">age</code> column were a string, this would not work. You can now see that the data has been filtered to give age values below 60 years:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val resRDD = sqlContext.sql("SELECT COUNT(*) FROM adult WHERE age &lt; 60")</strong></span>
<span class="strong"><strong>    resRDD.map(t =&gt; "Count - " + t(0)).collect().foreach(println)</strong></span>
</pre></div><p>This gives a row count of around 30,000 rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Count – 29917</strong></span>
</pre></div><p>It is possible to use Boolean logic in the <code class="literal">WHERE</code>-based filter clauses. The following example specifies an age range for the data. Note that I have used variables to describe the <code class="literal">select</code> and <code class="literal">filter</code> components of the SQL statement. This allows me to break down the statement into different parts as they become larger:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val selectClause = "SELECT COUNT(*) FROM adult "</strong></span>
<span class="strong"><strong>    val filterClause = "WHERE age &gt; 25 AND age &lt; 60"</strong></span>
<span class="strong"><strong>    val resRDD = sqlContext.sql( selectClause + filterClause )</strong></span>
<span class="strong"><strong>    resRDD.map(t =&gt; "Count - " + t(0)).collect().foreach(println)</strong></span>
</pre></div><p>Giving a data count of around 23,000 rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Count – 23506</strong></span>
</pre></div><p>I can create compound filter clauses using the Boolean terms, such as <code class="literal">AND</code>, <code class="literal">OR</code>, as well as parentheses:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val selectClause = "SELECT COUNT(*) FROM adult "</strong></span>
<span class="strong"><strong>    val filterClause =</strong></span>
<span class="strong"><strong>   "WHERE ( age &gt; 15 AND age &lt; 25 ) OR ( age &gt; 30 AND age &lt; 45 ) "</strong></span>

<span class="strong"><strong>    val resRDD = sqlContext.sql( selectClause + filterClause )</strong></span>
<span class="strong"><strong>    resRDD.map(t =&gt; "Count - " + t(0)).collect().foreach(println)</strong></span>
</pre></div><p>This gives me a row count of 17,000 rows, and represents a count of two age ranges in the data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Count – 17198</strong></span>
</pre></div><p>It is also possible to use <a id="id236" class="indexterm"></a>subqueries in Apache Spark SQL. You can see in the following example that I have created a subquery called <code class="literal">t1</code> by selecting three columns; <code class="literal">age</code>, <code class="literal">education</code>, and <code class="literal">occupation</code> from the table <code class="literal">adult</code>. I have then used the table called <code class="literal">t1</code> to create a row count. I have also added a filter clause acting on the age column from the table <code class="literal">t1</code>. Notice also that I have added <code class="literal">group by</code> and <code class="literal">order by</code> clauses, even though they are empty currently, to my SQL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val selectClause = "SELECT COUNT(*) FROM "</strong></span>
<span class="strong"><strong>    val tableClause = " ( SELECT age,education,occupation from adult) t1 "</strong></span>
<span class="strong"><strong>    val filterClause = "WHERE ( t1.age &gt; 25 ) "</strong></span>
<span class="strong"><strong>    val groupClause = ""</strong></span>
<span class="strong"><strong>    val orderClause = ""</strong></span>

<span class="strong"><strong>    val resRDD = sqlContext.sql( selectClause + tableClause +</strong></span>
<span class="strong"><strong>                                 filterClause +</strong></span>
<span class="strong"><strong>                                 groupClause + orderClause</strong></span>
<span class="strong"><strong>                               )</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; "Count - " + t(0)).collect().foreach(println)</strong></span>
</pre></div><p>In order to examine the table joins, I have created a version of the adult CSV data file called <code class="literal">adult.train.data2</code>, which only differs from the original by the fact that it has an added first column called <code class="literal">idx</code>, which is a unique index. The Hadoop file system's <code class="literal">cat</code> command here shows a sample of the data. The output from the file has been limited using the Linux <code class="literal">head</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn sql]$ hdfs dfs -cat /data/spark/sql/adult.train.data2 | head -2</strong></span>

<span class="strong"><strong>1,39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K</strong></span>
<span class="strong"><strong>2,50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, &lt;=50K</strong></span>
</pre></div><p>The schema has now been redefined to have an integer-based first column called <code class="literal">idx</code> for an index, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val schema =</strong></span>
<span class="strong"><strong>      StructType(</strong></span>
<span class="strong"><strong>        StructField("idx",                IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("age",                IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("workclass",          StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("fnlwgt",             IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("education",          StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("educational-num",    IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("marital-status",     StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("occupation",         StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("relationship",       StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("race",               StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("gender",             StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("capital-gain",       IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("capital-loss",       IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("hours-per-week",     IntegerType, false) ::</strong></span>
<span class="strong"><strong>        StructField("native-country",     StringType,  false) ::</strong></span>
<span class="strong"><strong>        StructField("income",             StringType,  false) ::</strong></span>
<span class="strong"><strong>        Nil)</strong></span>
</pre></div><p>And the raw row <a id="id237" class="indexterm"></a>RDD in the Scala example now processes the new initial column, and converts the string value into an integer:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val rowRDD = rawRdd.map(_.split(","))</strong></span>
<span class="strong"><strong>      .map(p =&gt; Row( p(0).trim.toInt,</strong></span>
<span class="strong"><strong>                     p(1).trim.toInt,</strong></span>
<span class="strong"><strong>                     p(2),</strong></span>
<span class="strong"><strong>                     p(3).trim.toInt,</strong></span>
<span class="strong"><strong>                     p(4),</strong></span>
<span class="strong"><strong>                     p(5).trim.toInt,</strong></span>
<span class="strong"><strong>                     p(6),</strong></span>
<span class="strong"><strong>                     p(7),</strong></span>
<span class="strong"><strong>                     p(8),</strong></span>
<span class="strong"><strong>                     p(9),</strong></span>
<span class="strong"><strong>                     p(10),</strong></span>
<span class="strong"><strong>                     p(11).trim.toInt,</strong></span>
<span class="strong"><strong>                     p(12).trim.toInt,</strong></span>
<span class="strong"><strong>                     p(13).trim.toInt,</strong></span>
<span class="strong"><strong>                     p(14),</strong></span>
<span class="strong"><strong>                     p(15)</strong></span>
<span class="strong"><strong>                   ))</strong></span>

<span class="strong"><strong>    val adultDataFrame = sqlContext.createDataFrame(rowRDD, schema)</strong></span>
</pre></div><p>We have looked at subqueries. Now, I would like to consider table joins. The next example will use the index that was just created. It uses it to join two derived tables. The example is somewhat contrived, given that it joins two data sets from the same underlying table, but<a id="id238" class="indexterm"></a> you get the idea. Two derived tables are created as subqueries, and are joined at a common index column.</p><p>The SQL for a table join now looks like this. Two derived tables have been created from the temporary table <code class="literal">adult</code> called <code class="literal">t1</code> and <code class="literal">t2</code> as subqueries. The new row index column called <code class="literal">idx</code> has been used to join the data in tables <code class="literal">t1</code> and <code class="literal">t2</code>. The major <code class="literal">SELECT</code> statement outputs all seven columns from the compound data set. I have added a <code class="literal">LIMIT</code> clause to minimize the data output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> val selectClause = "SELECT t1.idx,age,education,occupation,workclass,race,gender FROM "</strong></span>
<span class="strong"><strong> val tableClause1 = " ( SELECT idx,age,education,occupation FROM adult) t1 JOIN "</strong></span>
<span class="strong"><strong> val tableClause2 = " ( SELECT idx,workclass,race,gender FROM adult) t2 "</strong></span>
<span class="strong"><strong> val joinClause = " ON (t1.idx=t2.idx) "</strong></span>
<span class="strong"><strong> val limitClause = " LIMIT 10"</strong></span>

<span class="strong"><strong> val resRDD = sqlContext.sql( selectClause +</strong></span>
<span class="strong"><strong>                              tableClause1 + tableClause2 +</strong></span>
<span class="strong"><strong>                              joinClause   + limitClause</strong></span>
<span class="strong"><strong>                            )</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; t(0) + " " + t(1) + " " + t(2) + " " +</strong></span>
<span class="strong"><strong>                    t(3) + " " + t(4) + " " + t(5) + " " + t(6)</strong></span>
<span class="strong"><strong>              )</strong></span>
<span class="strong"><strong>              .collect().foreach(println)</strong></span>
</pre></div><p>Note that in the major <code class="literal">SELECT</code> statement, I have to define where the index column comes from, so I use <code class="literal">t1.idx</code>. All the other columns are unique to the <code class="literal">t1</code> and <code class="literal">t2</code> datasets, so I don't need to use an alias to refer to them (that is, <code class="literal">t1.age</code>). So, the data that is output now looks like the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>33 45  Bachelors  Exec-managerial  Private  White  Male</strong></span>
<span class="strong"><strong>233 25  Some-college  Adm-clerical  Private  White  Male</strong></span>
<span class="strong"><strong>433 40  Bachelors  Prof-specialty  Self-emp-not-inc  White  Female</strong></span>
<span class="strong"><strong>633 43  Some-college  Craft-repair  Private  White  Male</strong></span>
<span class="strong"><strong>833 26  Some-college  Handlers-cleaners  Private  White  Male</strong></span>
<span class="strong"><strong>1033 27  Some-college  Sales  Private  White  Male</strong></span>
<span class="strong"><strong>1233 27  Bachelors  Adm-clerical  Private  White  Female</strong></span>
<span class="strong"><strong>1433 32  Assoc-voc  Sales  Private  White  Male</strong></span>
<span class="strong"><strong>1633 40  Assoc-acdm  Adm-clerical  State-gov  White  Male</strong></span>
<span class="strong"><strong>1833 46  Some-college  Prof-specialty  Local-gov  White  Male</strong></span>
</pre></div><p>This gives some idea<a id="id239" class="indexterm"></a> of the SQL-based functionality within Apache Spark, but what if I find that the method that I need is not available? Perhaps, I need a new function. This is where the <span class="strong"><strong>user-defined functions</strong></span> (<span class="strong"><strong>UDFs</strong></span>) are useful. I will cover them in the next section.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec27"></a>User-defined functions</h2></div></div><hr /></div><p>In order to create some user-defined functions in Scala, I need to examine my data in the previous adult dataset. I plan to create a UDF that will enumerate the education column, so that I can<a id="id240" class="indexterm"></a> convert the column into an integer value. This will be useful if I need to use the data for machine learning, and so create a LabelPoint structure. The vector used, which represents each record, will need to be numeric. I will first determine what kind of unique education values exist, then I will create a function to enumerate them, and finally use it in SQL.</p><p>I have created some Scala code to display a sorted list of the education values. The <code class="literal">DISTINCT</code> keyword ensures that there is only one instance of each value. I have selected the data as a subtable, using an alias called <code class="literal">edu_dist</code> for the data column to ensure that the <code class="literal">ORDER BY</code> clause works:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val selectClause = "SELECT t1.edu_dist FROM "</strong></span>
<span class="strong"><strong>    val tableClause  = " ( SELECT DISTINCT education AS edu_dist FROM adult ) t1 "</strong></span>
<span class="strong"><strong>    val orderClause  = " ORDER BY t1.edu_dist "</strong></span>

<span class="strong"><strong>    val resRDD = sqlContext.sql( selectClause + tableClause  + orderClause )</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; t(0)).collect().foreach(println)</strong></span>
</pre></div><p>The data looks like the following. I have removed some values to save space, but you get the idea:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> 10th</strong></span>
<span class="strong"><strong> 11th</strong></span>
<span class="strong"><strong> 12th</strong></span>
<span class="strong"><strong> 1st-4th</strong></span>
<span class="strong"><strong> ………..</strong></span>
<span class="strong"><strong> Preschool</strong></span>
<span class="strong"><strong> Prof-school</strong></span>
<span class="strong"><strong> Some-college</strong></span>
</pre></div><p>I have defined a<a id="id241" class="indexterm"></a> method in Scala to accept the string-based education value, and return an enumerated integer value that represents it. If no value is recognized, then a special value called <code class="literal">9999</code> is returned:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  def enumEdu( education:String ) : Int =</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    var enumval = 9999</strong></span>

<span class="strong"><strong>         if ( education == "10th" )         { enumval = 0 }</strong></span>
<span class="strong"><strong>    else if ( education == "11th" )         { enumval = 1 }</strong></span>
<span class="strong"><strong>    else if ( education == "12th" )         { enumval = 2 }</strong></span>
<span class="strong"><strong>    else if ( education == "1st-4th" )      { enumval = 3 }</strong></span>
<span class="strong"><strong>    else if ( education == "5th-6th" )      { enumval = 4 }</strong></span>
<span class="strong"><strong>    else if ( education == "7th-8th" )      { enumval = 5 }</strong></span>
<span class="strong"><strong>    else if ( education == "9th" )          { enumval = 6 }</strong></span>
<span class="strong"><strong>    else if ( education == "Assoc-acdm" )   { enumval = 7 }</strong></span>
<span class="strong"><strong>    else if ( education == "Assoc-voc" )    { enumval = 8 }</strong></span>
<span class="strong"><strong>    else if ( education == "Bachelors" )    { enumval = 9 }</strong></span>
<span class="strong"><strong>    else if ( education == "Doctorate" )    { enumval = 10 }</strong></span>
<span class="strong"><strong>    else if ( education == "HS-grad" )      { enumval = 11 }</strong></span>
<span class="strong"><strong>    else if ( education == "Masters" )      { enumval = 12 }</strong></span>
<span class="strong"><strong>    else if ( education == "Preschool" )    { enumval = 13 }</strong></span>
<span class="strong"><strong>    else if ( education == "Prof-school" )  { enumval = 14 }</strong></span>
<span class="strong"><strong>    else if ( education == "Some-college" ) { enumval = 15 }</strong></span>

<span class="strong"><strong>    return enumval</strong></span>
<span class="strong"><strong>  }</strong></span>
</pre></div><p>I can now register this function using the SQL context in Scala, so that it can be used in an SQL statement:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    sqlContext.udf.register( "enumEdu", enumEdu _ )</strong></span>
</pre></div><p>The SQL, and the Scala code to enumerate the data then look like this. The newly registered function <a id="id242" class="indexterm"></a>called <code class="literal">enumEdu</code> is used in the <code class="literal">SELECT</code> statement. It takes the education type as a parameter, and returns the integer enumeration. The column that this value forms is aliased to the name <code class="literal">idx</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val selectClause = "SELECT enumEdu(t1.edu_dist) as idx,t1.edu_dist FROM "</strong></span>
<span class="strong"><strong>    val tableClause  = " ( SELECT DISTINCT education AS edu_dist FROM adult ) t1 "</strong></span>
<span class="strong"><strong>    val orderClause  = " ORDER BY t1.edu_dist "</strong></span>

<span class="strong"><strong>    val resRDD = sqlContext.sql( selectClause + tableClause  + orderClause )</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; t(0) + " " + t(1) ).collect().foreach(println)</strong></span>
</pre></div><p>The resulting data output, as a list of education values and their enumerations, looks like the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0  10th</strong></span>
<span class="strong"><strong>1  11th</strong></span>
<span class="strong"><strong>2  12th</strong></span>
<span class="strong"><strong>3  1st-4th</strong></span>
<span class="strong"><strong>4  5th-6th</strong></span>
<span class="strong"><strong>5  7th-8th</strong></span>
<span class="strong"><strong>6  9th</strong></span>
<span class="strong"><strong>7  Assoc-acdm</strong></span>
<span class="strong"><strong>8  Assoc-voc</strong></span>
<span class="strong"><strong>9  Bachelors</strong></span>
<span class="strong"><strong>10  Doctorate</strong></span>
<span class="strong"><strong>11  HS-grad</strong></span>
<span class="strong"><strong>12  Masters</strong></span>
<span class="strong"><strong>13  Preschool</strong></span>
<span class="strong"><strong>14  Prof-school</strong></span>
<span class="strong"><strong>15  Some-college</strong></span>
</pre></div><p>Another example function called <code class="literal">ageBracket</code> takes the adult integer age value, and returns an enumerated age bracket:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  def ageBracket( age:Int ) : Int =</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    var bracket = 9999</strong></span>

<span class="strong"><strong>         if ( age &gt;= 0  &amp;&amp; age &lt; 20  ) { bracket = 0 }</strong></span>
<span class="strong"><strong>    else if ( age &gt;= 20 &amp;&amp; age &lt; 40  ) { bracket = 1 }</strong></span>
<span class="strong"><strong>    else if ( age &gt;= 40 &amp;&amp; age &lt; 60  ) { bracket = 2 }</strong></span>
<span class="strong"><strong>    else if ( age &gt;= 60 &amp;&amp; age &lt; 80  ) { bracket = 3 }</strong></span>
<span class="strong"><strong>    else if ( age &gt;= 80 &amp;&amp; age &lt; 100 ) { bracket = 4 }</strong></span>
<span class="strong"><strong>    else if ( age &gt; 100 )              { bracket = 5 }</strong></span>

<span class="strong"><strong>    return bracket</strong></span>
<span class="strong"><strong>  }</strong></span>
</pre></div><p>Again, the <a id="id243" class="indexterm"></a>function is registered using the SQL context so that it can be used in an SQL statement:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    sqlContext.udf.register( "ageBracket", ageBracket _ )</strong></span>
</pre></div><p>Then, the Scala-based SQL uses it to select the age, age bracket, and education value from the adult dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val selectClause = "SELECT age, ageBracket(age) as bracket,education FROM "</strong></span>
<span class="strong"><strong>    val tableClause  = " adult "</strong></span>
<span class="strong"><strong>    val limitClause  = " LIMIT 10 "</strong></span>

<span class="strong"><strong>    val resRDD = sqlContext.sql( selectClause + tableClause  +</strong></span>
<span class="strong"><strong>                                 limitClause )</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; t(0) + " " + t(1) + " " + t(2) ).collect().foreach(println)</strong></span>
</pre></div><p>The resulting data then looks like this, given that I have used the <code class="literal">LIMIT</code> clause to limit the output to 10 rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>39 1  Bachelors</strong></span>
<span class="strong"><strong>50 2  Bachelors</strong></span>
<span class="strong"><strong>38 1  HS-grad</strong></span>
<span class="strong"><strong>53 2  11th</strong></span>
<span class="strong"><strong>28 1  Bachelors</strong></span>
<span class="strong"><strong>37 1  Masters</strong></span>
<span class="strong"><strong>49 2  9th</strong></span>
<span class="strong"><strong>52 2  HS-grad</strong></span>
<span class="strong"><strong>31 1  Masters</strong></span>
<span class="strong"><strong>42 2  Bachelors</strong></span>
</pre></div><p>It is also possible to define functions for use in SQL, inline, during the UDF registration<a id="id244" class="indexterm"></a> using the SQL context. The following example defines a function called <code class="literal">dblAge</code>, which just multiplies the adult's age by two. The registration looks like this. It takes integer parameters (<code class="literal">age</code>), and returns twice its value:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    sqlContext.udf.register( "dblAge", (a:Int) =&gt; 2*a )</strong></span>
</pre></div><p>And the SQL that uses it, now selects the <code class="literal">age</code>, and the double of the <code class="literal">age</code> value called <code class="literal">dblAge(age)</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val selectClause = "SELECT age,dblAge(age) FROM "</strong></span>
<span class="strong"><strong>    val tableClause  = " adult "</strong></span>
<span class="strong"><strong>    val limitClause  = " LIMIT 10 "</strong></span>

<span class="strong"><strong>    val resRDD = sqlContext.sql( selectClause + tableClause  + limitClause )</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; t(0) + " " + t(1) ).collect().foreach(println)</strong></span>
</pre></div><p>The two columns of the output data, which now contain the age and its doubled value, now look like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>39 78</strong></span>
<span class="strong"><strong>50 100</strong></span>
<span class="strong"><strong>38 76</strong></span>
<span class="strong"><strong>53 106</strong></span>
<span class="strong"><strong>28 56</strong></span>
<span class="strong"><strong>37 74</strong></span>
<span class="strong"><strong>49 98</strong></span>
<span class="strong"><strong>52 104</strong></span>
<span class="strong"><strong>31 62</strong></span>
<span class="strong"><strong>42 84</strong></span>
</pre></div><p>So far, DataFrames, SQL, and user-defined functions have been examined, but what if, as in my case, you are using a Hadoop stack cluster, and have Apache Hive available? The adult table that I have defined so far is a temporary table, but if I access Hive using Apache Spark SQL, I can access the static database tables. The next section will examine the steps needed to do this.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec28"></a>Using Hive</h2></div></div><hr /></div><p>If you have a<a id="id245" class="indexterm"></a> business intelligence-type workload with low latency requirements and multiple users, then you might consider using Impala for your database access. Apache Spark on Hive is for batch processing and ETL chains. This section will be used to show how to connect Spark to Hive, and how to use this configuration. First, I will develop an application that uses a local Hive Metastore, and show that it does not store and persist table data in Hive itself. I will then set up Apache Spark to connect to the Hive Metastore server, and store tables and data within Hive. I will start with the local Metastore server.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec40"></a>Local Hive Metastore server</h3></div></div></div><p>The following<a id="id246" class="indexterm"></a> example Scala code shows how to create a Hive context, and<a id="id247" class="indexterm"></a> create a Hive-based table using Apache Spark. First, the Spark configuration, context, SQL, and Hive classes are imported. Then, an object class called <code class="literal">hive_ex1</code>, and the main method are defined. The application name is defined, and a Spark configuration object is created. The Spark context is then created from the configuration object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import org.apache.spark.{SparkConf, SparkContext}</strong></span>
<span class="strong"><strong>import org.apache.spark.sql._</strong></span>
<span class="strong"><strong>import org.apache.spark.sql.hive.HiveContext</strong></span>

<span class="strong"><strong>object hive_ex1 {</strong></span>

<span class="strong"><strong>  def main(args: Array[String]) {</strong></span>

<span class="strong"><strong>    val appName = "Hive Spark Ex 1"</strong></span>
<span class="strong"><strong>    val conf    = new SparkConf()</strong></span>

<span class="strong"><strong>    conf.setAppName(appName)</strong></span>

<span class="strong"><strong>    val sc = new SparkContext(conf)</strong></span>
</pre></div><p>Next, I create a new Hive context from the Spark context, and import the Hive implicits, and the Hive context SQL. The <code class="literal">implicits</code> allow for implicit conversions, and the SQL include allows me to run Hive context-based SQL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val hiveContext = new HiveContext(sc)</strong></span>

<span class="strong"><strong>    import hiveContext.implicits._</strong></span>
<span class="strong"><strong>    import hiveContext.sql</strong></span>
</pre></div><p>The next statement creates an empty table called <code class="literal">adult2</code> in Hive. You will recognize the schema from the<a id="id248" class="indexterm"></a> adult data that has already been used in this chapter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>     hiveContext.sql( "    </strong></span>
<span class="strong"><strong>         CREATE TABLE IF NOT EXISTS adult2</strong></span>
<span class="strong"><strong>            (</strong></span>
<span class="strong"><strong>              idx             INT,</strong></span>
<span class="strong"><strong>              age             INT,</strong></span>
<span class="strong"><strong>              workclass       STRING,</strong></span>
<span class="strong"><strong>              fnlwgt          INT,</strong></span>
<span class="strong"><strong>              education       STRING,</strong></span>
<span class="strong"><strong>              educationnum    INT,</strong></span>
<span class="strong"><strong>              maritalstatus   STRING,</strong></span>
<span class="strong"><strong>              occupation      STRING,</strong></span>
<span class="strong"><strong>              relationship    STRING,</strong></span>
<span class="strong"><strong>              race            STRING,</strong></span>
<span class="strong"><strong>              gender          STRING,</strong></span>
<span class="strong"><strong>              capitalgain     INT,</strong></span>
<span class="strong"><strong>              capitalloss     INT,</strong></span>
<span class="strong"><strong>              nativecountry   STRING,</strong></span>
<span class="strong"><strong>              income          STRING</strong></span>
<span class="strong"><strong>            )</strong></span>
<span class="strong"><strong>    </strong></span>
<span class="strong"><strong>                     ")</strong></span>
</pre></div><p>Next, a row count is<a id="id249" class="indexterm"></a> taken from the table called <code class="literal">adult2</code> via a <code class="literal">COUNT(*)</code>, and the output value is printed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>   val resRDD = hiveContext.sql("SELECT COUNT(*) FROM adult2")</strong></span>

<span class="strong"><strong>   resRDD.map(t =&gt; "Count : " + t(0) ).collect().foreach(println)</strong></span>
</pre></div><p>As expected, there are no rows in the table.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Count : 0</strong></span>
</pre></div><p>It is also possible to create Hive-based external tables in Apache Spark Hive. The following HDFS file listing shows that the CSV file called <code class="literal">adult.train.data2</code> exists in the HDFS directory called <code class="literal">/data/spark/hive</code>, and it contains data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn hive]$ hdfs dfs -ls /data/spark/hive</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup    4171350 2015-06-24 15:18 /data/spark/hive/adult.train.data2</strong></span>
</pre></div><p>Now, I adjust my Scala-based Hive SQL to create an external table called <code class="literal">adult3</code> (if it does not exist), which<a id="id250" class="indexterm"></a> has the same structure as the previous table. The<a id="id251" class="indexterm"></a> row format in this table-create statement specifies a comma as a row column delimiter, as would be expected for CSV data. The location option in this statement specifies the <code class="literal">/data/spark/hive</code> directory on HDFS for data. So, there can be multiple files on HDFS, in this location, to populate this table. Each file would need to have the same data structure matching this table structure:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    hiveContext.sql("</strong></span>

<span class="strong"><strong>        CREATE EXTERNAL TABLE IF NOT EXISTS adult3</strong></span>
<span class="strong"><strong>           (</strong></span>
<span class="strong"><strong>             idx             INT,</strong></span>
<span class="strong"><strong>             age             INT,</strong></span>
<span class="strong"><strong>             workclass       STRING,</strong></span>
<span class="strong"><strong>             fnlwgt          INT,</strong></span>
<span class="strong"><strong>             education       STRING,</strong></span>
<span class="strong"><strong>             educationnum    INT,</strong></span>
<span class="strong"><strong>             maritalstatus   STRING,</strong></span>
<span class="strong"><strong>             occupation      STRING,</strong></span>
<span class="strong"><strong>             relationship    STRING,</strong></span>
<span class="strong"><strong>             race            STRING,</strong></span>
<span class="strong"><strong>             gender          STRING,</strong></span>
<span class="strong"><strong>             capitalgain     INT,</strong></span>
<span class="strong"><strong>             capitalloss     INT,</strong></span>
<span class="strong"><strong>             nativecountry   STRING,</strong></span>
<span class="strong"><strong>             income          STRING</strong></span>
<span class="strong"><strong>           )</strong></span>
<span class="strong"><strong>           ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</strong></span>
<span class="strong"><strong>           LOCATION '/data/spark/hive'</strong></span>

<span class="strong"><strong>                   ")</strong></span>
</pre></div><p>A row count is then taken against the <code class="literal">adult3</code> table, and the count is printed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val resRDD = hiveContext.sql("SELECT COUNT(*) FROM adult3")</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; "Count : " + t(0) ).collect().foreach(println)</strong></span>
</pre></div><p>As you can see, the table now contains around 32,000 rows. Since this is an external table, the HDFS-based data has not been moved, and the row calculation has been derived from the underlying CSV-based data.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Count : 32561</strong></span>
</pre></div><p>It occurs to me that I <a id="id252" class="indexterm"></a>want to start stripping dimension data out of the raw CSV-based data in the external <code class="literal">adult3</code> table. After all, Hive is a data warehouse, so a part of a general ETL chain <a id="id253" class="indexterm"></a>using the raw CSV-based data would strip dimensions and objects from the data, and create new tables. If I consider the education dimension, and try to determine what unique values exist, then for instance, the SQL would be as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    val resRDD = hiveContext.sql("</strong></span>

<span class="strong"><strong>       SELECT DISTINCT education AS edu FROM adult3</strong></span>
<span class="strong"><strong>         ORDER BY edu</strong></span>

<span class="strong"><strong>                   ")</strong></span>

<span class="strong"><strong>    resRDD.map(t =&gt; t(0) ).collect().foreach(println)</strong></span>
</pre></div><p>And the ordered data matches the values that were derived earlier in this chapter using Spark SQL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> 10th</strong></span>
<span class="strong"><strong> 11th</strong></span>
<span class="strong"><strong> 12th</strong></span>
<span class="strong"><strong> 1st-4th</strong></span>
<span class="strong"><strong> 5th-6th</strong></span>
<span class="strong"><strong> 7th-8th</strong></span>
<span class="strong"><strong> 9th</strong></span>
<span class="strong"><strong> Assoc-acdm</strong></span>
<span class="strong"><strong> Assoc-voc</strong></span>
<span class="strong"><strong> Bachelors</strong></span>
<span class="strong"><strong> Doctorate</strong></span>
<span class="strong"><strong> HS-grad</strong></span>
<span class="strong"><strong> Masters</strong></span>
<span class="strong"><strong> Preschool</strong></span>
<span class="strong"><strong> Prof-school</strong></span>
<span class="strong"><strong> Some-college</strong></span>
</pre></div><p>This is useful, but what if I want to create dimension values, and then assign integer index values to<a id="id254" class="indexterm"></a> each of the previous education dimension values. For instance, <code class="literal">10th</code> would be <code class="literal">0</code>, and <code class="literal">11th</code> would be <code class="literal">1</code>. I have set up a dimension CSV file for the education dimension on HDFS, as<a id="id255" class="indexterm"></a> shown here. The contents just contain the list of unique values, and an index:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn hive]$ hdfs dfs -ls /data/spark/dim1/</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup        174 2015-06-25 14:08 /data/spark/dim1/education.csv</strong></span>
<span class="strong"><strong>[hadoop@hc2nn hive]$ hdfs dfs -cat /data/spark/dim1/education.csv</strong></span>
<span class="strong"><strong>1,10th</strong></span>
<span class="strong"><strong>2,11th</strong></span>
<span class="strong"><strong>3,12th</strong></span>
</pre></div><p>Now, I can run some Hive QL in my Apache application to create an education dimension table. First, I drop the education table if it already exists, then I create the table by parsing the HDFS CSV file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    hiveContext.sql("  DROP TABLE IF EXISTS education ")</strong></span>
<span class="strong"><strong>    hiveContext.sql("</strong></span>

<span class="strong"><strong>      CREATE TABLE IF NOT EXISTS  education</strong></span>
<span class="strong"><strong>        (</strong></span>
<span class="strong"><strong>          idx        INT,</strong></span>
<span class="strong"><strong>          name       STRING</strong></span>
<span class="strong"><strong>        )</strong></span>
<span class="strong"><strong>        ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</strong></span>
<span class="strong"><strong>        LOCATION '/data/spark/dim1/'</strong></span>
<span class="strong"><strong>                   ")</strong></span>
</pre></div><p>I can then select the contents of the new education table to ensure that it looks correct.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val resRDD = hiveContext.sql(" SELECT * FROM education ")</strong></span>
<span class="strong"><strong>resRDD.map( t =&gt; t(0)+" "+t(1) ).collect().foreach(println)</strong></span>
</pre></div><p>This gives the expected list of indexes and the education dimension values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>1 10th</strong></span>
<span class="strong"><strong>2 11th</strong></span>
<span class="strong"><strong>3 12th</strong></span>
<span class="strong"><strong>………</strong></span>
<span class="strong"><strong>16 Some-college</strong></span>
</pre></div><p>So, I have the beginnings of an ETL pipeline. The raw CSV data is being used as external tables, and the dimension tables are being created, which could then be used to convert the dimensions in the raw data to numeric indexes. I have now successfully created a Spark application, which uses a Hive context to connect to a Hive Metastore server, which allows me to create and populate tables.</p><p>I have the Hadoop <a id="id256" class="indexterm"></a>stack Cloudera CDH 5.3 installed on my Linux servers. I am using it for HDFS access while<a id="id257" class="indexterm"></a> writing this book, and I also have Hive and Hue installed and running (CDH install information can be found at the <a id="id258" class="indexterm"></a>Cloudera website at <a class="ulink" href="http://cloudera.com/content/cloudera/en/documentation.html" target="_blank">http://cloudera.com/content/cloudera/en/documentation.html</a>). When I check HDFS for the <code class="literal">adult3</code> table, which should have been created under <code class="literal">/user/hive/warehouse</code>, I see the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn hive]$ hdfs dfs -ls /user/hive/warehouse/adult3</strong></span>
<span class="strong"><strong>ls: `/user/hive/warehouse/adult3': No such file or directory</strong></span>
</pre></div><p>The Hive-based table does not exist in the expected place for Hive. I can confirm this by checking the Hue Metastore manager to see what tables exist in the default database. The following figure shows that my default database is currently empty. I have added red lines to show that I am currently looking at the default database, and that there is no data. Clearly, when I run an Apache Spark-based application, with a Hive context, I am connecting to a Hive Metastore server. I know this because the log indicates that this is the case and also, my tables created in this way persist when Apache Spark is restarted.</p><div class="mediaobject"><img src="graphics/B01989_04_01.jpg" /></div><p>The Hive context within the application that was just run has used a local Hive Metastore server, and has stored <a id="id259" class="indexterm"></a>data to a local location; actually in this case under <code class="literal">/tmp</code> on HDFS. I now want to use the Hive-based Metastore server, so that I can <a id="id260" class="indexterm"></a>create tables and data in Hive directly. The next section will show how this can be done.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec41"></a>A Hive-based Metastore server</h3></div></div></div><p>I already mentioned that I am<a id="id261" class="indexterm"></a> using Cloudera's CDH 5.3 Hadoop stack. I have Hive, HDFS, Hue, and Zookeeper running. I am using Apache <a id="id262" class="indexterm"></a>Spark 1.3.1 installed under <code class="literal">/usr/local/spark</code>, in order to create and run applications (I know that CDH 5.3 is released with Spark 1.2, but I wanted to use DataFrames in this instance, which were available in Spark 1.3.x.).</p><p>The first thing that I need to do to configure Apache Spark to connect to Hive, is to drop the Hive configuration file called <code class="literal">hive-site.xml</code> into the Spark configuration directory on all servers where Spark is installed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn bin]# cp /var/run/cloudera-scm-agent/process/1237-hive-HIVEMETASTORE/hive-site.xml /usr/local/spark/conf</strong></span>
</pre></div><p>Then, given that I have installed Apache Hive via the CDH Manager to be able to use PostgreSQL, I need to install a PostgreSQL connector JAR for Spark, else it won't know how to connect to Hive, and errors like this will occur:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>15/06/25 16:32:24 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (s)</strong></span>
<span class="strong"><strong>Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.</strong></span>
<span class="strong"><strong>Caused by: java.lang.reflect.InvocationTargetException</strong></span>
<span class="strong"><strong>Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factor</strong></span>
<span class="strong"><strong>Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "dbcp-builtin" pnectionPool gave an </strong></span>
<span class="strong"><strong>error : The specified datastore driver ("org.postgresql.Driver") was not f. Please check your CLASSPATH</strong></span>
<span class="strong"><strong>specification, and the name of the driver.</strong></span>
<span class="strong"><strong>Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The spver</strong></span>
<span class="strong"><strong>("org.postgresql.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specme of the driver.</strong></span>
</pre></div><p>I have stripped that error message down to just the pertinent parts, otherwise it would have been many pages long. I have determined the version of PostgreSQL that I have installed, as follows. It appears to<a id="id263" class="indexterm"></a> be of version 9.0, determined from the Cloudera parcel-based jar file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn jars]# pwd ; ls postgresql*</strong></span>
<span class="strong"><strong>/opt/cloudera/parcels/CDH/jars</strong></span>
<span class="strong"><strong>postgresql-9.0-801.jdbc4.jar</strong></span>
</pre></div><p>Next, I have used the <a class="ulink" href="https://jdbc.postgresql.org/" target="_blank">https://jdbc.postgresql.org/</a> website to download the necessary<a id="id264" class="indexterm"></a> PostgreSQL connector library. I have determined my Java version to be 1.7, as shown here, which affects <a id="id265" class="indexterm"></a>which version of library to use:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn spark]$ java -version</strong></span>
<span class="strong"><strong>java version "1.7.0_75"</strong></span>
<span class="strong"><strong>OpenJDK Runtime Environment (rhel-2.5.4.0.el6_6-x86_64 u75-b13)</strong></span>
<span class="strong"><strong>OpenJDK 64-Bit Server VM (build 24.75-b04, mixed mode)</strong></span>
</pre></div><p>The site says that if you are using Java 1.7 or 1.8, then you should use the JDBC41 version of the library. So, I have sourced the <code class="literal">postgresql-9.4-1201.jdbc41.jar</code> file. The next step is to copy this file to the Apache Spark install <code class="literal">lib</code> directory, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn lib]$ pwd ; ls -l postgresql*</strong></span>
<span class="strong"><strong>/usr/local/spark/lib</strong></span>
<span class="strong"><strong>-rw-r--r-- 1 hadoop hadoop 648487 Jun 26 13:20 postgresql-9.4-1201.jdbc41.jar</strong></span>
</pre></div><p>Now, the PostgreSQL library must be added to the Spark <code class="literal">CLASSPATH</code>, by adding an entry to the file called <code class="literal">compute-classpath.sh</code>, in the Spark <code class="literal">bin</code> directory, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn bin]$ pwd ; tail compute-classpath.sh</strong></span>
<span class="strong"><strong>/usr/local/spark/bin</strong></span>

<span class="strong"><strong># add postgresql connector to classpath</strong></span>
<span class="strong"><strong>appendToClasspath "${assembly_folder}"/postgresql-9.4-1201.jdbc41.jar</strong></span>

<span class="strong"><strong>echo "$CLASSPATH"</strong></span>
</pre></div><p>In my case, I encountered an error regarding Hive versions between CDH 5.3 Hive and Apache Spark as shown here. I thought that the versions were so close that I should be able to ignore this error:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Caused by: MetaException(message:Hive Schema version 0.13.1aa does not match metastore's schema version 0.13.0</strong></span>

<span class="strong"><strong>Metastore is not upgraded or corrupt)</strong></span>
</pre></div><p>I decided, in this case, to switch off schema verification in my Spark version of the <code class="literal">hive-site.xml</code> file. This had to be done in all the Spark-based instances of this file, and then Spark restarted. The change is shown here; the value is set to <code class="literal">false</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  &lt;property&gt;</strong></span>
<span class="strong"><strong>    &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;</strong></span>
<span class="strong"><strong>    &lt;value&gt;false&lt;/value&gt;</strong></span>
<span class="strong"><strong>  &lt;/property&gt;</strong></span>
</pre></div><p>Now, when I run the same set <a id="id266" class="indexterm"></a>of application-based SQL as the<a id="id267" class="indexterm"></a> last section, I can create objects in the Apache Hive default database. First, I will create the empty table called <code class="literal">adult2</code> using the Spark-based Hive context:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    hiveContext.sql( "</strong></span>

<span class="strong"><strong>        CREATE TABLE IF NOT EXISTS adult2</strong></span>
<span class="strong"><strong>           (</strong></span>
<span class="strong"><strong>             idx             INT,</strong></span>
<span class="strong"><strong>             age             INT,</strong></span>
<span class="strong"><strong>             workclass       STRING,</strong></span>
<span class="strong"><strong>             fnlwgt          INT,</strong></span>
<span class="strong"><strong>             education       STRING,</strong></span>
<span class="strong"><strong>             educationnum    INT,</strong></span>
<span class="strong"><strong>             maritalstatus   STRING,</strong></span>
<span class="strong"><strong>             occupation      STRING,</strong></span>
<span class="strong"><strong>             relationship    STRING,</strong></span>
<span class="strong"><strong>             race            STRING,</strong></span>
<span class="strong"><strong>             gender          STRING,</strong></span>
<span class="strong"><strong>             capitalgain     INT,</strong></span>
<span class="strong"><strong>             capitalloss     INT,</strong></span>
<span class="strong"><strong>             nativecountry   STRING,</strong></span>
<span class="strong"><strong>             income          STRING</strong></span>
<span class="strong"><strong>           )</strong></span>

<span class="strong"><strong>                    ")</strong></span>
</pre></div><p>As you can see, when I run the application and check the Hue Metastore browser, the table <code class="literal">adult2</code> now exists:</p><div class="mediaobject"><img src="graphics/B01989_04_02.jpg" /></div><p>I have shown the table <a id="id268" class="indexterm"></a>entry previously, and it's structure is<a id="id269" class="indexterm"></a> obtained by selecting the table entry called <code class="literal">adult2</code>, in the Hue default database browser:</p><div class="mediaobject"><img src="graphics/B01989_04_03.jpg" /></div><p>Now the external table <code class="literal">adult3</code>  Spark based Hive QL can be executed and data access confirmed from Hue. In the <a id="id270" class="indexterm"></a>last section, the necessary Hive QL was as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    hiveContext.sql("</strong></span>

<span class="strong"><strong>        CREATE EXTERNAL TABLE IF NOT EXISTS adult3</strong></span>
<span class="strong"><strong>           (</strong></span>
<span class="strong"><strong>             idx             INT,</strong></span>
<span class="strong"><strong>             age             INT,</strong></span>
<span class="strong"><strong>             workclass       STRING,</strong></span>
<span class="strong"><strong>             fnlwgt          INT,</strong></span>
<span class="strong"><strong>             education       STRING,</strong></span>
<span class="strong"><strong>             educationnum    INT,</strong></span>
<span class="strong"><strong>             maritalstatus   STRING,</strong></span>
<span class="strong"><strong>             occupation      STRING,</strong></span>
<span class="strong"><strong>             relationship    STRING,</strong></span>
<span class="strong"><strong>             race            STRING,</strong></span>
<span class="strong"><strong>             gender          STRING,</strong></span>
<span class="strong"><strong>             capitalgain     INT,</strong></span>
<span class="strong"><strong>             capitalloss     INT,</strong></span>
<span class="strong"><strong>             nativecountry   STRING,</strong></span>
<span class="strong"><strong>             income          STRING</strong></span>
<span class="strong"><strong>           )</strong></span>
<span class="strong"><strong>           ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</strong></span>
<span class="strong"><strong>           LOCATION '/data/spark/hive'</strong></span>

<span class="strong"><strong>                   ")</strong></span>
</pre></div><p>As you can now see, the<a id="id271" class="indexterm"></a> Hive-based table called <code class="literal">adult3</code> has been<a id="id272" class="indexterm"></a> created in the default database by Spark. The following figure is again generated from the Hue Metastore browser:</p><div class="mediaobject"><img src="graphics/B01989_04_04.jpg" /></div><p>The following Hive QL has been executed from the Hue Hive query editor. It shows that the <code class="literal">adult3</code> table is accessible from Hive. I have limited the rows to make the image presentable. I am not worried about the data, only the fact that I can access it:</p><div class="mediaobject"><img src="graphics/B01989_04_05.jpg" /></div><p>The last thing that I will <a id="id273" class="indexterm"></a>mention in this section which will be useful when using Hive QL from Spark against Hive, will be user-defined functions or <a id="id274" class="indexterm"></a>UDF's. As an example, I will consider the <code class="literal">row_sequence</code> function, which is used in the following Scala-based code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hiveContext.sql("</strong></span>

<span class="strong"><strong>ADD JAR /opt/cloudera/parcels/CDH-5.3.3-1.cdh5.3.3.p0.5/jars/hive-contrib-0.13.1-cdh5.3.3.jar</strong></span>

<span class="strong"><strong>  ")</strong></span>

<span class="strong"><strong>hiveContext.sql("</strong></span>

<span class="strong"><strong>CREATE TEMPORARY FUNCTION row_sequence as 'org.apache.hadoop.hive.contrib.udf.UDFRowSequence';</strong></span>

<span class="strong"><strong>  ")</strong></span>

<span class="strong"><strong>     val resRDD = hiveContext.sql("</strong></span>

<span class="strong"><strong>          SELECT row_sequence(),t1.edu FROM</strong></span>
<span class="strong"><strong>            ( SELECT DISTINCT education AS edu FROM adult3 ) t1</strong></span>
<span class="strong"><strong>          ORDER BY t1.edu</strong></span>

<span class="strong"><strong>                    ")</strong></span>
</pre></div><p>Either existing, or your own, JAR-based libraries can be made available to your Spark Hive session via the <code class="literal">ADD JAR</code> command. Then, the functionality within that library can be registered as a temporary function with <code class="literal">CREATE TEMPORARY FUNCTION</code> using the package-based class name. Then, the new function name can be incorporated in Hive QL statements.</p><p>This chapter has managed to <a id="id275" class="indexterm"></a>connect an Apache Spark-based application to Hive, and run Hive QL against Hive, so that table and data changes <a id="id276" class="indexterm"></a>persist in Hive. But why is this important? Well, Spark is an in-memory parallel processing system. It is an order faster than Hadoop-based Map Reduce in processing speed. Apache Spark can now be used as a processing engine, whereas the Hive data warehouse can be used for storage. Fast in-memory Spark-based processing speed coupled with big data scale structured data warehouse storage available in Hive.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec29"></a>Summary</h2></div></div><hr /></div><p>This chapter started by explaining the Spark SQL context, and file I/O methods. It then showed that Spark and HDFS-based data could be manipulated, as both DataFrames with SQL-like methods and with Spark SQL by registering temporary tables. Next, user-defined functions were introduced to show that the functionality of Spark SQL could be extended by creating new functions to suit your needs, registering them as UDF's, and then calling them in SQL to process data.</p><p>Finally, the Hive context was introduced for use in Apache Spark. Remember that the Hive context in Spark offers a super set of the functionality of the SQL context. I understand that over time, the SQL context is going to be extended to match the Hive Context functionality. Hive QL data processing in Spark using a Hive context was shown using both, a local Hive, and a Hive-based Metastore server. I believe that the latter configuration is better, as the tables are created, and data changes persist in your Hive instance.</p><p>In my case, I used Cloudera CDH 5.3, which used Hive 0.13, PostgreSQL, ZooKeeper, and Hue. I also used Apache Spark version 1.3.1. The configuration setup that I have shown you is purely for this configuration. If you wanted to use MySQL, for instance, you would need to research the necessary changes. A good place to start would be the <code class="email">&lt;<a class="email" href="mailto:user@spark.apache.org">user@spark.apache.org</a>&gt;</code> mailing list.</p><p>Finally, I would say that Apache Spark Hive context configuration, with Hive-based storage, is very useful. It allows you to use Hive as a big data scale data warehouse, with Apache Spark for fast in-memory processing. It offers you the ability to manipulate your data with not only the Spark-based modules (MLlib, SQL, GraphX, and Stream), but also other Hadoop-based tools, making it easier to create ETL chains.</p><p>The next chapter will examine the Spark graph processing module, GraphX, it will also investigate the Neo4J graph database, and the MazeRunner application.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5. Apache Spark GraphX</h2></div></div></div><p>In this chapter, I want to examine the Apache Spark GraphX module, and graph processing in general. I also want to briefly examine graph-based storage by looking at the graph database called Neo4j. So, this chapter will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>GraphX coding</p></li><li style="list-style-type: disc"><p>Mazerunner for Neo4j</p></li></ul></div><p>The GraphX coding<a id="id277" class="indexterm"></a> section, written in Scala, will provide a series of graph coding examples. The work carried out on the experimental Mazerunner product by<a id="id278" class="indexterm"></a> Kenny Bastani, which I will also examine, ties the two topics together in one practical example. It provides an example prototype-based on Docker to replicate data between Apache Spark GraphX, and Neo4j storage.</p><p>Before writing code in Scala to use the Spark GraphX module, I think it would be useful to provide an overview of what a graph actually is in terms of graph processing. The following section provides a brief introduction using a couple of simple graphs as examples.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec30"></a>Overview</h2></div></div><hr /></div><p>A graph can be <a id="id279" class="indexterm"></a>considered to be a data structure, which consists of a group of vertices, and edges that connect them. The vertices or nodes in the graph can be objects or perhaps, people, and the edges are the relationships between them. The edges can be directional, meaning that the relationship operates from one node to the next. For instance, node A is the father of node B.</p><p>In the following diagram, the circles represent the vertices or nodes (<span class="strong"><strong>A</strong></span> to <span class="strong"><strong>D</strong></span>), whereas the thick lines represent the edges, or relationships between them (<span class="strong"><strong>E1</strong></span> to <span class="strong"><strong>E6</strong></span>). Each node, or edge may have properties, and these values are represented by the associated grey squares (<span class="strong"><strong>P1</strong></span> to <span class="strong"><strong>P7</strong></span>).</p><p>So, if a graph represented a physical route map for route finding, then the edges might represent minor roads or motorways. The nodes would be motorway junctions, or road intersections. The node and edge properties might be the road type, speed limit, distance, and the cost and grid locations.</p><p>There are many types<a id="id280" class="indexterm"></a> of graph implementation, but some examples are fraud modeling, financial currency transaction modeling, social modeling (as in friend-to-friend connections on Facebook), map processing, web processing, and page ranking.</p><div class="mediaobject"><img src="graphics/B01989_05_01.jpg" /></div><p>The previous diagram shows a generic example of a graph with associated properties. It also shows that the edge relationships can be directional, that is, the <span class="strong"><strong>E2</strong></span> edge acts from node <span class="strong"><strong>B</strong></span> to node <span class="strong"><strong>C</strong></span>. However, the following example uses family members, and the relationships between them to create a graph. Note that there can be multiple edges between two nodes or vertices. For instance, the husband-and-wife relationships between <span class="strong"><strong>Mike</strong></span> and <span class="strong"><strong>Sarah</strong></span>. Also, it is possible that there could be multiple properties on a node or edge.</p><div class="mediaobject"><img src="graphics/B01989_05_02.jpg" /></div><p>So, in the previous example, the <a id="id281" class="indexterm"></a>
<span class="strong"><strong>Sister</strong></span> property acts from node 6 <span class="strong"><strong>Flo</strong></span>, to node 1, <span class="strong"><strong>Mike</strong></span>. These are simple graphs to explain the structure of a graph, and the element nature. Real graph applications<a id="id282" class="indexterm"></a> can reach extreme sizes, and require both, distributed processing, and storage to enable them to be manipulated. Facebook is <a id="id283" class="indexterm"></a>able to process graphs, containing over 1 trillion edges using <span class="strong"><strong>Apache Giraph</strong></span> (source: Avery Ching-Facebook). Giraph is an Apache Hadoop eco-system tool for graph processing, which has historically based its processing on Map Reduce, but now uses TinkerPop, which will be introduced in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Graph-based Storage</em></span>. Although this book concentrates on Apache Spark, the number of edges provides a very impressive indicator of the size that a graph can reach.</p><p>In the next section, I will examine the use of the Apache Spark GraphX module using Scala.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec31"></a>GraphX coding</h2></div></div><hr /></div><p>This section will<a id="id284" class="indexterm"></a> examine Apache Spark GraphX programming in Scala, using the family relationship graph data sample, which was shown in the last section. This data will be stored on HDFS, and will be accessed as a list of vertices and edges. Although this data<a id="id285" class="indexterm"></a> set is small, the graphs that you build in this way could be very large. I have used HDFS for storage, because if your graph scales to the big data scale, then you will need some type of distributed and redundant storage. As this chapter shows by way of example, that could be HDFS. Using the Apache Spark SQL module, the storage could also be Apache Hive; see <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Apache Spark SQL</em></span>, for details.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec42"></a>Environment</h3></div></div></div><p>I have used the hadoop Linux account on the server <code class="literal">hc2nn</code> to develop the Scala-based GraphX code. The structure<a id="id286" class="indexterm"></a> for SBT compilation follows the same pattern as the previous examples, with the code tree existing in a subdirectory named <code class="literal">graphx</code>, where an <code class="literal">sbt</code> configuration file called <code class="literal">graph.sbt</code> resides:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn graphx]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/graphx</strong></span>

<span class="strong"><strong>[hadoop@hc2nn graphx]$ ls</strong></span>
<span class="strong"><strong>   src   graph.sbt          project     target</strong></span>
</pre></div><p>The source code lives, as expected, under a subtree of this level called <code class="literal">src/main/scala</code>, and contains five code samples:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn scala]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/graphx/src/main/scala</strong></span>

<span class="strong"><strong>[hadoop@hc2nn scala]$ ls</strong></span>
<span class="strong"><strong>graph1.scala  graph2.scala  graph3.scala  graph4.scala  graph5.scala</strong></span>
</pre></div><p>In each graph-based example, the Scala file uses the same code to load data from HDFS, and to create a graph; but then, each file provides a different facet of GraphX-based graph processing. As a different Spark module is being used in this chapter, the <code class="literal">sbt</code> configuration file <code class="literal">graph.sbt</code> has been changed to support this work:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn graphx]$ more graph.sbt</strong></span>

<span class="strong"><strong>name := "Graph X"</strong></span>


<span class="strong"><strong>version := "1.0"</strong></span>

<span class="strong"><strong>scalaVersion := "2.10.4"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.spark" %% "spark-core"  % "1.0.0"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.spark" %% "spark-graphx" % "1.0.0"</strong></span>

<span class="strong"><strong>// If using CDH, also add Cloudera repo</strong></span>
<span class="strong"><strong>resolvers += "Cloudera Repository" at https://repository.cloudera.com/artifactory/cloudera-repos/</strong></span>
</pre></div><p>The contents of the <code class="literal">graph.sbt</code> file are shown previously, via the Linux <code class="literal">more</code> command. There are only two changes here to note from previous examples—the value of name has changed to represent the content. Also, more importantly, the Spark GraphX 1.0.0 library has been added as a library dependency.</p><p>Two data files have been <a id="id287" class="indexterm"></a>placed on HDFS, under the <code class="literal">/data/spark/graphx/</code> directory. They contain the data that will be used for this section in terms of the vertices, and edges that make up a graph. As the Hadoop file system <code class="literal">ls</code> command shows next, the files are called <code class="literal">graph1_edges.cvs</code> and <code class="literal">graph1_vertex.csv</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn scala]$ hdfs dfs -ls /data/spark/graphx</strong></span>
<span class="strong"><strong>Found 2 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup        129 2015-03-01 13:52 /data/spark/graphx/graph1_edges.csv</strong></span>
<span class="strong"><strong>-rw-r--r--   3 hadoop supergroup         59 2015-03-01 13:52 /data/spark/graphx/graph1_vertex.csv</strong></span>
</pre></div><p>The <code class="literal">vertex</code> file, shown next, via a Hadoop file system <code class="literal">cat</code> command, contains just six lines, representing the graph used in the last section. Each vertex represents a person, and has a vertex ID number, a name and an age value:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn scala]$ hdfs dfs -cat /data/spark/graphx/graph1_vertex.csv</strong></span>
<span class="strong"><strong>1,Mike,48</strong></span>
<span class="strong"><strong>2,Sarah,45</strong></span>
<span class="strong"><strong>3,John,25</strong></span>
<span class="strong"><strong>4,Jim,53</strong></span>
<span class="strong"><strong>5,Kate,22</strong></span>
<span class="strong"><strong>6,Flo,52</strong></span>
</pre></div><p>The edge file contains a set of directed edge values in the form of source vertex ID, destination vertex ID, and relationship. So, record one forms a Sister relationship between <code class="literal">Flo</code> and <code class="literal">Mike</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn scala]$  hdfs dfs -cat /data/spark/graphx/graph1_edges.csv</strong></span>
<span class="strong"><strong>6,1,Sister</strong></span>
<span class="strong"><strong>1,2,Husband</strong></span>
<span class="strong"><strong>2,1,Wife</strong></span>
<span class="strong"><strong>5,1,Daughter</strong></span>
<span class="strong"><strong>5,2,Daughter</strong></span>
<span class="strong"><strong>3,1,Son</strong></span>
<span class="strong"><strong>3,2,Son</strong></span>
<span class="strong"><strong>4,1,Friend</strong></span>
<span class="strong"><strong>1,5,Father</strong></span>
<span class="strong"><strong>1,3,Father</strong></span>
<span class="strong"><strong>2,5,Mother</strong></span>
<span class="strong"><strong>2,3,Mother</strong></span>
</pre></div><p>Having explained the sbt environment, and the HDFS-based data, we are now ready to examine some of the GraphX code samples. As in the previous examples, the code can be compiled, and packaged<a id="id288" class="indexterm"></a> as follows from the <code class="literal">graphx</code> subdirectory. This creates a JAR called <code class="literal">graph-x_2.10-1.0.jar</code> from which the example applications can be run:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn graphx]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/graphx</strong></span>

<span class="strong"><strong>[hadoop@hc2nn graphx]$  sbt package</strong></span>

<span class="strong"><strong>Loading /usr/share/sbt/bin/sbt-launch-lib.bash</strong></span>
<span class="strong"><strong>[info] Set current project to Graph X (in build file:/home/hadoop/spark/graphx/)</strong></span>
<span class="strong"><strong>[info] Compiling 5 Scala sources to /home/hadoop/spark/graphx/target/scala-2.10/classes...</strong></span>
<span class="strong"><strong>[info] Packaging /home/hadoop/spark/graphx/target/scala-2.10/graph-x_2.10-1.0.jar ...</strong></span>
<span class="strong"><strong>[info] Done packaging.</strong></span>
<span class="strong"><strong>[success] Total time: 30 s, completed Mar 3, 2015 5:27:10 PM</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec43"></a>Creating a graph</h3></div></div></div><p>This section will<a id="id289" class="indexterm"></a> explain the generic Scala code, up to the point of creating a GraphX graph, from the HDFS-based data. This will save time, as the same code is reused in each example. Once this is explained, I will concentrate on the actual graph-based manipulation in each code example:</p><p>The generic code starts by importing the Spark context, graphx, and RDD functionality for use in the Scala code:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD</pre></div><p>Then, an application is<a id="id290" class="indexterm"></a> defined, which extends the <code class="literal">App</code> class, and the application name changes, for each example, from <code class="literal">graph1</code> to <code class="literal">graph5</code>. This application name will be used when running the application using <code class="literal">spark-submit</code>:</p><div class="informalexample"><pre class="programlisting">object graph1 extends App
{</pre></div><p>The data files are defined in terms of the HDFS server and port, the path that they reside under in HDFS and their file names. As already mentioned, there are two data files that contain the <code class="literal">vertex</code> and <code class="literal">edge</code> information:</p><div class="informalexample"><pre class="programlisting">  val hdfsServer = "hdfs://hc2nn.semtech-solutions.co.nz:8020"
  val hdfsPath   = "/data/spark/graphx/"
  val vertexFile = hdfsServer + hdfsPath + "graph1_vertex.csv"
  val edgeFile   = hdfsServer + hdfsPath + "graph1_edges.csv"</pre></div><p>The Spark Master URL is defined, as is the application name, which will appear in the Spark user interface when the application runs. A new Spark configuration object is created, and the URL and name are assigned to it:</p><div class="informalexample"><pre class="programlisting">  val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"
  val appName = "Graph 1"
  val conf = new SparkConf()
  conf.setMaster(sparkMaster)
  conf.setAppName(appName)</pre></div><p>A new Spark context is created using the configuration that was just defined:</p><div class="informalexample"><pre class="programlisting">  val sparkCxt = new SparkContext(conf)</pre></div><p>The vertex information from the HDFS-based file is then loaded into an RDD-based structure called <code class="literal">vertices</code> using the <code class="literal">sparkCxt.textFile</code> method. The data is stored as a long <code class="literal">VertexId</code>, and strings to represent the person's name and age. The data lines are split by commas as this is CSV based-data:</p><div class="informalexample"><pre class="programlisting">  val vertices: RDD[(VertexId, (String, String))] =
      sparkCxt.textFile(vertexFile).map { line =&gt;
        val fields = line.split(",")
        ( fields(0).toLong, ( fields(1), fields(2) ) )
  }</pre></div><p>Similary, the HDFS-based edge data is loaded into an RDD-based data structure called <code class="literal">edges</code>. The CSV-based data is again split by comma values. The first two data values are converted into Long<a id="id291" class="indexterm"></a> values, as they represent the source and destination vertex ID's. The final value, representing the relationship of the edge, is left as a string. Note that each record in the RDD structure edges is actually now an <code class="literal">Edge</code> record:</p><div class="informalexample"><pre class="programlisting">  val edges: RDD[Edge[String]] =
      sparkCxt.textFile(edgeFile).map { line =&gt;
        val fields = line.split(",")
        Edge(fields(0).toLong, fields(1).toLong, fields(2))
  }</pre></div><p>A default value is defined in case a connection, or a vertex is missing, then the graph is constructed from the RDD-based structures—<code class="literal">vertices</code>, <code class="literal">edges</code>, and the <code class="literal">default</code> record:</p><div class="informalexample"><pre class="programlisting">  val default = ("Unknown", "Missing")
  val graph = Graph(vertices, edges, default)</pre></div><p>This creates a GraphX-based structure called <code class="literal">graph</code>, which can now be used for each of the examples. Remember that although these data samples are small, you can create extremely large graphs using this approach. Many of these algorithms are iterative applications, for instance, PageRank and Triangle Count, and as a result, the programs will generate many iterative Spark jobs.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec44"></a>Example 1 – counting</h3></div></div></div><p>The graph has been loaded, and we know the data volumes in the data files, but what about the data <a id="id292" class="indexterm"></a>content in terms of vertices, and edges in the actual graph itself? It is very simple to extract this information by using the vertices, and the edges count function as shown here:</p><div class="informalexample"><pre class="programlisting">  println( "vertices : " + graph.vertices.count )
  println( "edges    : " + graph.edges.count )</pre></div><p>Running the <code class="literal">graph1</code> example, using the example name and the JAR file created previously, will provide the count information. The master URL is supplied to connect to the Spark cluster, and some default parameters are supplied for the executor memory, and the total executor cores:</p><div class="informalexample"><pre class="programlisting">spark-submit \
  --class graph1 \
  --master spark://hc2nn.semtech-solutions.co.nz:7077  \
  --executor-memory 700M \
  --total-executor-cores 100 \
 /home/hadoop/spark/graphx/target/scala-2.10/graph-x_2.10-1.0.jar</pre></div><p>The Spark cluster job called <code class="literal">graph1</code> provides the following output, which is as expected and also, it matches the<a id="id293" class="indexterm"></a> data files:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vertices : 6</strong></span>
<span class="strong"><strong>edges    : 12</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec45"></a>Example 2 – filtering</h3></div></div></div><p>What happens if we<a id="id294" class="indexterm"></a> need to create a subgraph from the main graph, and filter by the person's age or relationships? The example code from the second example Scala file, <code class="literal">graph2</code>, shows how this can be done:</p><div class="informalexample"><pre class="programlisting">  val c1 = graph.vertices.filter { case (id, (name, age)) =&gt; age.toLong &gt; 40 }.count

  val c2 = graph.edges.filter { case Edge(from, to, property)
    =&gt; property == "Father" | property == "Mother" }.count

  println( "Vertices count : " + c1 )
  println( "Edges    count : " + c2 )</pre></div><p>The two example counts have been created from the main graph. The first filters the person-based vertices on the age, only taking those people who are greater than 40 years old. Notice that the <code class="literal">age</code> value, which was stored as a string, has been converted into a long for comparison. The previous second example filters the edges on the relationship property of <code class="literal">Mother</code> or <code class="literal">Father</code>. The two count values: <code class="literal">c1</code> and <code class="literal">c2</code> are created, and printed as the Spark output shows here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Vertices count : 4</strong></span>
<span class="strong"><strong>Edges    count : 4</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec46"></a>Example 3 – PageRank</h3></div></div></div><p>The PageRank algorithm provides a ranking value for each of the vertices in a graph. It makes the assumption that the<a id="id295" class="indexterm"></a> vertices that are connected to the most edges are the most important ones. Search engines use PageRank to provide ordering for the page display during a web search:</p><div class="informalexample"><pre class="programlisting">  val tolerance = 0.0001
  val ranking = graph.pageRank(tolerance).vertices
  val rankByPerson = vertices.join(ranking).map {
    case (id, ( (person,age) , rank )) =&gt; (rank, id, person)
  }</pre></div><p>The previous example code creates a <code class="literal">tolerance</code> value, and calls the graph <code class="literal">pageRank</code> method using it. The vertices are then ranked into a new value ranking. In order to make the ranking more meaningful the ranking values are joined with the original vertices RDD. The <code class="literal">rankByPerson</code> value then contains the rank, vertex ID, and person's name.</p><p>The PageRank result, held in <code class="literal">rankByPerson</code>, is then printed record by record, using a case statement to<a id="id296" class="indexterm"></a> identify the record contents, and a format statement to print the contents. I did this, because I wanted to define the format of the rank value which can vary:</p><div class="informalexample"><pre class="programlisting">  rankByPerson.collect().foreach {
    case (rank, id, person) =&gt;
      println ( f"Rank $rank%1.2f id $id person $person")
  }</pre></div><p>The output from the application is then shown here. As expected, <code class="literal">Mike</code> and <code class="literal">Sarah</code> have the highest rank, as they have the most relationships:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Rank 0.15 id 4 person Jim</strong></span>
<span class="strong"><strong>Rank 0.15 id 6 person Flo</strong></span>
<span class="strong"><strong>Rank 1.62 id 2 person Sarah</strong></span>
<span class="strong"><strong>Rank 1.82 id 1 person Mike</strong></span>
<span class="strong"><strong>Rank 1.13 id 3 person John</strong></span>
<span class="strong"><strong>Rank 1.13 id 5 person Kate</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec47"></a>Example 4 – triangle counting</h3></div></div></div><p>The triangle count algorithm <a id="id297" class="indexterm"></a>provides a vertex-based count of the number of triangles, associated with this vertex. For instance, vertex <code class="literal">Mike</code> (1) is connected to <code class="literal">Kate</code> (5), who is connected to <code class="literal">Sarah</code> (2); <code class="literal">Sarah</code> is connected to <code class="literal">Mike</code> (1) and so, a triangle is formed. This can be useful for route finding, where minimum, triangle-free, spanning tree graphs need to be generated for route planning.</p><p>The code to execute a triangle count, and print it, is simple, as shown next. The graph <code class="literal">triangleCount</code> method is executed for the graph vertices. The result is saved in the value <code class="literal">tCount</code>, and then printed:</p><div class="informalexample"><pre class="programlisting">  val tCount = graph.triangleCount().vertices
  println( tCount.collect().mkString("\n") )</pre></div><p>The results of the application job show that the vertices called, <code class="literal">Flo</code> (4) and <code class="literal">Jim</code> (6), have no triangles, whereas <code class="literal">Mike</code> (1) and <code class="literal">Sarah</code> (2) have the most, as expected, as they have the most relationships:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(4,0)</strong></span>
<span class="strong"><strong>(6,0)</strong></span>
<span class="strong"><strong>(2,4)</strong></span>
<span class="strong"><strong>(1,4)</strong></span>
<span class="strong"><strong>(3,2)</strong></span>
<span class="strong"><strong>(5,2)</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec48"></a>Example 5 – connected components</h3></div></div></div><p>When a large graph is <a id="id298" class="indexterm"></a>created from the data, it might contain unconnected subgraphs, that is, subgraphs that are isolated from each other, and contain no bridging or connecting edges between them. This algorithm provides a measure of this connectivity. It might be important, depending upon your processing, to know that all the vertices are connected.</p><p>The Scala code, for this example, calls two graph methods: <code class="literal">connectedComponents</code>, and <code class="literal">stronglyConnectedComponents</code>. The strong method required a maximum iteration count, which has been set to <code class="literal">1000</code>. These counts are acting on the graph vertices:</p><div class="informalexample"><pre class="programlisting">  val iterations = 1000
  val connected  = graph.connectedComponents().vertices
  val connectedS = graph.stronglyConnectedComponents(iterations).vertices</pre></div><p>The vertex counts are then joined with the original vertex records, so that the connection counts can be associated with the vertex information, such as the person's name:</p><div class="informalexample"><pre class="programlisting">  val connByPerson = vertices.join(connected).map {
    case (id, ( (person,age) , conn )) =&gt; (conn, id, person)
  }

  val connByPersonS = vertices.join(connectedS).map {
    case (id, ( (person,age) , conn )) =&gt; (conn, id, person)
  }
The results are then output using a case statement, and formatted printing:
  connByPerson.collect().foreach {
    case (conn, id, person) =&gt;
      println ( f"Weak $conn  $id $person" )
  }</pre></div><p>As expected for the <code class="literal">connectedComponents</code> algorithm, the results show that for each vertex, there is only one component. This means that all the vertices are the members of a single graph, as the graph diagram earlier in the chapter showed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Weak 1  4 Jim</strong></span>
<span class="strong"><strong>Weak 1  6 Flo</strong></span>
<span class="strong"><strong>Weak 1  2 Sarah</strong></span>
<span class="strong"><strong>Weak 1  1 Mike</strong></span>
<span class="strong"><strong>Weak 1  3 John</strong></span>
<span class="strong"><strong>Weak 1  5 Kate</strong></span>
</pre></div><p>The <code class="literal">stronglyConnectedComponents</code> method gives a measure of the connectivity in a graph, taking into account the direction of the relationships between them. The results for the <code class="literal">stronglyConnectedComponents</code> algorithm output is as follows:</p><div class="informalexample"><pre class="programlisting">  connByPersonS.collect().foreach {
    case (conn, id, person) =&gt;
      println ( f"Strong $conn  $id $person" )
  }</pre></div><p>You might notice from the graph that the relationships, <code class="literal">Sister</code> and <code class="literal">Friend</code>, act from vertices <code class="literal">Flo</code> (6) and <code class="literal">Jim</code> (4), to <code class="literal">Mike</code> (1) as the edge and vertex data shows here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>6,1,Sister</strong></span>
<span class="strong"><strong>4,1,Friend</strong></span>

<span class="strong"><strong>1,Mike,48</strong></span>
<span class="strong"><strong>4,Jim,53</strong></span>
<span class="strong"><strong>6,Flo,52</strong></span>
</pre></div><p>So, the strong method<a id="id299" class="indexterm"></a> output shows that for most vertices, there is only one graph component signified by the <code class="literal">1</code> in the second column. However, vertices <code class="literal">4</code> and <code class="literal">6</code> are not reachable due to the direction of their relationship, and so they have a vertex ID instead of a component ID:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Strong 4  4 Jim</strong></span>
<span class="strong"><strong>Strong 6  6 Flo</strong></span>
<span class="strong"><strong>Strong 1  2 Sarah</strong></span>
<span class="strong"><strong>Strong 1  1 Mike</strong></span>
<span class="strong"><strong>Strong 1  3 John</strong></span>
<span class="strong"><strong>Strong 1  5 Kate</strong></span>
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec32"></a>Mazerunner for Neo4j</h2></div></div><hr /></div><p>In the previous sections, you have been shown how to write Apache Spark graphx code in Scala to process the HDFS-based graph data. You have been able to execute the graph-based algorithms, such as PageRank, and triangle counting. However, this approach has a limitation. Spark does not have storage, and storing graph-based data in the flat files on HDFS does not allow you to<a id="id300" class="indexterm"></a> manipulate it in its place of storage. For instance, if you had data stored in a relational database, you could use SQL to interrogate it in place. Databases such as Neo4j are graph databases. This means that their storage mechanisms and data access language act on graphs. In this section, I want to take a look at the work done on Mazerunner, created as a GraphX Neo4j processing prototype by Kenny Bastani.</p><p>The following figure describes the Mazerunner architecture. It shows that data in Neo4j is exported to HDFS, and processed by GraphX via a notification process. The GraphX data updates are then saved back to HDFS as a list of key value updates. These changes are then propagated to Neo4j to be stored. The algorithms in this prototype architecture are accessed via a Rest based HTTP URL, which will be shown later. The point here though, is that algorithms can be run via processing in graphx, but the data changes can be checked via Neo4j database cypher language queries. Kenny's work and further details can be found at: <a class="ulink" href="http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html" target="_blank">http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html</a>.</p><p>This section will be dedicated to<a id="id301" class="indexterm"></a> explaining the Mazerunner architecture, and will show, with the help of an example, how it can be used. This architecture provides a unique example of GraphX-based processing, coupled with graph-based storage.</p><div class="mediaobject"><img src="graphics/B01989_05_03.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec49"></a>Installing Docker</h3></div></div></div><p>The process for<a id="id302" class="indexterm"></a> installing the Mazerunner example<a id="id303" class="indexterm"></a> code is described via <a class="ulink" href="https://github.com/kbastani/neo4j-mazerunner" target="_blank">https://github.com/kbastani/neo4j-mazerunner</a>.</p><p>I have used the 64 bit Linux<a id="id304" class="indexterm"></a> Centos 6.5 machine <code class="literal">hc1r1m1</code> for the install. The Mazerunner example uses the Docker tool, which creates virtual containers with a small foot print for running HDFS, Neo4j, and Mazerunner in this example. First, I must install Docker. I have done this, as follows, using the Linux root user via <code class="literal">yum</code> commands. The first command installs the <code class="literal">docker-io</code> module (the docker name was already used for <a id="id305" class="indexterm"></a>CentOS 6.5 by another application):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 bin]# yum -y install docker-io</strong></span>
</pre></div><p>I needed to enable the <code class="literal">public_ol6_latest</code> repository, and install the <code class="literal">device-mapper-event-libs</code> package, as I found that my current lib-device-mapper, which I had installed, wasn't exporting<a id="id306" class="indexterm"></a> the symbol Base that Docker needed. I executed the following commands as <code class="literal">root</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 ~]# yum-config-manager --enable public_ol6_latest</strong></span>
<span class="strong"><strong>[root@hc1r1m1 ~]# yum install device-mapper-event-libs</strong></span>
</pre></div><p>The actual error that I encountered was as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/usr/bin/docker: relocation error: /usr/bin/docker: symbol dm_task_get_info_with_deferred_remove, version Base not defined in file libdevmapper.so.1.02 with link time reference</strong></span>
</pre></div><p>I can then check that Docker will run by checking the Docker version number with the following call:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 ~]# docker version</strong></span>
<span class="strong"><strong>Client version: 1.4.1</strong></span>
<span class="strong"><strong>Client API version: 1.16</strong></span>
<span class="strong"><strong>Go version (client): go1.3.3</strong></span>
<span class="strong"><strong>Git commit (client): 5bc2ff8/1.4.1</strong></span>
<span class="strong"><strong>OS/Arch (client): linux/amd64</strong></span>
<span class="strong"><strong>Server version: 1.4.1</strong></span>
<span class="strong"><strong>Server API version: 1.16</strong></span>
<span class="strong"><strong>Go version (server): go1.3.3</strong></span>
<span class="strong"><strong>Git commit (server): 5bc2ff8/1.4.1</strong></span>
</pre></div><p>I can start the Linux docker service using the following service command. I can also force Docker to start on Linux server startup using the following <code class="literal">chkconfig</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 bin]# service docker start</strong></span>
<span class="strong"><strong>[root@hc1r1m1 bin]# chkconfig docker on</strong></span>
</pre></div><p>The three Docker images (HDFS, Mazerunner, and Neo4j) can then be downloaded. They are large, so this may take some time:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 ~]# docker pull sequenceiq/hadoop-docker:2.4.1</strong></span>
<span class="strong"><strong>Status: Downloaded newer image for sequenceiq/hadoop-docker:2.4.1</strong></span>

<span class="strong"><strong>[root@hc1r1m1 ~]# docker pull kbastani/docker-neo4j:latest</strong></span>
<span class="strong"><strong>Status: Downloaded newer image for kbastani/docker-neo4j:latest</strong></span>

<span class="strong"><strong>[root@hc1r1m1 ~]# docker pull kbastani/neo4j-graph-analytics:latest</strong></span>
<span class="strong"><strong>Status: Downloaded newer image for kbastani/neo4j-graph-analytics:latest</strong></span>
</pre></div><p>Once downloaded, the Docker <a id="id307" class="indexterm"></a>containers can be started in the order; HDFS, Mazerunner, and then Neo4j. The default Neo4j movie database will be loaded and the Mazerunner algorithms run using this data. The HDFS container starts as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 ~]# docker run -i -t --name hdfs sequenceiq/hadoop-docker:2.4.1 /etc/bootstrap.sh –bash</strong></span>

<span class="strong"><strong>Starting sshd:                                [  OK  ]</strong></span>
<span class="strong"><strong>Starting namenodes on [26d939395e84]</strong></span>
<span class="strong"><strong>26d939395e84: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-26d939395e84.out</strong></span>
<span class="strong"><strong>localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-26d939395e84.out</strong></span>
<span class="strong"><strong>Starting secondary namenodes [0.0.0.0]</strong></span>
<span class="strong"><strong>0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-26d939395e84.out</strong></span>
<span class="strong"><strong>starting yarn daemons</strong></span>
<span class="strong"><strong>starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-26d939395e84.out</strong></span>
<span class="strong"><strong>localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-26d939395e84.out</strong></span>
</pre></div><p>The Mazerunner service<a id="id308" class="indexterm"></a> container starts as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 ~]# docker run -i -t --name mazerunner --link hdfs:hdfs kbastani/neo4j-graph-analytics</strong></span>
</pre></div><p>The output is long, so I will not include it all here, but you will see no errors. There also comes a line, which states that the install is waiting for messages:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[*] Waiting for messages. To exit press CTRL+C</strong></span>
</pre></div><p>In order to start the Neo4j container, I need the install to create a new Neo4j database for me, as this is a first time install. Otherwise on restart, I would just supply the path of the database directory. Using the <code class="literal">link</code> command, the Neo4j container is linked to the HDFS and Mazerunner containers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 ~]# docker run -d -P -v /home/hadoop/neo4j/data:/opt/data --name graphdb --link mazerunner:mazerunner --link hdfs:hdfs kbastani/docker-neo4j</strong></span>
</pre></div><p>By checking the <code class="literal">neo4j/data</code> path, I can now see that a database directory, named <code class="literal">graph.db</code> has been created:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 data]# pwd</strong></span>
<span class="strong"><strong>/home/hadoop/neo4j/data</strong></span>

<span class="strong"><strong>[root@hc1r1m1 data]# ls</strong></span>
<span class="strong"><strong>graph.db</strong></span>
</pre></div><p>I can then use the following <code class="literal">docker inspect</code> command, which the container-based IP address and the Docker-based Neo4j container is making available. The <code class="literal">inspect</code> command <a id="id309" class="indexterm"></a>supplies me with the local IP address that I will need to access the Neo4j container. The <code class="literal">curl</code> command, along with the <a id="id310" class="indexterm"></a>port number, which I know from Kenny's website, will default to <code class="literal">7474</code>, shows me that the Rest interface is running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc1r1m1 data]# docker inspect --format="{{.NetworkSettings.IPAddress}}" graphdb</strong></span>
<span class="strong"><strong>172.17.0.5</strong></span>

<span class="strong"><strong>[root@hc1r1m1 data]# curl  172.17.0.5:7474</strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  "management" : "http://172.17.0.5:7474/db/manage/",</strong></span>
<span class="strong"><strong>  "data" : "http://172.17.0.5:7474/db/data/"</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec50"></a>The Neo4j browser</h3></div></div></div><p>The rest of the work in this<a id="id311" class="indexterm"></a> section will now be carried out <a id="id312" class="indexterm"></a>using the Neo4j browser URL, which is as follows:</p><p>
<code class="literal">http://172.17.0.5:7474/browser</code>.</p><p>This is a local, Docker-based IP <a id="id313" class="indexterm"></a>address that will be accessible from the <code class="literal">hc1r1m1</code> server. It will not be visible on the rest of the local intranet without further network configuration.</p><p>This will show the default Neo4j browser page. The Movie graph can be installed by following the movie link here, selecting the Cypher query, and executing it.</p><div class="mediaobject"><img src="graphics/B01989_05_04.jpg" /></div><p>The data can then be interrogated using Cypher queries, which will be examined in more depth in the next <a id="id314" class="indexterm"></a>chapter. The following figures are supplied<a id="id315" class="indexterm"></a> along with their associated Cypher queries, in order to show that the data can be accessed as graphs that are displayed visually. The first graph shows a simple Person to Movie relationship, with the relationship details displayed on the connecting edges.</p><div class="mediaobject"><img src="graphics/B01989_05_05.jpg" /></div><p>The second graph, provided as a visual example of the power of Neo4j, shows a far more complex cypher query, and resulting graph. This graph states that it contains 135 nodes and 180 relationships. These are relatively small numbers in processing terms, but it is clear that the graph is<a id="id316" class="indexterm"></a> becoming complex.</p><div class="mediaobject"><img src="graphics/B01989_05_06.jpg" /></div><p>The following figures show the Mazerunner example algorithms being called via an HTTP Rest URL. The call is defined by the algorithm to be called, and the attribute that it is going to act upon<a id="id317" class="indexterm"></a> within the graph:</p><p>
<code class="literal">http://localhost:7474/service/mazerunner/analysis/{algorithm}/{attribute}</code>.</p><p>So for instance, as the next section will show, this generic URL can be used to run the PageRank algorithm by setting <code class="literal">algorithm=pagerank</code>. The algorithm will operate on the <code class="literal">follows</code> relationship by setting <code class="literal">attribute=FOLLOWS</code>. The next section will show how each Mazerunner algorithm can be run along with an example of the Cypher output.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec51"></a>The Mazerunner algorithms</h3></div></div></div><p>This section shows how the <a id="id318" class="indexterm"></a>Mazerunner example algorithms may be run using the Rest based HTTP URL, which was shown in the last section. Many of these <a id="id319" class="indexterm"></a>algorithms have already been examined, and coded in this chapter. Remember that the interesting thing occurring in this section is that data starts in Neo4j, it is processed on Spark with GraphX, and then is updated back into Neo4j. It looks simple, but there are underlying processes doing all of the work. In each example, the attribute that the algorithm has added to the graph is interrogated via a Cypher query. So, each example isn't so much about the query, but that the data update to Neo4j has occurred.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec02"></a>The PageRank algorithm</h4></div></div></div><p>The first call shows the<a id="id320" class="indexterm"></a> PageRank algorithm, and the PageRank attribute being added to the movie graph. As<a id="id321" class="indexterm"></a> before, the PageRank algorithm gives a rank to each vertex, depending on how many edge connections it has. In this case, it is using the <code class="literal">FOLLOWS</code> relationship for processing.</p><div class="mediaobject"><img src="graphics/B01989_05_07.jpg" /></div><p>The following image shows a screenshot of the PageRank algorithm result. The text at the top of the image (starting with <code class="literal">MATCH</code>) shows the cypher query, which proves that the PageRank property has been added to the graph.</p><div class="mediaobject"><img src="graphics/B01989_05_08.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec03"></a>The closeness centrality algorithm</h4></div></div></div><p>The closeness algorithm <a id="id322" class="indexterm"></a>attempts to <a id="id323" class="indexterm"></a>determine the most important vertices in the graph. In this case, the <code class="literal">closeness</code> attribute has been added to the graph.</p><div class="mediaobject"><img src="graphics/B01989_05_09.jpg" /></div><p>The following image<a id="id324" class="indexterm"></a> shows a screenshot of the closeness algorithm result. The text at the top of the image (starting with <code class="literal">MATCH</code>) shows<a id="id325" class="indexterm"></a> the Cypher query, which proves that the <code class="literal">closeness_centrality</code> property has been added to the graph. Note that an alias called <code class="literal">closeness</code> has been used in this Cypher query, to represent the <code class="literal">closeness_centrality</code> property, and so the output is more presentable.</p><div class="mediaobject"><img src="graphics/B01989_05_10.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec04"></a>The triangle count algorithm</h4></div></div></div><p>The <code class="literal">triangle_count</code> algorithm<a id="id326" class="indexterm"></a> has been used to count triangles<a id="id327" class="indexterm"></a> associated with vertices. The <code class="literal">FOLLOWS</code> relationship has been used, and the <code class="literal">triangle_count</code> attribute has been added to the graph.</p><div class="mediaobject"><img src="graphics/B01989_05_11.jpg" /></div><p>The following image shows a screenshot of the triangle algorithm result. The text at the top of the image (starting with <code class="literal">MATCH</code>) shows the cypher query, which proves that the <code class="literal">triangle_count</code> property has<a id="id328" class="indexterm"></a> been added to the graph. Note that an alias called <span class="strong"><strong>tcount</strong></span> has been used in this <a id="id329" class="indexterm"></a>cypher query, to represent the <code class="literal">triangle_count</code> property, and so the output is more presentable.</p><div class="mediaobject"><img src="graphics/B01989_05_12.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec05"></a>The connected components algorithm</h4></div></div></div><p>The connected components<a id="id330" class="indexterm"></a> algorithm is a<a id="id331" class="indexterm"></a> measure of how many actual components exist in the graph data. For instance, the data might contain two subgraphs with no routes between them. In this case, the <code class="literal">connected_components</code> attribute has been added to the graph.</p><div class="mediaobject"><img src="graphics/B01989_05_13.jpg" /></div><p>The following image shows a screenshot of the connected component algorithm result. The text at the top of the image (starting with <code class="literal">MATCH</code>) shows the cypher query, which proves that the <code class="literal">connected_components</code> property has been added to the graph. Note that an alias called <span class="strong"><strong>ccomp</strong></span> has been used in this cypher query, to represent the <code class="literal">connected_components</code> property, and so the output is more presentable.</p><div class="mediaobject"><img src="graphics/B01989_05_14.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec06"></a>The strongly connected components algorithm</h4></div></div></div><p>The strongly connected <a id="id332" class="indexterm"></a>components algorithm is very similar to the connected components algorithm. Subgraphs are<a id="id333" class="indexterm"></a> created from the graph data using the directional <code class="literal">FOLLOWS</code> relationship. Multiple subgraphs are created until all the graph components are used. These subgraphs form the strongly connected components. As seen here, a <code class="literal">strongly_connected_components</code> attribute has been added to the graph:</p><div class="mediaobject"><img src="graphics/B01989_05_15.jpg" /></div><p>The following image shows a screenshot of the strongly connected component algorithm result. The text at the top of the image (starting with <code class="literal">MATCH</code>) shows the cypher query, which proves that the <code class="literal">strongly_connected_components</code> connected component property has been added to the graph. Note that an alias called <span class="strong"><strong>sccomp</strong></span> has been used in this cypher query, to represent the <code class="literal">strongly_connected_components</code> property, and so the output is more presentable.</p><div class="mediaobject"><img src="graphics/B01989_05_16.jpg" /></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec33"></a>Summary</h2></div></div><hr /></div><p>This chapter has shown, with the help of examples, how the Scala-based code can be used to call GraphX algorithms in Apache Spark. Scala has been used, because it requires less code to develop the examples, which saves time. A Scala-based shell can be used, and the code can be compiled into Spark applications. Examples of the application compilation and configuration have been supplied using the SBT tool. The configuration and the code examples from this chapter will also be available for download with the book.</p><p>Finally, the Mazerunner example architecture (developed by Kenny Bastani while at Neo) for Neo4j and Apache Spark has been introduced. Why is Mazerunner important? It provides an example of how a graph-based database can be used for graph storage, while Apache Spark is used for graph processing. I am not suggesting that Mazerunner be used in a production scenario at this time. Clearly, more work needs to be done to make this architecture ready for release. However, graph-based storage, when associated with the graph-based processing within a distributed environment, offers the option to interrogate the data using a query language such as Cypher from Neo4j.</p><p>I hope that you have found this chapter useful. The next chapter will delve into graph-based storage in more depth. You can now delve into further GraphX coding, try to run the examples provided, and try modifying the code, so that you become familiar with the development process.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>Chapter 6. Graph-based Storage</h2></div></div></div><p>Processing with Apache Spark and especially GraphX provides the ability to use in memory cluster-based, real-time processing for graphs. However, Apache Spark does not provide storage; the graph-based data must come from somewhere and after processing, probably there will be a need for storage. In this chapter, I will examine graph-based storage using the Titan graph database as an example. This chapter will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>An overview of Titan</p></li><li style="list-style-type: disc"><p>An overview of TinkerPop</p></li><li style="list-style-type: disc"><p>Installing Titan</p></li><li style="list-style-type: disc"><p>Using Titan with HBase</p></li><li style="list-style-type: disc"><p>Using Titan with Cassandra</p></li><li style="list-style-type: disc"><p>Using Titan with Spark</p></li></ul></div><p>The young age of this field of processing means that the storage integration between Apache Spark, and the graph-based storage system Titan is not yet mature.</p><p>In the previous chapter, the Neo4j Mazerunner architecture was examined, which showed how the Spark-based transactions could be replicated to Neo4j. This chapter deals with Titan not because of the functionality that it shows today, but due to the future promise that it offers for the field of the graph-based storage when used with Apache Spark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec34"></a>Titan</h2></div></div><hr /></div><p>Titan is a graph<a id="id334" class="indexterm"></a> database that was developed by Aurelius (<a class="ulink" href="http://thinkaurelius.com/" target="_blank">http://thinkaurelius.com/</a>). The application source and<a id="id335" class="indexterm"></a> binaries can be downloaded from GitHub (<a class="ulink" href="http://thinkaurelius.github.io/titan/" target="_blank">http://thinkaurelius.github.io/titan/</a>), and this location also contains the Titan documentation. Titan has been released as an open source application under an Apache 2 license. At the time of writing this book, Aurelius has been acquired by DataStax, although Titan releases should go ahead.</p><p>Titan offers a number of storage options, but I will concentrate only on two, HBase—the Hadoop NoSQL database, and Cassandra—the non-Hadoop NoSQL database. Using these underlying storage mechanisms, Titan is able to provide a graph-based storage in the big data range.</p><p>The TinkerPop3-based Titan release 0.9.0-M2 was released in June 2015, which will enable greater integration with Apache Spark (TinkerPop will be explained in the next section). It is this release that I will use in this chapter. It is TinkerPop that the Titan database now uses for graph manipulation. This Titan release is an experimental development release but hopefully, future releases should consolidate Titan functionality.</p><p>This chapter concentrates on the Titan database rather than an alternative graph database, such as Neo4j, because Titan can use Hadoop-based storage. Also, Titan offers the future promise of integration with Apache Spark for a big data scale, in memory graph-based processing. The following diagram shows the architecture being discussed in this chapter. The dotted line shows direct Spark database access, whereas the solid lines represent Spark access to the data through Titan classes.</p><div class="mediaobject"><img src="graphics/B01989_06_01.jpg" /></div><p>The Spark<a id="id336" class="indexterm"></a> interface doesn't officially exist yet (it is only available in the M2 development release), but it is just added for reference. Although Titan offers the option of using Oracle for storage, it will not be covered in this chapter. I will initially examine the Titan to the HBase and Cassandra architectures, and consider the Apache Spark integration later. When considering (distributed) HBase, ZooKeeper is required as well for integration. Given that I am using an existing CDH5 cluster, HBase and ZooKeeper are already installed.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec35"></a>TinkerPop</h2></div></div><hr /></div><p>TinkerPop, currently at version 3<a id="id337" class="indexterm"></a> as of July 2015, is an Apache incubator project, and <a id="id338" class="indexterm"></a>can be found at <a class="ulink" href="http://tinkerpop.incubator.apache.org/" target="_blank">http://tinkerpop.incubator.apache.org/</a>. It enables both graph databases ( like Titan ) and graph analytic systems ( like Giraph ) to use it as a sub system for graph processing rather than creating their own graph processing modules.</p><div class="mediaobject"><img src="graphics/B01989_06_02.jpg" /></div><p>The previous figure (borrowed from the TinkerPop website) shows the TinkerPop architecture. The blue layer shows the Core TinkerPop API, which offers the graph processing API for graph, vertex, and edge processing. The <a id="id339" class="indexterm"></a>
<span class="strong"><strong>Vendor API</strong></span> boxes show the APIs that the vendors will implement to integrate their systems. The diagram shows that there are two possible APIs: one for the <span class="strong"><strong>OLTP</strong></span> database systems, and another for the <span class="strong"><strong>OLAP</strong></span> analytics systems.</p><p>The diagram also shows<a id="id340" class="indexterm"></a> that the <span class="strong"><strong>Gremlin</strong></span> language is used to create and manage graphs for TinkerPop, and so for Titan. Finally, the Gremlin server sits at the top of the architecture, and <a id="id341" class="indexterm"></a>allows integration to monitoring systems like Ganglia.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec36"></a>Installing Titan</h2></div></div><hr /></div><p>As Titan is required<a id="id342" class="indexterm"></a> throughout this chapter, I will install it now, and show how it can be acquired, installed, and configured. I have downloaded the latest prebuilt<a id="id343" class="indexterm"></a> version (0.9.0-M2) of Titan at: <a class="ulink" href="http://s3.thinkaurelius.com/downloads/titan/titan-0.9.0-M2-hadoop1.zip" target="_blank">s3.thinkaurelius.com/downloads/titan/titan-0.9.0-M2-hadoop1.zip</a>.</p><p>I have downloaded the zipped release to a temporary directory, as shown next. Carry out the following steps to ensure that Titan is installed on each node in the cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[[hadoop@hc2nn tmp]$ ls -lh titan-0.9.0-M2-hadoop1.zip</strong></span>
<span class="strong"><strong>-rw-r--r-- 1 hadoop hadoop 153M Jul 22 15:13 titan-0.9.0-M2-hadoop1.zip</strong></span>
</pre></div><p>Using the Linux unzip command, unpack the zipped Titan release file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn tmp]$ unzip titan-0.9.0-M2-hadoop1.zip</strong></span>

<span class="strong"><strong>[hadoop@hc2nn tmp]$ ls -l</strong></span>
<span class="strong"><strong>total 155752</strong></span>
<span class="strong"><strong>drwxr-xr-x 10 hadoop hadoop      4096 Jun  9 00:56 titan-0.9.0-M2-hadoop1</strong></span>
<span class="strong"><strong>-rw-r--r--  1 hadoop hadoop 159482381 Jul 22 15:13 titan-0.9.0-M2-hadoop1.zip</strong></span>
</pre></div><p>Now, use the Linux <code class="literal">su</code> (switch user) command to change to the <code class="literal">root</code> account, and move the install to the <code class="literal">/usr/local/</code> location. Change the file and group membership of the install to the <code class="literal">hadoop</code> user, and create a symbolic link called <code class="literal">titan</code> so that the current Titan release can be referred to as the simplified path called <code class="literal">/usr/local/titan</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ su –</strong></span>
<span class="strong"><strong>[root@hc2nn ~]# cd /home/hadoop/tmp</strong></span>
<span class="strong"><strong>[root@hc2nn titan]# mv titan-0.9.0-M2-hadoop1 /usr/local</strong></span>
<span class="strong"><strong>[root@hc2nn titan]# cd /usr/local</strong></span>
<span class="strong"><strong>[root@hc2nn local]# chown -R hadoop:hadoop titan-0.9.0-M2-hadoop1</strong></span>
<span class="strong"><strong>[root@hc2nn local]# ln -s titan-0.9.0-M2-hadoop1 titan</strong></span>
<span class="strong"><strong>[root@hc2nn local]# ls -ld *titan*</strong></span>
<span class="strong"><strong>lrwxrwxrwx  1 root   root     19 Mar 13 14:10 titan -&gt; titan-0.9.0-M2-hadoop1</strong></span>
<span class="strong"><strong>drwxr-xr-x 10 hadoop hadoop 4096 Feb 14 13:30 titan-0.9.0-M2-hadoop1</strong></span>
</pre></div><p>Using a Titan Gremlin shell that will be demonstrated later, Titan is now available for use. This version of Titan <a id="id344" class="indexterm"></a>needs Java 8; make sure that you have it installed.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec37"></a>Titan with HBase</h2></div></div><hr /></div><p>As the previous<a id="id345" class="indexterm"></a> diagram shows, HBase depends upon ZooKeeper. Given that I <a id="id346" class="indexterm"></a>have a working ZooKeeper quorum on my CDH5 cluster (running on the <code class="literal">hc2r1m2</code>, <code class="literal">hc2r1m3</code>, and <code class="literal">hc2r1m4</code> nodes), I only <a id="id347" class="indexterm"></a>need to ensure that HBase is installed and working on my Hadoop cluster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec52"></a>The HBase cluster</h3></div></div></div><p>I will install a distributed version of<a id="id348" class="indexterm"></a> HBase using the Cloudera CDH cluster manager. Using the manager console, it is a simple task to install HBase. The only decision required is where to locate the HBase servers on the cluster. The following figure shows the <span class="strong"><strong>View By Host</strong></span> form from the CDH HBase installation. The HBase components are shown to the right as <span class="strong"><strong>Added Roles</strong></span>.</p><p>I have chosen to add the HBase region servers (RS) to the <code class="literal">hc2r1m2</code>, <code class="literal">hc2r1m3</code>, and <code class="literal">hc2r1m4</code> nodes. I have installed the HBase master (M), the HBase REST server (HBREST), and HBase Thrift server (HBTS) on the <code class="literal">hc2r1m1</code> host.</p><div class="mediaobject"><img src="graphics/B01989_06_03.jpg" /></div><p>I have manually installed and configured many Hadoop-based components in the past, and I find that this simple manager-based installation and configuration of components is both quick and reliable. It <a id="id349" class="indexterm"></a>saves me time so that I can concentrate on other systems, such as Titan.</p><p>Once HBase is installed, and has been started from the CDH manager console, it needs to be checked to ensure that it is working. I will do this using the HBase shell command shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 ~]$ hbase shell</strong></span>
<span class="strong"><strong>Version 0.98.6-cdh5.3.2, rUnknown, Tue Feb 24 12:56:59 PST 2015</strong></span>
<span class="strong"><strong>hbase(main):001:0&gt;</strong></span>
</pre></div><p>As you can see from the previous commands, I run the HBase shell as the Linux user <code class="literal">hadoop</code>. The HBase version 0.98.6 has been installed; this version number will become important later when we start using Titan:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hbase(main):001:0&gt; create 'table2', 'cf1'</strong></span>
<span class="strong"><strong>hbase(main):002:0&gt; put 'table2', 'row1', 'cf1:1', 'value1'</strong></span>
<span class="strong"><strong>hbase(main):003:0&gt; put 'table2', 'row2', 'cf1:1', 'value2'</strong></span>
</pre></div><p>I have created a simple table called <code class="literal">table2</code> with a column family of <code class="literal">cf1</code>. I have then added two rows with two different values. This table has been created from the <code class="literal">hc2r1m2</code> node, and will now be checked from an alternate node called <code class="literal">hc2r1m4</code> in the HBase cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m4 ~]$ hbase shell</strong></span>

<span class="strong"><strong>hbase(main):001:0&gt; scan 'table2'</strong></span>

<span class="strong"><strong>ROW                     COLUMN+CELL</strong></span>
<span class="strong"><strong> row1                   column=cf1:1, timestamp=1437968514021, value=value1</strong></span>
<span class="strong"><strong> row2                   column=cf1:1, timestamp=1437968520664, value=value2</strong></span>
<span class="strong"><strong>2 row(s) in 0.3870 seconds</strong></span>
</pre></div><p>As you can see, the two<a id="id350" class="indexterm"></a> data rows are visible in <code class="literal">table2</code> from a different host, so HBase is installed and working. It is now time to try and create a graph in Titan using HBase and the Titan Gremlin shell.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec53"></a>The Gremlin HBase script</h3></div></div></div><p>I have checked my Java<a id="id351" class="indexterm"></a> version to make sure that I am on version 8, otherwise Titan 0.9.0-M2 will not work:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 ~]$ java -version</strong></span>
<span class="strong"><strong>openjdk version "1.8.0_51"</strong></span>
</pre></div><p>If you do not set your Java version correctly, you will get errors like this, which don't seem to be meaningful until you Google them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Exception in thread "main" java.lang.UnsupportedClassVersionError: org/apache/tinkerpop/gremlin/groovy/plugin/RemoteAcceptor :</strong></span>
<span class="strong"><strong>Unsupported major.minor version 52.0</strong></span>
</pre></div><p>The interactive Titan Gremlin shell can be found within the bin directory of the Titan install, as shown here. Once started, it offers a Gremlin prompt:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 bin]$ pwd</strong></span>
<span class="strong"><strong>/usr/local/titan/</strong></span>

<span class="strong"><strong>[hadoop@hc2r1m2 titan]$ bin/gremlin.sh</strong></span>
<span class="strong"><strong>gremlin&gt;</strong></span>
</pre></div><p>The following script will be entered using the Gremlin shell. The first section of the script defines the configuration in terms of the storage (HBase), the ZooKeeper servers used, the ZooKeeper port number, and the HBase table name that is to be used:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hBaseConf = new BaseConfiguration();</strong></span>
<span class="strong"><strong>hBaseConf.setProperty("storage.backend","hbase");</strong></span>
<span class="strong"><strong>hBaseConf.setProperty("storage.hostname","hc2r1m2,hc2r1m3,hc2r1m4");</strong></span>
<span class="strong"><strong>hBaseConf.setProperty("storage.hbase.ext.hbase.zookeeper.property.clientPort","2181")</strong></span>
<span class="strong"><strong>hBaseConf.setProperty("storage.hbase.table","titan")</strong></span>

<span class="strong"><strong>titanGraph = TitanFactory.open(hBaseConf);</strong></span>
</pre></div><p>The next section defines the <a id="id352" class="indexterm"></a>generic vertex properties' name and age for the graph to be created using the Management System. It then commits the management system changes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>manageSys = titanGraph.openManagement();</strong></span>
<span class="strong"><strong>nameProp = manageSys.makePropertyKey('name').dataType(String.class).make();</strong></span>
<span class="strong"><strong>ageProp  = manageSys.makePropertyKey('age').dataType(String.class).make();</strong></span>
<span class="strong"><strong>manageSys.buildIndex('nameIdx',Vertex.class).addKey(nameProp).buildCompositeIndex();</strong></span>
<span class="strong"><strong>manageSys.buildIndex('ageIdx',Vertex.class).addKey(ageProp).buildCompositeIndex();</strong></span>

<span class="strong"><strong>manageSys.commit();</strong></span>
</pre></div><p>Now, six vertices are added to the graph. Each one is given a numeric label to represent its identity. Each vertex is given an age and name value:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>v1=titanGraph.addVertex(label, '1');</strong></span>
<span class="strong"><strong>v1.property('name', 'Mike');</strong></span>
<span class="strong"><strong>v1.property('age', '48');</strong></span>

<span class="strong"><strong>v2=titanGraph.addVertex(label, '2');</strong></span>
<span class="strong"><strong>v2.property('name', 'Sarah');</strong></span>
<span class="strong"><strong>v2.property('age', '45');</strong></span>

<span class="strong"><strong>v3=titanGraph.addVertex(label, '3');</strong></span>
<span class="strong"><strong>v3.property('name', 'John');</strong></span>
<span class="strong"><strong>v3.property('age', '25');</strong></span>

<span class="strong"><strong>v4=titanGraph.addVertex(label, '4');</strong></span>
<span class="strong"><strong>v4.property('name', 'Jim');</strong></span>
<span class="strong"><strong>v4.property('age', '53');</strong></span>

<span class="strong"><strong>v5=titanGraph.addVertex(label, '5');</strong></span>
<span class="strong"><strong>v5.property('name', 'Kate');</strong></span>
<span class="strong"><strong>v5.property('age', '22');</strong></span>

<span class="strong"><strong>v6=titanGraph.addVertex(label, '6');</strong></span>
<span class="strong"><strong>v6.property('name', 'Flo');</strong></span>
<span class="strong"><strong>v6.property('age', '52');</strong></span>
</pre></div><p>Finally, the graph edges are<a id="id353" class="indexterm"></a> added to join the vertices together. Each edge has a relationship value. Once created, the changes are committed to store them to Titan, and therefore HBase:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>v6.addEdge("Sister", v1)</strong></span>
<span class="strong"><strong>v1.addEdge("Husband", v2)</strong></span>
<span class="strong"><strong>v2.addEdge("Wife", v1)</strong></span>
<span class="strong"><strong>v5.addEdge("Daughter", v1)</strong></span>
<span class="strong"><strong>v5.addEdge("Daughter", v2)</strong></span>
<span class="strong"><strong>v3.addEdge("Son", v1)</strong></span>
<span class="strong"><strong>v3.addEdge("Son", v2)</strong></span>
<span class="strong"><strong>v4.addEdge("Friend", v1)</strong></span>
<span class="strong"><strong>v1.addEdge("Father", v5)</strong></span>
<span class="strong"><strong>v1.addEdge("Father", v3)</strong></span>
<span class="strong"><strong>v2.addEdge("Mother", v5)</strong></span>
<span class="strong"><strong>v2.addEdge("Mother", v3)</strong></span>

<span class="strong"><strong>titanGraph.tx().commit();</strong></span>
</pre></div><p>This results in a simple person-based graph, shown in the following figure, which was also used in the previous chapter:</p><div class="mediaobject"><img src="graphics/B01989_06_04.jpg" /></div><p>This graph can then be tested in Titan via the Gremlin shell using a similar script to the previous one. Just enter the<a id="id354" class="indexterm"></a> following script at the <code class="literal">gremlin&gt;</code> prompt, as was shown previously. It uses the same initial six lines to create the <code class="literal">titanGraph</code> configuration, but it then creates a graph traversal variable <code class="literal">g</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hBaseConf = new BaseConfiguration();</strong></span>
<span class="strong"><strong>hBaseConf.setProperty("storage.backend","hbase");</strong></span>
<span class="strong"><strong>hBaseConf.setProperty("storage.hostname","hc2r1m2,hc2r1m3,hc2r1m4");</strong></span>
<span class="strong"><strong>hBaseConf.setProperty("storage.hbase.ext.hbase.zookeeper.property.clientPort","2181")</strong></span>
<span class="strong"><strong>hBaseConf.setProperty("storage.hbase.table","titan")</strong></span>

<span class="strong"><strong>titanGraph = TitanFactory.open(hBaseConf);</strong></span>

<span class="strong"><strong>gremlin&gt; g = titanGraph.traversal()</strong></span>
</pre></div><p>Now, the graph traversal variable can be used to check the graph contents. Using the <code class="literal">ValueMap</code> option, it is possible to<a id="id355" class="indexterm"></a> search for the graph nodes called <code class="literal">Mike</code> and <code class="literal">Flo</code>. They have been successfully found here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gremlin&gt; g.V().has('name','Mike').valueMap();</strong></span>
<span class="strong"><strong>==&gt;[name:[Mike], age:[48]]</strong></span>

<span class="strong"><strong>gremlin&gt; g.V().has('name','Flo').valueMap();</strong></span>
<span class="strong"><strong>==&gt;[name:[Flo], age:[52]]</strong></span>
</pre></div><p>So, the graph has been created and checked in Titan using the Gremlin shell, but we can also check the storage in HBase using the HBase shell, and check the contents of the Titan table. The following scan shows that the table exists, and contains <code class="literal">72</code> rows of the data for this small graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 ~]$ hbase shell</strong></span>
<span class="strong"><strong>hbase(main):002:0&gt; scan 'titan'</strong></span>
<span class="strong"><strong>72 row(s) in 0.8310 seconds</strong></span>
</pre></div><p>Now that the graph has been created, and I am confident that it has been stored in HBase, I will attempt to access the data using apache Spark. I have already started Apache Spark on all the nodes as shown in the previous chapter. This will be a direct access from Apache Spark 1.3 to the HBase storage. I won't at this stage be attempting to use Titan to interpret the HBase stored graph.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec54"></a>Spark on HBase</h3></div></div></div><p>In order to access <a id="id356" class="indexterm"></a>HBase from Spark, I will be<a id="id357" class="indexterm"></a> using Cloudera's <code class="literal">SparkOnHBase</code> module, which can be downloaded from <a class="ulink" href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank">https://github.com/cloudera-labs/SparkOnHBase</a>.</p><p>The downloaded file is in a zipped format, and needs to be unzipped. I have done this using the Linux unzip command in a temporary directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 tmp]$ ls -l SparkOnHBase-cdh5-0.0.2.zip</strong></span>
<span class="strong"><strong>-rw-r--r-- 1 hadoop hadoop 370439 Jul 27 13:39 SparkOnHBase-cdh5-0.0.2.zip</strong></span>

<span class="strong"><strong>[hadoop@hc2r1m2 tmp]$ unzip SparkOnHBase-cdh5-0.0.2.zip</strong></span>

<span class="strong"><strong>[hadoop@hc2r1m2 tmp]$ ls</strong></span>
<span class="strong"><strong>SparkOnHBase-cdh5-0.0.2  SparkOnHBase-cdh5-0.0.2.zip</strong></span>
</pre></div><p>I have then moved into the unpacked module, and used the Maven command <code class="literal">mvn</code> to build the JAR file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 tmp]$ cd SparkOnHBase-cdh5-0.0.2</strong></span>
<span class="strong"><strong>[hadoop@hc2r1m2 SparkOnHBase-cdh5-0.0.2]$ mvn clean package</strong></span>

<span class="strong"><strong>[INFO] -----------------------------------------------------------</strong></span>
<span class="strong"><strong>[INFO] BUILD SUCCESS</strong></span>
<span class="strong"><strong>[INFO] -----------------------------------------------------------</strong></span>
<span class="strong"><strong>[INFO] Total time: 13:17 min</strong></span>
<span class="strong"><strong>[INFO] Finished at: 2015-07-27T14:05:55+12:00</strong></span>
<span class="strong"><strong>[INFO] Final Memory: 50M/191M</strong></span>
<span class="strong"><strong>[INFO] -----------------------------------------------------------</strong></span>
</pre></div><p>Finally, I moved the<a id="id358" class="indexterm"></a> built component to my development area to keep things tidy, so that I could use this module in my Spark HBase code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 SparkOnHBase-cdh5-0.0.2]$ cd ..</strong></span>
<span class="strong"><strong>[hadoop@hc2r1m2 tmp]$ mv SparkOnHBase-cdh5-0.0.2 /home/hadoop/spark</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec55"></a>Accessing HBase with Spark</h3></div></div></div><p>As in previous chapters, I will be<a id="id359" class="indexterm"></a> using SBT and Scala to compile my Spark-based scripts into applications. Then, I will use spark-submit to run these applications<a id="id360" class="indexterm"></a> on the Spark cluster. My SBT configuration file looks like this. It contains the Hadoop, Spark, and HBase libraries:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 titan_hbase]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/titan_hbase</strong></span>

<span class="strong"><strong>[hadoop@hc2r1m2 titan_hbase]$ more titan.sbt</strong></span>
<span class="strong"><strong>name := "T i t a n"</strong></span>
<span class="strong"><strong>version := "1.0"</strong></span>
<span class="strong"><strong>scalaVersion := "2.10.4"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.spark" %% "spark-core"  % "1.3.1"</strong></span>
<span class="strong"><strong>libraryDependencies += "com.cloudera.spark" % "hbase"   % "5-0.0.2" from "file:///home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/SparkHBase.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.hadoop.hbase" % "client"   % "5-0.0.2" from "file:///home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/SparkHBase.jar"</strong></span>
<span class="strong"><strong>resolvers += "Cloudera Repository" at "https://repository.cloudera.com/artifactory/clouder</strong></span>
<span class="strong"><strong>a-repos/"</strong></span>
</pre></div><p>Notice that I am running<a id="id361" class="indexterm"></a> this application on the <code class="literal">hc2r1m2</code> server, using the<a id="id362" class="indexterm"></a> Linux <code class="literal">hadoop</code> account, under the directory <code class="literal">/home/hadoop/spark/titan_hbase</code>. I have created a Bash shell script called <code class="literal">run_titan.bash.hbase</code>, which allows me to run any application that is created and compiled under the <code class="literal">src/main/scala</code> subdirectory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 titan_hbase]$ pwd ; more run_titan.bash.hbase</strong></span>
<span class="strong"><strong>/home/hadoop/spark/titan_hbase</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>SPARK_HOME=/usr/local/spark</strong></span>
<span class="strong"><strong>SPARK_BIN=$SPARK_HOME/bin</strong></span>
<span class="strong"><strong>SPARK_SBIN=$SPARK_HOME/sbin</strong></span>

<span class="strong"><strong>JAR_PATH=/home/hadoop/spark/titan_hbase/target/scala-2.10/t-i-t-a-n_2.10-1.0.jar</strong></span>
<span class="strong"><strong>CLASS_VAL=$1</strong></span>

<span class="strong"><strong>CDH_JAR_HOME=/opt/cloudera/parcels/CDH/lib/hbase/</strong></span>
<span class="strong"><strong>CONN_HOME=/home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/</strong></span>

<span class="strong"><strong>HBASE_JAR1=$CDH_JAR_HOME/hbase-common-0.98.6-cdh5.3.3.jar</strong></span>
<span class="strong"><strong>HBASE_JAR2=$CONN_HOME/SparkHBase.jar</strong></span>

<span class="strong"><strong>cd $SPARK_BIN</strong></span>

<span class="strong"><strong>./spark-submit \</strong></span>
<span class="strong"><strong>  --jars $HBASE_JAR1 \</strong></span>
<span class="strong"><strong>  --jars $HBASE_JAR2 \</strong></span>
<span class="strong"><strong>  --class $CLASS_VAL \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 100M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 50 \</strong></span>
<span class="strong"><strong>  $JAR_PATH</strong></span>
</pre></div><p>The Bash script is held <a id="id363" class="indexterm"></a>within the same <code class="literal">titan_hbase</code> directory, and takes a single parameter of the application class name. The parameters to the <code class="literal">spark-submit</code> call are the same as the previous examples. In this case, there is only a single script under <code class="literal">src/main/scala</code>, called <code class="literal">spark3_hbase2.scala</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 scala]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/titan_hbase/src/main/scala</strong></span>

<span class="strong"><strong>[hadoop@hc2r1m2 scala]$ ls</strong></span>
<span class="strong"><strong>spark3_hbase2.scala</strong></span>
</pre></div><p>The Scala script starts by defining the package name to which the application class will belong. It then imports the<a id="id364" class="indexterm"></a> Spark, Hadoop, and HBase classes:</p><div class="informalexample"><pre class="programlisting">package nz.co.semtechsolutions

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import org.apache.hadoop.hbase._
import org.apache.hadoop.fs.Path
import com.cloudera.spark.hbase.HBaseContext
import org.apache.hadoop.hbase.client.Scan</pre></div><p>The application class name is defined as well as the main method. A configuration object is then created in terms of the application name, and the Spark URL. Finally, a Spark context is created from the configuration:</p><div class="informalexample"><pre class="programlisting">object spark3_hbase2
{

  def main(args: Array[String]) {

    val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"
    val appName = "Spark HBase 2"
    val conf = new SparkConf()

    conf.setMaster(sparkMaster)
    conf.setAppName(appName)

    val sparkCxt = new SparkContext(conf)</pre></div><p>Next, an HBase configuration object is created, and a Cloudera CDH <code class="literal">hbase-site.xml</code> file-based resource is added:</p><div class="informalexample"><pre class="programlisting">    val jobConf = HBaseConfiguration.create()

    val hbasePath="/opt/cloudera/parcels/CDH/etc/hbase/conf.dist/"

    jobConf.addResource(new Path(hbasePath+"hbase-site.xml"))</pre></div><p>An HBase context object is created using the Spark context and the HBase configuration object. The scan and<a id="id365" class="indexterm"></a> cache configurations are also defined:</p><div class="informalexample"><pre class="programlisting">    val hbaseContext = new HBaseContext(sparkCxt, jobConf)

    var scan = new Scan()
    scan.setCaching(100)</pre></div><p>Finally, the data from<a id="id366" class="indexterm"></a> the HBase <code class="literal">Titan</code> table is retrieved using the <code class="literal">hbaseRDD</code> HBase context method, and the scan object. The RDD count is printed, and then the script closes:</p><div class="informalexample"><pre class="programlisting">    var hbaseRdd = hbaseContext.hbaseRDD("titan", scan)

    println( "Rows in Titan hbase table : " + hbaseRdd.count() )

    println( " &gt;&gt;&gt;&gt;&gt; Script Finished &lt;&lt;&lt;&lt;&lt; " )

  } // end main

} // end spark3_hbase2</pre></div><p>I am only printing the count of the data retrieved because Titan compresses the data in GZ format. So, it would make little sense in trying to manipulate it directly.</p><p>Using the <code class="literal">run_titan.bash.hbase</code> script, the Spark application called <code class="literal">spark3_hbase2</code> is run. It outputs an RDD row count of <code class="literal">72</code>, matching the Titan table row count that was previously found. This proves that Apache Spark has been able to access the raw Titan HBase stored graph data, but Spark has not yet used the Titan libraries to access the Titan data as a graph. This will be<a id="id367" class="indexterm"></a> discussed later. And here is the <a id="id368" class="indexterm"></a>code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 titan_hbase]$ ./run_titan.bash.hbase nz.co.semtechsolutions.spark3_hbase2</strong></span>

<span class="strong"><strong>Rows in Titan hbase table : 72</strong></span>
<span class="strong"><strong> &gt;&gt;&gt;&gt;&gt; Script Finished &lt;&lt;&lt;&lt;&lt;</strong></span>
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec38"></a>Titan with Cassandra</h2></div></div><hr /></div><p>In this section, the Cassandra NoSQL database will be used as a storage mechanism for Titan. Although it does <a id="id369" class="indexterm"></a>not use Hadoop, it is a large-scale, cluster-based database in its own right, and can scale to very large cluster sizes. This section will follow the<a id="id370" class="indexterm"></a> same process. As for HBase, a graph will be created, and stored in Cassandra using the Titan Gremlin shell. It will then be checked<a id="id371" class="indexterm"></a> using Gremlin, and the stored data will be checked in Cassandra. The raw Titan Cassandra graph-based data will then be accessed from Spark. The first step then will be to install Cassandra on each node in the cluster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec56"></a>Installing Cassandra</h3></div></div></div><p>Create a repo file<a id="id372" class="indexterm"></a> that will allow the <a id="id373" class="indexterm"></a>community version of DataStax Cassandra to be installed using the Linux <code class="literal">yum</code> command. Root access will be required for this, so the <code class="literal">su</code> command has been used to switch the user to the root. Install Cassandra on all the nodes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn lib]$ su -</strong></span>
<span class="strong"><strong>[root@hc2nn ~]# vi /etc/yum.repos.d/datastax.repo</strong></span>

<span class="strong"><strong>[datastax]</strong></span>
<span class="strong"><strong>name= DataStax Repo for Apache Cassandra</strong></span>
<span class="strong"><strong>baseurl=http://rpm.datastax.com/community</strong></span>
<span class="strong"><strong>enabled=1</strong></span>
<span class="strong"><strong>gpgcheck=0</strong></span>
</pre></div><p>Now, install Cassandra on each node in the cluster using the Linux <code class="literal">yum</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# yum -y install dsc20-2.0.13-1 cassandra20-2.0.13-1</strong></span>
</pre></div><p>Set up the Cassandra configuration under <code class="literal">/etc/cassandra/conf</code> by altering the <code class="literal">cassandra.yaml</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# cd /etc/cassandra/conf   ; vi cassandra.yaml</strong></span>
</pre></div><p>I have made the following changes to specify my cluster name, the server seed IP addresses, the RPC address, and the snitch value. Seed nodes are the nodes that the other nodes will try to <a id="id374" class="indexterm"></a>connect to first. In this case, the NameNode (<code class="literal">103</code>), and node2 (<code class="literal">108</code>) have been used as <code class="literal">seeds</code>. The snitch method<a id="id375" class="indexterm"></a> manages network topology and routing:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cluster_name: 'Cluster1'</strong></span>
<span class="strong"><strong>seeds: "192.168.1.103,192.168.1.108"</strong></span>
<span class="strong"><strong>listen_address:</strong></span>
<span class="strong"><strong>rpc_address: 0.0.0.0</strong></span>
<span class="strong"><strong>endpoint_snitch: GossipingPropertyFileSnitch</strong></span>
</pre></div><p>Cassandra can now be started on each node as root using the service command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn ~]# service cassandra start</strong></span>
</pre></div><p>Log files can be found under <code class="literal">/var/log/cassandra</code>, and the data is stored under <code class="literal">/var/lib/cassandra</code>. The <code class="literal">nodetool</code> command can be used on any Cassandra node to check the status of the Cassandra cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[root@hc2nn cassandra]# nodetool status</strong></span>
<span class="strong"><strong>Datacenter: DC1</strong></span>
<span class="strong"><strong>===============</strong></span>
<span class="strong"><strong>Status=Up/Down</strong></span>
<span class="strong"><strong>|/ State=Normal/Leaving/Joining/Moving</strong></span>
<span class="strong"><strong>--  Address        Load       Tokens  Owns (effective)  Host ID Rack</strong></span>
<span class="strong"><strong>UN  192.168.1.105  63.96 KB   256     37.2%             f230c5d7-ff6f-43e7-821d-c7ae2b5141d3  RAC1</strong></span>
<span class="strong"><strong>UN  192.168.1.110  45.86 KB   256     39.9%             fc1d80fe-6c2d-467d-9034-96a1f203c20d  RAC1</strong></span>
<span class="strong"><strong>UN  192.168.1.109  45.9 KB    256     40.9%             daadf2ee-f8c2-4177-ae72-683e39fd1ea0  RAC1</strong></span>
<span class="strong"><strong>UN  192.168.1.108  50.44 KB   256     40.5%             b9d796c0-5893-46bc-8e3c-187a524b1f5a  RAC1</strong></span>
<span class="strong"><strong>UN  192.168.1.103  70.68 KB   256     41.5%             53c2eebd-</strong></span>
<span class="strong"><strong>a66c-4a65-b026-96e232846243  RAC1</strong></span>
</pre></div><p>The Cassandra CQL shell command called <code class="literal">cqlsh</code> can be used to access the cluster, and create objects. The shell is invoked next, and it shows that Cassandra version 2.0.13 is installed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ cqlsh</strong></span>
<span class="strong"><strong>Connected to Cluster1 at localhost:9160.</strong></span>
<span class="strong"><strong>[cqlsh 4.1.1 | Cassandra 2.0.13 | CQL spec 3.1.1 | Thrift protocol 19.39.0]</strong></span>
<span class="strong"><strong>Use HELP for help.</strong></span>
<span class="strong"><strong>cqlsh&gt;</strong></span>
</pre></div><p>The Cassandra query<a id="id376" class="indexterm"></a> language next shows <a id="id377" class="indexterm"></a>a key space called <code class="literal">keyspace1</code> that is being created and used via the CQL shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; CREATE KEYSPACE keyspace1 WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };</strong></span>

<span class="strong"><strong>cqlsh&gt; USE keyspace1;</strong></span>

<span class="strong"><strong>cqlsh:keyspace1&gt; SELECT * FROM system.schema_keyspaces;</strong></span>

<span class="strong"><strong> keyspace_name | durable_writes | strategy_class                              | strategy_options</strong></span>
<span class="strong"><strong>--------------+------+---------------------------------------------+----------------------------</strong></span>
<span class="strong"><strong>   keyspace1  | True | org.apache.cassandra.locator.SimpleStrategy | {"replication_factor":"1"}</strong></span>
<span class="strong"><strong>      system  | True |  org.apache.cassandra.locator.LocalStrategy |                         {}</strong></span>
<span class="strong"><strong>system_traces | True | org.apache.cassandra.locator.SimpleStrategy | {"replication_factor":"2"}</strong></span>
</pre></div><p>Since Cassandra is installed and working, it is now time to create a Titan graph using Cassandra for storage. This will be tackled in the next section using the Titan Gremlin shell. It will follow the same format as the HBase section previously.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec57"></a>The Gremlin Cassandra script</h3></div></div></div><p>As with the previous <a id="id378" class="indexterm"></a>Gremlin script, this Cassandra version creates the same simple graph. The difference with this script is in the configuration. The backend storage type is defined as Cassandra, and the hostnames are defined to be the Cassandra seed nodes. The key space and the port number are specified and finally, the graph is created:</p><div class="informalexample"><pre class="programlisting">cassConf = new BaseConfiguration();
cassConf.setProperty("storage.backend","cassandra");
cassConf.setProperty("storage.hostname","hc2nn,hc2r1m2");
cassConf.setProperty("storage.port","9160")
cassConf.setProperty("storage.keyspace","titan")
titanGraph = TitanFactory.open(cassConf);</pre></div><p>From this point, the script is the same as the previous HBase example, so I will not repeat it. This script will be <a id="id379" class="indexterm"></a>available in the download package as <code class="literal">cassandra_create.bash</code>. The same checks, using the previous configuration, can be carried out in the Gremlin shell to check the data. This returns the same results as the previous checks, and so proves that the graph has been stored:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gremlin&gt; g = titanGraph.traversal()</strong></span>

<span class="strong"><strong>gremlin&gt; g.V().has('name','Mike').valueMap();</strong></span>
<span class="strong"><strong>==&gt;[name:[Mike], age:[48]]</strong></span>

<span class="strong"><strong>gremlin&gt; g.V().has('name','Flo').valueMap();</strong></span>
<span class="strong"><strong>==&gt;[name:[Flo], age:[52]]</strong></span>
</pre></div><p>Using the Cassandra CQL shell, and the Titan <code class="literal">keyspace</code>, it can be seen that a number of Titan tables have been created in Cassandra:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ cqlsh</strong></span>
<span class="strong"><strong>cqlsh&gt; use titan;</strong></span>
<span class="strong"><strong>cqlsh:titan&gt; describe tables;</strong></span>
<span class="strong"><strong>edgestore        graphindex        system_properties systemlog  txlog</strong></span>
<span class="strong"><strong>edgestore_lock_  graphindex_lock_  system_properties_lock_  titan_ids</strong></span>
</pre></div><p>It can also be seen that the data exists in the <code class="literal">edgestore</code> table within Cassandra:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh:titan&gt; select * from edgestore;</strong></span>
<span class="strong"><strong> key                | column1            | value</strong></span>
<span class="strong"><strong>--------------------+--------------------+------------------------------------------------</strong></span>
<span class="strong"><strong> 0x0000000000004815 |               0x02 |                                     0x00011ee0</strong></span>
<span class="strong"><strong> 0x0000000000004815 |             0x10c0 |                           0xa0727425536fee1ec0</strong></span>
<span class="strong"><strong>.......</strong></span>
<span class="strong"><strong> 0x0000000000001005 |             0x10c8 |                       0x00800512644c1b149004a0</strong></span>
<span class="strong"><strong> 0x0000000000001005 | 0x30c9801009800c20 |   0x000101143c01023b0101696e6465782d706ff30200</strong></span>
</pre></div><p>This assures me that a Titan <a id="id380" class="indexterm"></a>graph has been created in the Gremlin shell, and is stored in Cassandra. Now, I will try to access the data from Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec58"></a>The Spark Cassandra connector</h3></div></div></div><p>In order to access <a id="id381" class="indexterm"></a>Cassandra from Spark, I will download the DataStax Spark Cassandra connector and driver libraries. Information and version <a id="id382" class="indexterm"></a>matching on this can be found at <a class="ulink" href="http://mvnrepository.com/artifact/com.datastax.spark/" target="_blank">http://mvnrepository.com/artifact/com.datastax.spark/</a>.</p><p>The version compatibility section of this URL shows the Cassandra connector version that should be used with each Cassandra and Spark version. The version table shows that the connector version should match the Spark version that is being used. The next URL allows the libraries to be sourced at <a class="ulink" href="http://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10" target="_blank">http://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10</a>.</p><p>By following the previous URL, and selecting a library version, you will see a compile dependencies table associated with the library, which indicates all of the other dependent libraries, and their versions that you will need. The following libraries are those that are needed for use with Spark 1.3.1. If you use the previous URLs, you will see which version of the Cassandra connector library to use with each version of Spark. You will also see the libraries that the Cassandra connector depends upon. Be careful to choose just (and all of) those library versions that are required:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 titan_cass]$ pwd ; ls *.jar</strong></span>
<span class="strong"><strong>/home/hadoop/spark/titan_cass</strong></span>

<span class="strong"><strong>spark-cassandra-connector_2.10-1.3.0-M1.jar</strong></span>
<span class="strong"><strong>cassandra-driver-core-2.1.5.jar</strong></span>
<span class="strong"><strong>cassandra-thrift-2.1.3.jar</strong></span>
<span class="strong"><strong>libthrift-0.9.2.jar</strong></span>
<span class="strong"><strong>cassandra-clientutil-2.1.3.jar</strong></span>
<span class="strong"><strong>guava-14.0.1.jar</strong></span>
<span class="strong"><strong>joda-time-2.3.jar</strong></span>
<span class="strong"><strong>joda-convert-1.2.jar</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec59"></a>Accessing Cassandra with Spark</h3></div></div></div><p>Now that I have the Cassandra connector library and all of it's dependencies in place, I can begin to think<a id="id383" class="indexterm"></a> about the Scala code, required to connect to Cassandra. The first thing to do, given that I am using SBT as a development<a id="id384" class="indexterm"></a> tool, is to set up the SBT build configuration file. Mine looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 titan_cass]$ pwd ; more titan.sbt</strong></span>
<span class="strong"><strong>/home/hadoop/spark/titan_cass</strong></span>

<span class="strong"><strong>name := "Spark Cass"</strong></span>
<span class="strong"><strong>version := "1.0"</strong></span>
<span class="strong"><strong>scalaVersion := "2.10.4"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.spark" %% "spark-core"  % "1.3.1"</strong></span>
<span class="strong"><strong>libraryDependencies += "com.datastax.spark" % "spark-cassandra-connector"  % "1.3.0-M1" fr</strong></span>
<span class="strong"><strong>om "file:///home/hadoop/spark/titan_cass/spark-cassandra-connector_2.10-1.3.0-M1.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "com.datastax.cassandra" % "cassandra-driver-core"  % "2.1.5" from</strong></span>
<span class="strong"><strong>"file:///home/hadoop/spark/titan_cass/cassandra-driver-core-2.1.5.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.joda"  % "time" % "2.3" from "file:///home/hadoop/spark/titan_</strong></span>
<span class="strong"><strong>cass/joda-time-2.3.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.cassandra" % "thrift" % "2.1.3" from "file:///home/hado</strong></span>
<span class="strong"><strong>op/spark/titan_cass/cassandra-thrift-2.1.3.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "com.google.common" % "collect" % "14.0.1" from "file:///home/hadoo</strong></span>
<span class="strong"><strong>p/spark/titan_cass/guava-14.0.1.jar</strong></span>
<span class="strong"><strong>resolvers += "Cloudera Repository" at "https://repository.cloudera.com/artifactory/clouder</strong></span>
<span class="strong"><strong>a-repos/"</strong></span>
</pre></div><p>The Scala script for the Cassandra connector example, called <code class="literal">spark3_cass.scala</code>, now looks like the following code. First, the package name is defined. Then, the classes are imported for Spark, and the Cassandra connector. Next, the object application class <code class="literal">spark3_cass</code> ID is defined, and so is the main method:</p><div class="informalexample"><pre class="programlisting">package nz.co.semtechsolutions

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import com.datastax.spark.connector._

object spark3_cass
{

  def main(args: Array[String]) {</pre></div><p>A Spark configuration<a id="id385" class="indexterm"></a> object is created using a Spark URL and application name. The Cassandra connection host is added to the configuration. Then, the Spark context is created using the configuration object:</p><div class="informalexample"><pre class="programlisting">    val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"
    val appName = "Spark Cass 1"
    val conf = new SparkConf()

    conf.setMaster(sparkMaster)
    conf.setAppName(appName)

    conf.set("spark.cassandra.connection.host", "hc2r1m2")

    val sparkCxt = new SparkContext(conf)</pre></div><p>The Cassandra <code class="literal">keyspace</code>, and table names that are to be checked are defined. Then, the Spark context method called <code class="literal">cassandraTable</code> is used to connect to Cassandra, and obtain the contents <a id="id386" class="indexterm"></a>of the <code class="literal">edgestore</code> table as an RDD. The size of this RDD is then printed, and the script exits. We won't look at this data at this time, because all that was needed was to prove that a connection to Cassandra could be made:</p><div class="informalexample"><pre class="programlisting">    val keySpace =  "titan"
    val tableName = "edgestore"

    val cassRDD = sparkCxt.cassandraTable( keySpace, tableName )

    println( "Cassandra Table Rows : " + cassRDD.count )

    println( " &gt;&gt;&gt;&gt;&gt; Script Finished &lt;&lt;&lt;&lt;&lt; " )

  } // end main

} // end spark3_cass</pre></div><p>As in the previous examples, the Spark <code class="literal">submit</code> command has been placed in a Bash script called <code class="literal">run_titan.bash.cass</code>. This script, shown next, looks similar to many others used already. The point to note here is that there is a JARs option, which lists all of the JAR files<a id="id387" class="indexterm"></a> used so that they are available at run time. The order of JAR files in this option has been determined to avoid the class exception <a id="id388" class="indexterm"></a>errors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 titan_cass]$ more run_titan.bash</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>SPARK_HOME=/usr/local/spark</strong></span>
<span class="strong"><strong>SPARK_BIN=$SPARK_HOME/bin</strong></span>
<span class="strong"><strong>SPARK_SBIN=$SPARK_HOME/sbin</strong></span>

<span class="strong"><strong>JAR_PATH=/home/hadoop/spark/titan_cass/target/scala-2.10/spark-cass_2.10-1.0.jar</strong></span>
<span class="strong"><strong>CLASS_VAL=$1</strong></span>

<span class="strong"><strong>CASS_HOME=/home/hadoop/spark/titan_cass/</strong></span>

<span class="strong"><strong>CASS_JAR1=$CASS_HOME/spark-cassandra-connector_2.10-1.3.0-M1.jar</strong></span>
<span class="strong"><strong>CASS_JAR2=$CASS_HOME/cassandra-driver-core-2.1.5.jar</strong></span>
<span class="strong"><strong>CASS_JAR3=$CASS_HOME/cassandra-thrift-2.1.3.jar</strong></span>
<span class="strong"><strong>CASS_JAR4=$CASS_HOME/libthrift-0.9.2.jar</strong></span>
<span class="strong"><strong>CASS_JAR5=$CASS_HOME/cassandra-clientutil-2.1.3.jar</strong></span>
<span class="strong"><strong>CASS_JAR6=$CASS_HOME/guava-14.0.1.jar</strong></span>
<span class="strong"><strong>CASS_JAR7=$CASS_HOME/joda-time-2.3.jar</strong></span>
<span class="strong"><strong>CASS_JAR8=$CASS_HOME/joda-convert-1.2.jar</strong></span>

<span class="strong"><strong>cd $SPARK_BIN</strong></span>

<span class="strong"><strong>./spark-submit \</strong></span>
<span class="strong"><strong>  --jars $CASS_JAR8,$CASS_JAR7,$CASS_JAR5,$CASS_JAR4,$CASS_JAR3,$CASS_JAR6,$CASS_JAR2,$CASS_JAR1 \</strong></span>
<span class="strong"><strong>  --class $CLASS_VAL \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 100M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 50 \</strong></span>
<span class="strong"><strong>  $JAR_PATH</strong></span>
</pre></div><p>This application is invoked using the previous Bash script. It connects to Cassandra, selects the data, and returns a Cassandra table data-based count of <code class="literal">218</code> rows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 titan_cass]$ ./run_titan.bash.cass nz.co.semtechsolutions.spark3_cass</strong></span>

<span class="strong"><strong>Cassandra Table Rows : 218</strong></span>
<span class="strong"><strong> &gt;&gt;&gt;&gt;&gt; Script Finished &lt;&lt;&lt;&lt;&lt;</strong></span>
</pre></div><p>This proves that the raw<a id="id389" class="indexterm"></a> Cassandra-based Titan table data can be accessed from Apache Spark. However, as in the HBase example, this is raw table-based Titan data, and not the data in Titan <a id="id390" class="indexterm"></a>graph form. The next step will be to use Apache Spark as a processing engine for the Titan database. This will be examined in the next section.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec39"></a>Accessing Titan with Spark</h2></div></div><hr /></div><p>So far in this chapter, Titan 0.9.0-M2 has been installed, and the graphs have successfully been created using both HBase and Cassandra as backend storage options. These graphs have been created using<a id="id391" class="indexterm"></a> Gremlin-based scripts. In this section, a properties file will be used via a Gremlin script to process a Titan-based graph using<a id="id392" class="indexterm"></a> Apache Spark. The same two backend storage options, HBase and Cassandra, will be used with Titan.</p><p>The following<a id="id393" class="indexterm"></a> figure, based on the TinkerPop3 diagram earlier in this chapter, shows the architecture used in this section. I have simplified the diagram, but it is basically the same as the previous TinkerPop version. I have just added the link to Apache Spark via the Graph Computer API. I have also added both HBase and Cassandra storage via the Titan vendor API. Of course, a distributed installation of HBase uses both Zookeeper for configuration, and HDFS for storage.</p><p>Titan uses TinkerPop's Hadoop-Gremlin package for graph processing OLAP processes. The link to the documentation section can be found at: <a class="ulink" href="http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html" target="_blank">http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html</a>.</p><p>This section will show how the Bash shell, Groovy, and properties files can be used to configure, and run a Titan Spark-based job. It will show different methods for configuring the job, and it <a id="id394" class="indexterm"></a>will also show methods for managing logging to enable error tracking. Also, different configurations of the property file will be described to give access to HBase, Cassandra, and the Linux file system.</p><p>Remember that the Titan release 0.9.0-M2, that this chapter is based on, is a development release. It is a prototype release, and is not yet ready for production. I assume that as the future Titan releases become available, the link between Titan and Spark will be more developed and stable. Currently, the work in this section is for demonstration purposes only, given the nature of the Titan release.</p><div class="mediaobject"><img src="graphics/B01989_06_05.jpg" /></div><p>In the next <a id="id395" class="indexterm"></a>section, I will explain the use of Gremlin, and Groovy scripts before moving onto connecting Titan to Spark using Cassandra and HBase as storage options.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec60"></a>Gremlin and Groovy</h3></div></div></div><p>The<a id="id396" class="indexterm"></a> Gremlin shell, which is used to execute<a id="id397" class="indexterm"></a> Groovy commands against Titan, can be used in a number of ways. The first method of use just involves starting a Gremlin shell for use as an interactive session. Just execute the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd $TITAN_HOME/bin ; ./ gremlin.sh</strong></span>
</pre></div><p>This starts the session, and automatically sets up required plug-ins such as TinkerPop and Titan (see next). Obviously, the previous <code class="literal">TITAN_HOME</code> variable is used to indicate that the bin directory in question is located within your Titan install (<code class="literal">TITAN_HOME</code>) directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>plugin activated: tinkerpop.server</strong></span>
<span class="strong"><strong>plugin activated: tinkerpop.utilities</strong></span>
<span class="strong"><strong>plugin activated: tinkerpop.hadoop</strong></span>
<span class="strong"><strong>plugin activated: tinkerpop.tinkergraph</strong></span>
<span class="strong"><strong>plugin activated: aurelius.titan</strong></span>
</pre></div><p>It then provides you<a id="id398" class="indexterm"></a> with a Gremlin shell prompt where you can interactively execute your shell commands against your Titan database. This shell is useful for testing scripts and running ad hoc commands against your Titan database.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gremlin&gt;</strong></span>
</pre></div><p>A second method is to embed your Groovy commands inline in a script when you call the <code class="literal">gremlin.sh</code> command. In this example, the Groovy commands between the EOF markers are piped into<a id="id399" class="indexterm"></a> the Gremlin shell. When the last Groovy command has executed, the Gremlin shell will terminate. This is useful when you still want to use the automated environment setup of the Gremlin shell, but you still want to be able to quickly re-execute a script. This code snippet has been executed from a Bash shell script, as can be seen in the next example. The following script uses the <code class="literal">titan.sh</code> script to manage the Gremlin server:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>TITAN_HOME=/usr/local/titan/</strong></span>

<span class="strong"><strong>cd $TITAN_HOME</strong></span>

<span class="strong"><strong>bin/titan.sh start</strong></span>

<span class="strong"><strong>bin/gremlin.sh   &lt;&lt;  EOF</strong></span>

<span class="strong"><strong>  t = TitanFactory.open('cassandra.properties')</strong></span>
<span class="strong"><strong>  GraphOfTheGodsFactory.load(t)</strong></span>
<span class="strong"><strong>  t.close()</strong></span>
<span class="strong"><strong>EOF</strong></span>

<span class="strong"><strong>bin/titan.sh stop</strong></span>
</pre></div><p>A third method involves moving the Groovy commands into a separate Groovy file, and using the <code class="literal">–e</code> option with the Gremlin shell to execute the file. This method offers extra logging options for error tracking, but means that extra steps need to be taken when setting up the Gremlin environment for a Groovy script:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>TITAN_HOME=/usr/local/titan/</strong></span>
<span class="strong"><strong>SCRIPTS_HOME=/home/hadoop/spark/gremlin</strong></span>
<span class="strong"><strong>GREMLIN_LOG_FILE=$TITAN_HOME/log/gremlin_console.log</strong></span>

<span class="strong"><strong>GROOVY_SCRIPT=$1</strong></span>

<span class="strong"><strong>export GREMLIN_LOG_LEVEL="DEBUG"</strong></span>

<span class="strong"><strong>cd $TITAN_HOME</strong></span>

<span class="strong"><strong>bin/titan.sh start</strong></span>

<span class="strong"><strong>bin/gremlin.sh -e  $SCRIPTS_HOME/$GROOVY_SCRIPT  &gt; $GREMLIN_LOG_FILE 2&gt;&amp;1</strong></span>

<span class="strong"><strong>bin/titan.sh stop</strong></span>
</pre></div><p>So, this script defines a Gremlin log level, which can be set to different logging levels to obtain extra information <a id="id400" class="indexterm"></a>about a problem, that is, INFO, WARN, and DEBUG. It also redirects the script output to a log file (<code class="literal">GREMLIN_LOG_FILE</code>), and redirects errors to the same log file (<code class="literal">2&gt;&amp;1</code>). This has the benefit of allowing the log file to be continuously monitored, and provides a<a id="id401" class="indexterm"></a> permanent record of the session. The Groovy script name that is to be executed is then passed to the encasing Bash shell script as a parameter (<code class="literal">$1</code>).</p><p>As I already mentioned, the Groovy scripts invoked in this way need extra environment configuration to set up the Gremlin session when compared to the previous Gremlin session options. For instance, it is necessary to import the necessary TinkerPop and Aurelius classes that will be used:</p><div class="informalexample"><pre class="programlisting">import com.thinkaurelius.titan.core.*
import com.thinkaurelius.titan.core.titan.*
import org.apache.tinkerpop.gremlin.*</pre></div><p>Having described the script<a id="id402" class="indexterm"></a> and configuration options necessary to start a Gremlin shell session, and run a Groovy script, from this point onwards I will concentrate on Groovy scripts, and the property files necessary to configure the Gremlin session.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec61"></a>TinkerPop's Hadoop Gremlin</h3></div></div></div><p>As already mentioned <a id="id403" class="indexterm"></a>previously in this section, it is the TinkerPop Hadoop Gremlin package within Titan that will be used to call Apache Spark as a processing engine (Hadoop Giraph can be used for processing as well). The link available at <a class="ulink" href="http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html" target="_blank">http://s3.thinkaurelius.com/docs/titan/0.9.0-M2/titan-hadoop-tp3.html</a> provides documentation for<a id="id404" class="indexterm"></a> Hadoop Gremlin; remember that this TinkerPop package is still being developed and is subject to change.</p><p>At this point, I will examine a properties file that can be used to connect to Cassandra as a storage backend for Titan. It contains sections for Cassandra, Apache Spark, and the Hadoop Gremlin configuration. My Cassandra properties file is called <code class="literal">cassandra.properties</code>, and it looks like this (lines beginning with a hash character (<code class="literal">#</code>) are comments):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>####################################</strong></span>
<span class="strong"><strong># Storage details</strong></span>
<span class="strong"><strong>####################################</strong></span>
<span class="strong"><strong>storage.backend=cassandra</strong></span>
<span class="strong"><strong>storage.hostname=hc2r1m2</strong></span>
<span class="strong"><strong>storage.port=9160</strong></span>
<span class="strong"><strong>storage.cassandra.keyspace=dead</strong></span>
<span class="strong"><strong>cassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner</strong></span>
</pre></div><p>The previous Cassandra-based properties describe the Cassandra host and port. This is why the storage backend type is Cassandra, the Cassandra <code class="literal">keyspace</code> that is to be used is called <code class="literal">dead</code> (short for grateful dead—the data that will be used in this example). Remember that the Cassandra tables are grouped within keyspaces. The previous <code class="literal">partitioner</code> class defines the Cassandra class that will be used to partition the Cassandra data. The Apache Spark configuration section contains the master URL, executor memory, and the data <code class="literal">serializer</code> class that is to be used:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>####################################</strong></span>
<span class="strong"><strong># Spark</strong></span>
<span class="strong"><strong>####################################</strong></span>
<span class="strong"><strong>spark.master=spark://hc2nn.semtech-solutions.co.nz:6077</strong></span>
<span class="strong"><strong>spark.executor.memory=400M</strong></span>
<span class="strong"><strong>spark.serializer=org.apache.spark.serializer.KryoSerializer</strong></span>
</pre></div><p>Finally, the Hadoop Gremlin section of the properties file, which defines the classes to be used for graph and non-graph input and output is shown here. It also defines the data input and output locations, as well as the flags for caching JAR files, and deriving memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>####################################</strong></span>
<span class="strong"><strong># Hadoop Gremlin</strong></span>
<span class="strong"><strong>####################################</strong></span>
<span class="strong"><strong>gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph</strong></span>
<span class="strong"><strong>gremlin.hadoop.graphInputFormat=com.thinkaurelius.titan.hadoop.formats.cassandra.CassandraInputFormat</strong></span>
<span class="strong"><strong>gremlin.hadoop.graphOutputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat</strong></span>
<span class="strong"><strong>gremlin.hadoop.memoryOutputFormat=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</strong></span>

<span class="strong"><strong>gremlin.hadoop.deriveMemory=false</strong></span>
<span class="strong"><strong>gremlin.hadoop.jarsInDistributedCache=true</strong></span>
<span class="strong"><strong>gremlin.hadoop.inputLocation=none</strong></span>
<span class="strong"><strong>gremlin.hadoop.outputLocation=output</strong></span>
</pre></div><p>Blueprints is the<a id="id405" class="indexterm"></a> TinkerPop property graph model interface. Titan releases it's own implementation of blueprints, so instead of seeing <code class="literal">blueprints.graph</code> in the preceding properties, you see <code class="literal">gremlin.graph</code>. This defines the class, used to define the graph that is supposed to be used. If this option were omitted, then the graph type would default to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>com.thinkaurelius.titan.core.TitanFactory</strong></span>
</pre></div><p>The <code class="literal">CassandraInputFormat</code> class defines that the data is being retrieved from the Cassandra database. The graph output serialization class is defined to be <code class="literal">GryoOutputFormat</code>. The memory output format class is defined to use the Hadoop Map Reduce class <code class="literal">SequenceFileOutputFormat</code>.</p><p>The <code class="literal">jarsInDistributedCache</code> value has been defined to be true so that the JAR files are copied to the memory, enabling Apache Spark to source them. Given more time, I would investigate ways to make the Titan classes visible to Spark, on the class path, to avoid excessive memory usage.</p><p>Given that the TinkerPop Hadoop Gremlin module is only available as a development prototype release, currently the documentation is minimal. There are very limited coding examples, and there does not seem to be documentation available describing each of the previous properties.</p><p>Before I delve into the examples of Groovy scripts, I thought that I would show you an alternative method for configuring your Groovy jobs using a configuration object.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec62"></a>Alternative Groovy configuration</h3></div></div></div><p>A configuration <a id="id406" class="indexterm"></a>object can be created using<a id="id407" class="indexterm"></a> the <code class="literal">BaseConfiguration</code> method. In this example, I have created a Cassandra configuration called <code class="literal">cassConf</code>:</p><div class="informalexample"><pre class="programlisting">cassConf = new BaseConfiguration();

cassConf.setProperty("storage.backend","cassandra");
cassConf.setProperty("storage.hostname","hc2r1m2");
cassConf.setProperty("storage.port","9160")
cassConf.setProperty("storage.cassandra.keyspace","titan")

titanGraph = TitanFactory.open(cassConf);</pre></div><p>The <code class="literal">setProperty</code> method is then used to define Cassandra connection properties, such as backend type, host, port, and <code class="literal">keyspace</code>. Finally, a Titan graph is created called <code class="literal">titanGraph</code> using the open method. As will be shown later, a Titan graph can be created using a configuration object or a path to a properties file. The properties that have been set match those that were defined in the Cassandra properties file described previously.</p><p>The next few sections will show how graphs can be created, and traversed. They will show how Cassandra, HBase, and the file system can be used for storage. Given that I have gone to such lengths to describe the Bash scripts, and the properties files, I will just describe those properties that need to be changed in each instance. I will also provide simple Groovy script snippets in each instance.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec63"></a>Using Cassandra</h3></div></div></div><p>The Cassandra-based<a id="id408" class="indexterm"></a> properties file called <code class="literal">cassandra.properties</code> has already been described, so I will not repeat the details here. This example Groovy script creates a sample graph, and stores it in Cassandra. It has been executed using the <a id="id409" class="indexterm"></a>
<span class="strong"><strong>end of file markers</strong></span> (<span class="strong"><strong>EOF</strong></span>) to pipe the script to the Gremlin shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>t1 = TitanFactory.open('/home/hadoop/spark/gremlin/cassandra.properties')</strong></span>
<span class="strong"><strong>GraphOfTheGodsFactory.load(t1)</strong></span>

<span class="strong"><strong>t1.traversal().V().count()</strong></span>

<span class="strong"><strong>t1.traversal().V().valueMap()</strong></span>

<span class="strong"><strong>t1.close()</strong></span>
</pre></div><p>A Titan graph has been created using<a id="id410" class="indexterm"></a> the <code class="literal">TitanFactory.open</code> method and the Cassandra properties file. It is called <code class="literal">t1</code>. The graph of the Gods, an example graph provided with Titan, has been loaded into the graph <code class="literal">t1</code> using the method <code class="literal">GraphOfTheGodsFactory.load</code>. A count of vertices (<code class="literal">V()</code>) has then been generated along with a <code class="literal">ValueMap</code> to display the contents of the graph. The output looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>==&gt;12</strong></span>

<span class="strong"><strong>==&gt;[name:[jupiter], age:[5000]]</strong></span>
<span class="strong"><strong>==&gt;[name:[hydra]]</strong></span>
<span class="strong"><strong>==&gt;[name:[nemean]]</strong></span>
<span class="strong"><strong>==&gt;[name:[tartarus]]</strong></span>
<span class="strong"><strong>==&gt;[name:[saturn], age:[10000]]</strong></span>
<span class="strong"><strong>==&gt;[name:[sky]]</strong></span>
<span class="strong"><strong>==&gt;[name:[pluto], age:[4000]]</strong></span>
<span class="strong"><strong>==&gt;[name:[alcmene], age:[45]]</strong></span>
<span class="strong"><strong>==&gt;[name:[hercules], age:[30]]</strong></span>
<span class="strong"><strong>==&gt;[name:[sea]]</strong></span>
<span class="strong"><strong>==&gt;[name:[cerberus]]</strong></span>
<span class="strong"><strong>==&gt;[name:[neptune], age:[4500]]</strong></span>
</pre></div><p>So, there are 12 vertices in the graph, each has a name and age element shown in the previous data. Having successfully created a graph, it is now possible to configure the previous graph traversal Gremlin command to use Apache Spark for processing. This is simply achieved by specifying the <code class="literal">SparkGraphComputer</code> in the traversal command. See the full <span class="emphasis"><em>TinkerPop</em></span> diagram at the top of this chapter for architectural details. When this command is executed, you will <a id="id411" class="indexterm"></a>see the task appear on the Spark cluster user interface:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>t1.traversal(computer(SparkGraphComputer)).V().count()</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec64"></a>Using HBase</h3></div></div></div><p>When using<a id="id412" class="indexterm"></a> HBase, the properties file needs to change. The following values have been taken from my <code class="literal">hbase.properties</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gremlin.hadoop.graphInputFormat=com.thinkaurelius.titan.hadoop.formats.hbase.HBaseInputFormat</strong></span>

<span class="strong"><strong>input.conf.storage.backend=hbase</strong></span>
<span class="strong"><strong>input.conf.storage.hostname=hc2r1m2</strong></span>
<span class="strong"><strong>input.conf.storage.port=2181</strong></span>
<span class="strong"><strong>input.conf.storage.hbase.table=titan</strong></span>
<span class="strong"><strong>input.conf.storage.hbase.ext.zookeeper.znode.parent=/hbase</strong></span>
</pre></div><p>Remember that HBase uses Zookeeper for configuration purposes. So, the port number, and server for connection now becomes a <code class="literal">zookeeper</code> server, and <code class="literal">zookeeper</code> master port 2181. The <code class="literal">znode</code> parent value in Zookeeper is also defined as the top level node <code class="literal">/hbase</code>. Of course, the backend type is now defined to be <code class="literal">hbase</code>.</p><p>Also, the<a id="id413" class="indexterm"></a> <code class="literal">GraphInputFormat</code> class has been changed to <code class="literal">HBaseInputFormat</code> to describe HBase as an input source. A Titan graph can now be created using this properties file, as shown in the last section. I won't repeat the graph creation here, as it will be the same as the last section. Next, I will move on to filesystem storage.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec65"></a>Using the filesystem</h3></div></div></div><p>In order to<a id="id414" class="indexterm"></a> run this example, I used a basic Gremlin shell (<code class="literal">bin/gremlin.sh</code>). Within the data directory of the Titan release, there are many example data file formats that can be loaded to create graphs. In this example, I will use the file called <code class="literal">grateful-dead.kryo</code>. So this time, the data will be loaded straight from the file to a graph without specifying a storage backend, such as Cassandra. The properties file that I will use only contains the following entries:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph</strong></span>
<span class="strong"><strong>gremlin.hadoop.graphInputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoInputFormat</strong></span>
<span class="strong"><strong>gremlin.hadoop.graphOutputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat</strong></span>
<span class="strong"><strong>gremlin.hadoop.jarsInDistributedCache=true</strong></span>
<span class="strong"><strong>gremlin.hadoop.deriveMemory=true</strong></span>

<span class="strong"><strong>gremlin.hadoop.inputLocation=/usr/local/titan/data/grateful-dead.kryo</strong></span>
<span class="strong"><strong>gremlin.hadoop.outputLocation=output</strong></span>
</pre></div><p>Again, it uses the<a id="id415" class="indexterm"></a> Hadoop Gremlin package but this time the graph input and output formats are defined as <code class="literal">GryoInputFormat</code> and <code class="literal">GryoOutputFormat</code>. The input location is specified to be the actual <code class="literal">kyro</code>-based file. So, the source for input and output is the file. So now, the Groovy script looks like this. First, the graph is created using the properties file. Then, a graph traversal is created, so that we can count vertices and see the structure:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>graph = GraphFactory.open('/home/hadoop/spark/gremlin/hadoop-gryo.properties')</strong></span>
<span class="strong"><strong>g1 = graph.traversal()</strong></span>
</pre></div><p>Next, a vertex count is executed, which shows that there are over 800 vertices; and finally, a value map shows the structure of the data, which I have obviously clipped to save the space. But you can see the song name, type, and the performance details:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>g1.V().count()</strong></span>
<span class="strong"><strong>==&gt;808</strong></span>
<span class="strong"><strong>g1.V().valueMap()</strong></span>
<span class="strong"><strong>==&gt;[name:[MIGHT AS WELL], songType:[original], performances:[111]]</strong></span>
<span class="strong"><strong>==&gt;[name:[BROWN EYED WOMEN], songType:[original], performances:[347]]</strong></span>
</pre></div><p>This gives you a basic idea of the available functionality. I am sure that if you search the web, you will find more complex ways of using Spark with Titan. Take this for example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>r = graph.compute(SparkGraphComputer.class).program(PageRankVertexProgram.build().create()).submit().get()</strong></span>
</pre></div><p>The previous example specifies the use of the <code class="literal">SparkGraphComputer</code> class using the compute method. It also shows how the page rank vertex program, supplied with Titan, can be executed using the program method. This would modify your graph by adding page ranks to each vertex. I provide this as an example, as I am not convinced that it will work with Spark at this time.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec40"></a>Summary</h2></div></div><hr /></div><p>This chapter has introduced the Titan graph database from Aurelius. It has shown how it can be installed and configured on a Linux cluster. Using a Titan Gremlin shell example, the graphs have been created, and stored in both HBase and Cassandra NoSQL databases. The choice of Titan storage option required will depend upon your project requirements; HBase HDFS based storage or Cassandra non HDFS based  storage. This chapter has also shown that you can use the Gremlin shell both interactively to develop the graph scripts, and with Bash shell scripts so that you can run scheduled jobs with associated logging.</p><p>Simple Spark Scala code has been provided, which shows that Apache Spark can access the underlying tables that Titan creates on both HBase and Cassandra. This has been achieved by using the database connector modules provided by Cloudera (for HBase), and DataStax (for Cassandra). All example code and build scripts have been described along with the example output. I have included this Scala-based section to show you that the graph-based data can be accessed in Scala. The previous section processed data from the Gremlin shell, and used Spark as a processing backend. This section uses Spark as the main processing engine, and accesses Titan data from Spark. If the Gremlin shell was not suitable for your requirements, you might consider this approach. As Titan matures, so will the ways in which you can integrate Titan with Spark via Scala.</p><p>Finally, Titan's Gremlin shell has been used along with Apache Spark to demonstrate simple methods for creating, and accessing Titan-based graphs. Data has been stored on the file system, Cassandra, and HBase to do this.</p><p>Google groups are available for Aurelius and Gremlin users via the URLs at <a class="ulink" href="https://groups.google.com/forum/#!forum/aureliusgraphs" target="_blank">https://groups.google.com/forum/#!forum/aureliusgraphs</a> and <a class="ulink" href="https://groups.google.com/forum/#!forum/gremlin-users" target="_blank">https://groups.google.com/forum/#!forum/gremlin-users</a>.</p><p>Although the community seems smaller than other Apache projects, posting volume can be somewhat light, and it can be difficult to get a response to posts.</p><p>DataStax, the people who created Cassandra, acquired Aurelius, the creators of Titan this year. The creators of Titan are now involved in the development of DataStax's DSE graph database, which may have a knock-on effect on Titan's development. Having said that, the 0.9.x Titan release has been created, and a 1.0 release is expected.</p><p>So, having shown some of the Titan functionality with the help of an example with both Scala and Gremlin, I will close the chapter here. I wanted to show the pairing of Spark-based graph processing, and a graph storage system. I like open source systems for their speed of development and accessibility. I am not saying that Titan is the database for you, but it is a good example. If its future can be assured, and its community grows, then as it matures, it could offer a valuable resource.</p><p>Note that two versions of Spark have been used in this chapter: 1.3 and 1.2.1. The earlier version was required, because it was apparently the only version that would work with Titan's <code class="literal">SparkGraphComputer</code>, and so avoids Kyro serialization errors.</p><p>In the next chapter, extensions to the Apache Spark MLlib machine learning library will be examined in terms of the <a class="ulink" href="http://h2o.ai/" target="_blank">http://h2o.ai/</a> H2O product. A neural-based deep learning example will be developed in Scala to demonstrate its potential functionality.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>Chapter 7. Extending Spark with H2O</h2></div></div></div><p>H2O is an open <a id="id416" class="indexterm"></a>source system, developed in Java by <a class="ulink" href="http://h2o.ai/" target="_blank">http://h2o.ai/</a> for machine learning. It <a id="id417" class="indexterm"></a>offers a rich set of machine learning algorithms, and a web-based data processing user interface. It offers the ability to develop in a range of languages: Java, Scala, Python, and R. It also has the ability to interface to Spark, HDFS, Amazon S3, SQL, and NoSQL databases. This chapter will <a id="id418" class="indexterm"></a>concentrate on H2O's integration with Apache Spark using the <span class="strong"><strong>Sparkling Water</strong></span> component of H2O. A simple example, developed in Scala, will be used, based on real data to create a deep-learning model. This chapter will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Examine the H2O functionality</p></li><li style="list-style-type: disc"><p>Consider the necessary Spark H2O environment</p></li><li style="list-style-type: disc"><p>Examine the Sparkling Water architecture</p></li><li style="list-style-type: disc"><p>Introduce and use the H2O Flow interface</p></li><li style="list-style-type: disc"><p>Introduce deep learning with an example</p></li><li style="list-style-type: disc"><p>Consider performance tuning</p></li><li style="list-style-type: disc"><p>Examine data quality</p></li></ul></div><p>The next step will be to provide an overview of the H2O functionality, and the Sparkling Water architecture that will be used in this chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec41"></a>Overview</h2></div></div><hr /></div><p>Since it is only possible to <a id="id419" class="indexterm"></a>examine, and use, a small amount of H2O's functionality in this chapter, I thought that it would be useful to provide a list of all of the functional areas that it covers. This list is taken from <a class="ulink" href="http://h2o.ai/" target="_blank">http://h2o.ai/</a> website at <a class="ulink" href="http://h2o.ai/product/algorithms/" target="_blank">http://h2o.ai/product/algorithms/</a> and is based upon munging/wrangling data, modeling using the data, and scoring the resulting models:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Process</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Model</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>The score tool</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Data profiling</p>
</td><td style="" align="left" valign="top">
<p>Generalized Linear Models (GLM)</p>
</td><td style="" align="left" valign="top">
<p>Predict</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Summary statistics</p>
</td><td style="" align="left" valign="top">
<p>Decision trees</p>
</td><td style="" align="left" valign="top">
<p>Confusion Matrix</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Aggregate, filter, bin, and derive columns</p>
</td><td style="" align="left" valign="top">
<p>Gradient Boosting (GBM)</p>
</td><td style="" align="left" valign="top">
<p>AUC</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Slice, log transform, and anonymize</p>
</td><td style="" align="left" valign="top">
<p>K-Means</p>
</td><td style="" align="left" valign="top">
<p>Hit Ratio</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Variable creation</p>
</td><td style="" align="left" valign="top">
<p>Anomaly detection</p>
</td><td style="" align="left" valign="top">
<p>PCA Score</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>PCA</p>
</td><td style="" align="left" valign="top">
<p>Deep<a id="id420" class="indexterm"></a> learning</p>
</td><td style="" align="left" valign="top">
<p>Multi Model Scoring</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Training and validation sampling plan</p>
</td><td style="" align="left" valign="top">
<p>Naïve Bayes</p>
</td><td style="" align="left" valign="top"> </td></tr><tr><td style="" align="left" valign="top"> </td><td style="" align="left" valign="top">
<p>Grid search</p>
</td><td style="" align="left" valign="top"> </td></tr></tbody></table></div><p>The following section will explain the environment used for the Spark and H2O examples in this chapter and it will also explain some of the problems encountered.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec42"></a>The processing environment</h2></div></div><hr /></div><p>If any of you have examined my web-based blogs, or read my first book, <span class="emphasis"><em>Big Data Made Easy</em></span>, you will see that I am interested in Big Data integration, and how the big data tools connect. None of these<a id="id421" class="indexterm"></a> systems exist in isolation. The data will start upstream, be processed in Spark plus H2O, and then the result will be stored, or moved to the next step in the ETL chain. Given this idea in this example, I will use Cloudera CDH HDFS for <a id="id422" class="indexterm"></a>storage, and source my data from there. I could just as easily use S3, an SQL or NoSQL database.</p><p>At the point of starting the development work for this chapter, I had a Cloudera CDH 4.1.3 cluster installed and working. I also had various Spark versions installed, and available for use. They are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark 1.0 installed as CentOS services</p></li><li style="list-style-type: disc"><p>Spark 1.2 binary downloaded and installed</p></li><li style="list-style-type: disc"><p>Spark 1.3 built from a source snapshot</p></li></ul></div><p>I thought that I would experiment to see which combinations of Spark, and Hadoop I could get to work together. I <a id="id423" class="indexterm"></a>downloaded Sparkling water at <a class="ulink" href="http://h2o-release.s3.amazonaws.com/sparkling-water/master/98/index.html" target="_blank">http://h2o-release.s3.amazonaws.com/sparkling-water/master/98/index.html</a> and used the 0.2.12-95 version. I found that the 1.0 Spark version worked with H2O, but the Spark libraries were missing. Some of the functionality that was used in many of the Sparkling Water-based examples was available. Spark versions 1.2 and 1.3 caused the following error to occur:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>15/04/25 17:43:06 ERROR netty.NettyTransport: failed to bind to /192.168.1.103:0, shutting down Netty transport</strong></span>
<span class="strong"><strong>15/04/25 17:43:06 WARN util.Utils: Service 'sparkDriver' could not bind on port 0. Attempting port 1.</strong></span>
</pre></div><p>The Spark master port<a id="id424" class="indexterm"></a> number, although correctly configured in Spark, was not being picked up, and so the H2O-based application could not connect to Spark. After discussing the issue with the guys at H2O, I decided to upgrade to an H2O <a id="id425" class="indexterm"></a>certified version of both Hadoop and Spark. The<a id="id426" class="indexterm"></a> recommended system versions that should be used are available at <a class="ulink" href="http://h2o.ai/product/recommended-systems-for-h2o/" target="_blank">http://h2o.ai/product/recommended-systems-for-h2o/</a>.</p><p>I upgraded my CDH cluster from version 5.1.3 to version 5.3 using the Cloudera Manager interface parcels page. This automatically provided Spark 1.2—the version that has been integrated into the CDH cluster. This solved all the H2O-related issues, and provided me with an H2O-certified Hadoop and Spark environment.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec43"></a>Installing H2O</h2></div></div><hr /></div><p>For completeness, I will show you<a id="id427" class="indexterm"></a> how I downloaded, installed, and used H2O. Although, I finally settled on version 0.2.12-95, I first downloaded and used 0.2.12-92. This section is based on the earlier install, but the approach used to source the software is the same. The download link changes over time so follow the Sparkling Water <a id="id428" class="indexterm"></a>download option at <a class="ulink" href="http://h2o.ai/download/" target="_blank">http://h2o.ai/download/</a>.</p><p>This will source the zipped Sparkling water release, as shown by the CentOS Linux long file listing here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 h2o]$ pwd ; ls -l</strong></span>
<span class="strong"><strong>/home/hadoop/h2o</strong></span>
<span class="strong"><strong>total 15892</strong></span>
<span class="strong"><strong>-rw-r--r-- 1 hadoop hadoop 16272364 Apr 11 12:37 sparkling-water-0.2.12-92.zip</strong></span>
</pre></div><p>This zipped release file is unpacked using the Linux <code class="literal">unzip</code> command, and it results in a sparkling water release file tree:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 h2o]$ unzip sparkling-water-0.2.12-92.zip</strong></span>

<span class="strong"><strong>[hadoop@hc2r1m2 h2o]$ ls -d sparkling-water*</strong></span>
<span class="strong"><strong>sparkling-water-0.2.12-92  sparkling-water-0.2.12-92.zip</strong></span>
</pre></div><p>I have moved the release tree to the <code class="literal">/usr/local/</code> area using the root account, and created a simple symbolic link to the release called <code class="literal">h2o</code>. This means that my H2O-based build can refer to this link, and it doesn't need to change as new versions of sparkling water are sourced. I have<a id="id429" class="indexterm"></a> also made sure, using the Linux <code class="literal">chmod</code> command, that my development account, hadoop, has access to the release:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 h2o]$ su -</strong></span>
<span class="strong"><strong>[root@hc2r1m2 ~]# cd /home/hadoop/h2o</strong></span>
<span class="strong"><strong>[root@hc2r1m2 h2o]# mv sparkling-water-0.2.12-92 /usr/local</strong></span>
<span class="strong"><strong>[root@hc2r1m2 h2o]# cd /usr/local</strong></span>

<span class="strong"><strong>[root@hc2r1m2 local]# chown -R hadoop:hadoop sparkling-water-0.2.12-92</strong></span>
<span class="strong"><strong>[root@hc2r1m2 local]#  ln –s sparkling-water-0.2.12-92 h2o</strong></span>

<span class="strong"><strong>[root@hc2r1m2 local]# ls –lrt  | grep sparkling</strong></span>
<span class="strong"><strong>total 52</strong></span>
<span class="strong"><strong>drwxr-xr-x   6 hadoop hadoop 4096 Mar 28 02:27 sparkling-water-0.2.12-92</strong></span>
<span class="strong"><strong>lrwxrwxrwx   1 root   root     25 Apr 11 12:43 h2o -&gt; sparkling-water-0.2.12-92</strong></span>
</pre></div><p>The release has been installed on all the nodes of my Hadoop CDH clusters.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec44"></a>The build environment</h2></div></div><hr /></div><p>From past <a id="id430" class="indexterm"></a>examples, you will know that I favor SBT as a build tool for developing Scala source examples. I have created a development environment on the Linux CentOS 6.5 server called <code class="literal">hc2r1m2</code> using the hadoop development account. The development directory is called <code class="literal">h2o_spark_1_2</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 h2o_spark_1_2]$ pwd</strong></span>
<span class="strong"><strong>/home/hadoop/spark/h2o_spark_1_2</strong></span>
</pre></div><p>My SBT build configuration file named <code class="literal">h2o.sbt</code> is located here; it contains the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 h2o_spark_1_2]$ more h2o.sbt</strong></span>

<span class="strong"><strong>name := "H 2 O"</strong></span>

<span class="strong"><strong>version := "1.0"</strong></span>

<span class="strong"><strong>scalaVersion := "2.10.4"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.3.0"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "spark-core"  % "1.2.0" from "file:///opt/cloudera/parcels/CDH-5.3.3-1.cdh5.3.3.p0.5/jars/spark-assembly-1.2.0-cdh5.3.3-hadoop2.5.0-cdh5.3.3.jar"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "mllib"  % "1.2.0" from "file:///opt/cloudera/parcels/CDH-5.3-1.cdh5.3.3.p0.5/jars/spark-assembly-1.2.0-cdh5.3.3-hadoop2.5.0-cdh5.3.3.jar"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "sql"  % "1.2.0" from "file:///opt/cloudera/parcels/CDH-5.3.3-1.cdh5.3.3.p0.5/jars/spark-assembly-1.2.0-cdh5.3.3-hadoop2.5.0-cdh5.3.3.jar"</strong></span>

<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "h2o"  % "0.2.12-95" from "file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar"</strong></span>

<span class="strong"><strong>libraryDependencies += "hex.deeplearning" % "DeepLearningModel"  % "0.2.12-95" from "file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar"</strong></span>

<span class="strong"><strong>libraryDependencies += "hex" % "ModelMetricsBinomial"  % "0.2.12-95" from "file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar"</strong></span>

<span class="strong"><strong>libraryDependencies += "water" % "Key"  % "0.2.12-95" from "file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar"</strong></span>

<span class="strong"><strong>libraryDependencies += "water" % "fvec"  % "0.2.12-95" from "file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar"</strong></span>
</pre></div><p>I have provided SBT configuration <a id="id431" class="indexterm"></a>examples in the previous chapters, so I won't go into the line-by line-detail here. I have used the file-based URLs to define the library dependencies, and have sourced the Hadoop JAR files from the Cloudera parcel path for the CDH install. The Sparkling Water JAR path is defined as <code class="literal">/usr/local/h2o/</code> that was just created.</p><p>I use a Bash script called <code class="literal">run_h2o.bash</code> within this development directory to execute my H2O-based example code. It takes the application class name as a parameter, and is shown below:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 h2o_spark_1_2]$ more run_h2o.bash</strong></span>

<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>SPARK_HOME=/opt/cloudera/parcels/CDH</strong></span>
<span class="strong"><strong>SPARK_LIB=$SPARK_HOME/lib</strong></span>
<span class="strong"><strong>SPARK_BIN=$SPARK_HOME/bin</strong></span>
<span class="strong"><strong>SPARK_SBIN=$SPARK_HOME/sbin</strong></span>
<span class="strong"><strong>SPARK_JAR=$SPARK_LIB/spark-assembly-1.2.0-cdh5.3.3-hadoop2.5.0-cdh5.3.3.jar</strong></span>

<span class="strong"><strong>H2O_PATH=/usr/local/h2o/assembly/build/libs</strong></span>
<span class="strong"><strong>H2O_JAR=$H2O_PATH/sparkling-water-assembly-0.2.12-95-all.jar</strong></span>

<span class="strong"><strong>PATH=$SPARK_BIN:$PATH</strong></span>
<span class="strong"><strong>PATH=$SPARK_SBIN:$PATH</strong></span>
<span class="strong"><strong>export PATH</strong></span>

<span class="strong"><strong>cd $SPARK_BIN</strong></span>

<span class="strong"><strong>./spark-submit \</strong></span>
<span class="strong"><strong>  --class $1 \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 85m \</strong></span>
<span class="strong"><strong>  --total-executor-cores 50 \</strong></span>
<span class="strong"><strong>  --jars $H2O_JAR \</strong></span>
<span class="strong"><strong>  /home/hadoop/spark/h2o_spark_1_2/target/scala-2.10/h-2-o_2.10-1.0.jar</strong></span>
</pre></div><p>This example of Spark application submission has already been covered, so again, I won't get into the detail. Setting the executor memory at a correct value was critical to avoiding out-of-memory issues and performance problems. This will be examined in the <span class="emphasis"><em>Performance Tuning </em></span>section.</p><p>As in the previous examples, the application Scala code is located in the <code class="literal">src/main/scala</code> subdirectory, under<a id="id432" class="indexterm"></a> the <code class="literal">development</code> directory level. The next section will examine the Apache Spark, and the H2O architecture.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec45"></a>Architecture</h2></div></div><hr /></div><p>The diagrams in this section<a id="id433" class="indexterm"></a> have been sourced from the <a class="ulink" href="http://h2o.ai/" target="_blank">http://h2o.ai/</a> web site at <a class="ulink" href="http:// http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark/" target="_blank"> http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark/</a> to provide a <a id="id434" class="indexterm"></a>clear method of describing the way in which H2O Sparkling Water can be used to extend the functionality of Apache Spark. Both, H2O and Spark are open <a id="id435" class="indexterm"></a>source systems. Spark MLlib contains a great deal of functionality, while H2O extends this with a wide range of extra functionality, including deep learning. It offers tools to <span class="emphasis"><em>munge</em></span> (transform), model, and score the data. It also offers a web-based user interface to interact with.</p><p>The next diagram, borrowed<a id="id436" class="indexterm"></a> from <a class="ulink" href="http://h2o.ai/" target="_blank">http://h2o.ai/</a>, shows how H2O integrates with Spark. As we already know, Spark has master and worker servers; the workers create executors to do the actual work. The following steps occur to run a Sparkling water-based application:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Spark's <code class="literal">submit</code> command sends the sparkling water JAR to the Spark master.</p></li><li><p>The Spark master starts the workers, and distributes the JAR file.</p></li><li><p>The Spark workers start the executor JVMs to carry out the work.</p></li><li><p>The Spark executor starts an H2O instance.</p></li></ol></div><p>The H2O instance is embedded with the Executor JVM, and so it shares the JVM heap space with Spark. When all of the H2O instances have started, H2O forms a cluster, and then the H2O flow web interface is made available.</p><div class="mediaobject"><img src="graphics/B01989_07_01.jpg" /></div><p>The preceding diagram<a id="id437" class="indexterm"></a> explains how H2O fits into the Apache Spark architecture, and how<a id="id438" class="indexterm"></a> it starts, but what about data sharing? How does data pass between Spark and H2O? The following diagram explains this:</p><div class="mediaobject"><img src="graphics/B01989_07_02.jpg" /></div><p>A new H2O RDD data structure has been created for H2O and Sparkling Water. It is a layer, based at the top of an H2O frame, each column of which represents a data item, and is independently <a id="id439" class="indexterm"></a>compressed to provide the best compression ratio.</p><p>In the deep learning example, Scala code presented later in this chapter you will see that a data frame has been created implicitly from a Spark schema RDD and a columnar data item, income has been enumerated. I won't dwell on this now as it will be explained later but this is a practical example of the above <a id="id440" class="indexterm"></a>architecture:</p><div class="informalexample"><pre class="programlisting">  val testFrame:DataFrame = schemaRddTest
  testFrame.replace( testFrame.find("income"), testFrame.vec("income").toEnum)</pre></div><p>In the Scala-based example that will be tackled in this chapter, the following actions will take place:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Data is being sourced from HDFS, and is being stored in a Spark RDD.</p></li><li><p>Spark SQL is used to filter data.</p></li><li><p>The Spark schema RDD is converted into an H2O RDD.</p></li><li><p>The H2O-based processing and modeling occurs.</p></li><li><p>The results are passed back to Spark for accuracy checking.</p></li></ol></div><p>To this point, the general architecture of H2O has been examined, and the product has been sourced for use. The development environment has been explained, and the process by which H2O and Spark<a id="id441" class="indexterm"></a> integrate has been considered. Now, it is time to delve into a <a id="id442" class="indexterm"></a>practical example of the use of H2O. First though, some real-world data must be sourced for modeling purposes.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec46"></a>Sourcing the data</h2></div></div><hr /></div><p>Since I have already used the<a id="id443" class="indexterm"></a> <span class="strong"><strong>Artificial Neural Net</strong></span> (<span class="strong"><strong>ANN</strong></span>) functionality in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Apache Spark MLlib</em></span>, to classify images, it seems only fitting that I use H2O deep learning to classify data in this chapter. In order to do this, I need to source data sets that are suitable for <a id="id444" class="indexterm"></a>classification. I need either image data with associated image labels, or the data containing vectors and a label that I can enumerate, so that I can force H2O to use its classification algorithm.</p><p>The MNIST test and training<a id="id445" class="indexterm"></a> image data was sourced from <a class="ulink" href="http://ann.lecun.com/exdb/mnist/" target="_blank">ann.lecun.com/exdb/mnist/</a>. It contains 50,000 training rows, and 10,000 rows for testing. It contains digital images of numbers 0 to 9 and associated labels.</p><p>I was not able to use this data as, at the time of writing, there was a bug in H2O Sparkling water that limited the record size to 128 elements. The MNIST data has a record size of <span class="emphasis"><em>28 x 28 + 1</em></span> elements for the image plus the label:</p><div class="informalexample"><pre class="programlisting">15/05/14 14:05:27 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 256, hc2r1m4.semtech-solutions.co.nz): java.lang.ArrayIndexOutOfBoundsException: -128</pre></div><p>This issue should have been fixed and released by the time you read this, but in the short term I sourced another data set called income from <a class="ulink" href="http://www.cs.toronto.edu/~delve/data/datasets.html" target="_blank">http://www.cs.toronto.edu/~delve/data/datasets.html</a>, which contains Canadian employee income data. The following information shows the attributes and the data volume. It also shows the list of columns in the data, and a sample row of the data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Number of attributes: 16</strong></span>
<span class="strong"><strong>Number of cases: 45,225</strong></span>

<span class="strong"><strong>age workclass fnlwgt education educational-num marital-status occupation relationship race gender capital-gain capital-loss hours-per-week native-country income</strong></span>

<span class="strong"><strong>39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K</strong></span>
</pre></div><p>I will enumerate the<a id="id446" class="indexterm"></a> last column in the data—the income bracket, so <code class="literal">&lt;=50k</code> will enumerate to <code class="literal">0</code>. This will allow me to force the H2O deep learning algorithm to carry out classification rather than regression. I will also use Spark SQL to limit the data columns, and filter the data.</p><p>Data quality is absolutely critical when creating an H2O-based example like that described in this chapter. The next section examines the steps that can be taken to improve the data quality, and so save time.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec47"></a>Data Quality</h2></div></div><hr /></div><p>When I import CSV data files from HDFS to my Spark Scala H2O example code, I can filter the incoming data. The following example code contains two filter lines; the first checks that a data line is not empty, while the second checks that the final column in each data row (income), which will be enumerated, is not empty:</p><div class="informalexample"><pre class="programlisting">val testRDD  = rawTestData
  .filter(!_.isEmpty)
  .map(_.split(","))
  .filter( rawRow =&gt; ! rawRow(14).trim.isEmpty )</pre></div><p>I also needed to clean my raw data. There are two data sets, one for training and one for testing. It is important that the<a id="id447" class="indexterm"></a> training and testing data have the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The same number of columns</p></li><li style="list-style-type: disc"><p>The same data types</p></li><li style="list-style-type: disc"><p>The null values must be allowed for in the code</p></li><li style="list-style-type: disc"><p>The enumerated type values must match—especially for the labels</p></li></ul></div><p>I encountered an error related to the enumerated label column income and the values that it contained. I found that my test data set rows were terminated with a full stop character "<code class="literal">.</code>" When processed, this caused the training and the test data values to mismatch when enumerated.</p><p>So, I think that time and <a id="id448" class="indexterm"></a>effort should be spent safeguarding the data quality, as a pre-step to training, and testing machine learning functionality so that time is not lost, and extra cost incurred.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec48"></a>Performance tuning</h2></div></div><hr /></div><p>It is important to<a id="id449" class="indexterm"></a> monitor the Spark application error and the standard output logs in the Spark web user interface if you see errors like the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>05-15 13:55:38.176 192.168.1.105:54321   6375   Thread-10 ERRR: Out of Memory and no swap space left from hc2r1m1.semtech-solutions.co.nz/192.168.1.105:54321</strong></span>
</pre></div><p>If you encounter instances where application executors seem to hang without response, you may need to tune your executor memory. You need to do so if you see an error like the following in your executor log:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>05-19 13:46:57.300 192.168.1.105:54321   10044  Thread-11 WARN: Unblock allocations; cache emptied but memory is low:  OOM but cache is emptied:  MEM_MAX = 89.5 MB, DESIRED_CACHE = 96.4 MB, CACHE = N/A, POJO = N/A, this request bytes = 36.4 MB</strong></span>
</pre></div><p>This can cause a loop, as the application requests more memory than is available, and so waits until the next iteration retries. The application can seem to hang until the executors are killed, and the tasks re-executed on alternate nodes. A short task's run time can extend considerably due to such problems.</p><p>Monitor the Spark logs for these types of error. In the previous example, changing the executor memory setting in the <code class="literal">spark-submit</code> command removes the error, and reduces the runtime substantially. The memory value requested has been reduced to a figure below that which is available.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  --executor-memory 85m</strong></span>
</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec49"></a>Deep learning</h2></div></div><hr /></div><p>Neural networks were<a id="id450" class="indexterm"></a> introduced in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Apache Spark MLlib</em></span>. This chapter builds upon this understanding by introducing deep learning, which uses deep neural networks. These are neural networks that are feature-rich, and contain extra hidden layers, so that their ability to extract data features is increased. These networks are generally feed-forward networks, where the feature characteristics are inputs to the input layer neurons. These<a id="id451" class="indexterm"></a> neurons then fire and spread the activation through the hidden layer neurons to an output layer, which should present the feature label values. Errors in the output are then propagated back through the network (at least in back propagation), adjusting the neuron connection weight matrices so that classification errors are reduced during training.</p><div class="mediaobject"><img src="graphics/B01989_07_03.jpg" /></div><p>The previous example image, described<a id="id452" class="indexterm"></a> in the H2O booklet at <a class="ulink" href="https://leanpub.com/deeplearning/read" target="_blank">https://leanpub.com/deeplearning/read</a> ,shows a deep learning network with four input neurons to the left, two hidden layers in the middle, and two output neurons. The arrows show both the connections between neurons and the direction that activation takes through the network.</p><p>These networks are feature-rich because they provide the following options:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Multiple training algorithms</p></li><li style="list-style-type: disc"><p>Automated network configuration</p></li><li style="list-style-type: disc"><p>The ability to configure many options</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Structure</p><p>Hidden layer structure</p></li><li style="list-style-type: disc"><p>Training</p><p>Learning rate, annealing, and momentum</p></li></ul></div></li></ul></div><p>So, after giving this brief introduction<a id="id453" class="indexterm"></a> to deep learning, it is now time to look at some of the sample Scala-based code. H2O provides a great deal of functionality; the classes that are needed to build and run the network have been developed for you. You just need to do the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Prepare the data and parameters</p></li><li style="list-style-type: disc"><p>Create and train the model</p></li><li style="list-style-type: disc"><p>Validate the model with a second data set</p></li><li style="list-style-type: disc"><p>Score the validation data set output</p></li></ul></div><p>When scoring your model, you must hope for a high value in percentage terms. Your model must be able to accurately predict and classify your data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec66"></a>Example code – income</h3></div></div></div><p>This section <a id="id454" class="indexterm"></a>examines the Scala-based H2O Sparkling Water deep learning example using the previous Canadian income data source. First, the Spark (<code class="literal">Context</code>, <code class="literal">Conf</code>, <code class="literal">mllib</code>, and <code class="literal">RDD</code>), and H2O (<code class="literal">h2o</code>, <code class="literal">deeplearning</code>, and <code class="literal">water</code>) classes are imported:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import hex.deeplearning.{DeepLearningModel, DeepLearning}
import hex.deeplearning.DeepLearningModel.DeepLearningParameters
import org.apache.spark.h2o._
import org.apache.spark.mllib
import org.apache.spark.mllib.feature.{IDFModel, IDF, HashingTF}
import org.apache.spark.rdd.RDD
import water.Key</pre></div><p>Next an application class called <code class="literal">h2o_spark_dl2</code> is defined, the master URL is created, and then a configuration object is created, based on this URL, and the application name. The Spark context is then created using the configuration object:</p><div class="informalexample"><pre class="programlisting">object h2o_spark_dl2  extends App
{
  val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"
  val appName = "Spark h2o ex1"
  val conf = new SparkConf()

  conf.setMaster(sparkMaster)
  conf.setAppName(appName)

  val sparkCxt = new SparkContext(conf)</pre></div><p>An H2O context is created from the Spark context, and also an SQL context:</p><div class="informalexample"><pre class="programlisting">  import org.apache.spark.h2o._
  implicit val h2oContext = new org.apache.spark.h2o.H2OContext(sparkCxt).start()

  import h2oContext._
  import org.apache.spark.sql._

  implicit val sqlContext = new SQLContext(sparkCxt)</pre></div><p>The H2O Flow user interface is started with the <code class="literal">openFlow</code> command:</p><div class="informalexample"><pre class="programlisting">  import sqlContext._
  openFlow</pre></div><p>The training and testing of the <a id="id455" class="indexterm"></a>data files are now defined (on HDFS) using the server URL, path, and the file names:</p><div class="informalexample"><pre class="programlisting">  val server    = "hdfs://hc2nn.semtech-solutions.co.nz:8020"
  val path      = "/data/spark/h2o/"

  val train_csv =  server + path + "adult.train.data" // 32,562 rows
  val test_csv  =  server + path + "adult.test.data"  // 16,283 rows</pre></div><p>The CSV based training and testing data is loaded using the Spark context's <code class="literal">textFile</code> method:</p><div class="informalexample"><pre class="programlisting">  val rawTrainData = sparkCxt.textFile(train_csv)
  val rawTestData  = sparkCxt.textFile(test_csv)</pre></div><p>Now, the schema is defined in terms of a string of attributes. Then, a schema variable is created by splitting the string using a series of <code class="literal">StructField</code>, based on each column. The data types are left as String, and the true value allows for the Null values in the data:</p><div class="informalexample"><pre class="programlisting">  val schemaString = "age workclass fnlwgt education “ + 
“educationalnum maritalstatus " + "occupation relationship race 
gender “ + “capitalgain capitalloss " + hoursperweek nativecountry income"

  val schema = StructType( schemaString.split(" ")
      .map(fieldName =&gt; StructField(fieldName, StringType, true)))</pre></div><p>The raw CSV line <code class="literal">training</code> and testing data is now split by commas into columns. The data is filtered<a id="id456" class="indexterm"></a> on empty lines to ensure that the last column (<code class="literal">income</code>) is not empty. The actual data rows are created from the fifteen (0-14) trimmed elements in the raw CSV data. Both, the training and the test data sets are processed:</p><div class="informalexample"><pre class="programlisting">  val trainRDD  = rawTrainData
         .filter(!_.isEmpty)
         .map(_.split(","))
         .filter( rawRow =&gt; ! rawRow(14).trim.isEmpty )
         .map(rawRow =&gt; Row(
               rawRow(0).toString.trim,  rawRow(1).toString.trim,
               rawRow(2).toString.trim,  rawRow(3).toString.trim,
               rawRow(4).toString.trim,  rawRow(5).toString.trim,
               rawRow(6).toString.trim,  rawRow(7).toString.trim,
               rawRow(8).toString.trim,  rawRow(9).toString.trim,
               rawRow(10).toString.trim, rawRow(11).toString.trim,
               rawRow(12).toString.trim, rawRow(13).toString.trim,
               rawRow(14).toString.trim
                           )
             )


  val testRDD  = rawTestData
         .filter(!_.isEmpty)
         .map(_.split(","))
         .filter( rawRow =&gt; ! rawRow(14).trim.isEmpty )
         .map(rawRow =&gt; Row(
               rawRow(0).toString.trim,  rawRow(1).toString.trim,
               rawRow(2).toString.trim,  rawRow(3).toString.trim,
               rawRow(4).toString.trim,  rawRow(5).toString.trim,
               rawRow(6).toString.trim,  rawRow(7).toString.trim,
               rawRow(8).toString.trim,  rawRow(9).toString.trim,
               rawRow(10).toString.trim, rawRow(11).toString.trim,
               rawRow(12).toString.trim, rawRow(13).toString.trim,
               rawRow(14).toString.trim
                           )
             )</pre></div><p>Spark Schema RDD variables are now created for the training and test data sets by applying the schema<a id="id457" class="indexterm"></a> variable, created previously for the data using the Spark context's <code class="literal">applySchema</code> method:</p><div class="informalexample"><pre class="programlisting">  val trainSchemaRDD = sqlContext.applySchema(trainRDD, schema)
  val testSchemaRDD  = sqlContext.applySchema(testRDD,  schema)</pre></div><p>Temporary tables are created for the training and testing data:</p><div class="informalexample"><pre class="programlisting">  trainSchemaRDD.registerTempTable("trainingTable")
  testSchemaRDD.registerTempTable("testingTable")</pre></div><p>Now, SQL is run against these temporary tables, both to filter the number of columns, and to potentially limit the data. I could have added a <code class="literal">WHERE</code> or <code class="literal">LIMIT</code> clause. This is a useful approach that enables me to manipulate both the column and row-based data:</p><div class="informalexample"><pre class="programlisting">  val schemaRddTrain = sqlContext.sql(
    """SELECT
         |age,workclass,education,maritalstatus,
         |occupation,relationship,race,
         |gender,hoursperweek,nativecountry,income
         |FROM trainingTable """.stripMargin)

  val schemaRddTest = sqlContext.sql(
    """SELECT
         |age,workclass,education,maritalstatus,
         |occupation,relationship,race,
         |gender,hoursperweek,nativecountry,income
         |FROM testingTable """.stripMargin)</pre></div><p>The H2O data frames are now created from the data. The final column in each data set (income) is enumerated, because this is the column that will form the deep learning label for the data. Also, enumerating this column forces the deep learning model to carry out classification rather than regression:</p><div class="informalexample"><pre class="programlisting">  val trainFrame:DataFrame = schemaRddTrain
  trainFrame.replace( trainFrame.find("income"),        trainFrame.vec("income").toEnum)
  trainFrame.update(null)

  val testFrame:DataFrame = schemaRddTest
  testFrame.replace( testFrame.find("income"),        testFrame.vec("income").toEnum)
  testFrame.update(null)</pre></div><p>The enumerated <a id="id458" class="indexterm"></a>results data income column is now saved so that the values in this column can be used to score the tested model prediction values:</p><div class="informalexample"><pre class="programlisting">  val testResArray = schemaRddTest.collect()
  val sizeResults  = testResArray.length
  var resArray     = new Array[Double](sizeResults)

  for ( i &lt;- 0 to ( resArray.length - 1)) {
     resArray(i) = testFrame.vec("income").at(i)
  }</pre></div><p>The deep learning model parameters are now set up in terms of the number of epochs, or iterations—the data sets for training and validation and the label column income, which will be used to classify the data. Also, we chose to use variable importance to determine which data columns are most important in the data. The deep learning model is then created:</p><div class="informalexample"><pre class="programlisting">  val dlParams = new DeepLearningParameters()

  dlParams._epochs               = 100
  dlParams._train                = trainFrame
  dlParams._valid                = testFrame
  dlParams._response_column      = 'income
  dlParams._variable_importances = true
  val dl = new DeepLearning(dlParams)
  val dlModel = dl.trainModel.get</pre></div><p>The model is then scored against the test data set for predictions, and these income predictions are compared to the previously stored enumerated test data income values. Finally, an accuracy percentage is output from the test data:</p><div class="informalexample"><pre class="programlisting">  val testH2oPredict  = dlModel.score(schemaRddTest )('predict)
  val testPredictions  = toRDD[DoubleHolder](testH2oPredict)
          .collect.map(_.result.getOrElse(Double.NaN))
  var resAccuracy = 0
  for ( i &lt;- 0 to ( resArray.length - 1)) {
    if (  resArray(i) == testPredictions(i) )
      resAccuracy = resAccuracy + 1
  }

  println()
  println( "&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;" )
  println( "&gt;&gt;&gt;&gt;&gt;&gt; Model Test Accuracy = "
       + 100*resAccuracy / resArray.length  + " % " )
  println( "&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;" )
  println()</pre></div><p>In the last step, the application is stopped, the H2O functionality is terminated via a <code class="literal">shutdown</code> call, and then the Spark context is stopped:</p><div class="informalexample"><pre class="programlisting">  water.H2O.shutdown()
  sparkCxt.stop()

  println( " &gt;&gt;&gt;&gt;&gt; Script Finished &lt;&lt;&lt;&lt;&lt; " )

} // end application</pre></div><p>Based upon a<a id="id459" class="indexterm"></a> training data set of 32,000, and a test data set of 16,000 income records, this deep learning model is quite accurate. It reaches an accuracy level of <code class="literal">83</code> percent, which is impressive for a few lines of code, small data sets, and just 100 epochs, as the run output shows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;&gt;&gt;&gt; Model Test Accuracy = 83 %</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</strong></span>
</pre></div><p>In the next section, I will examine some of the coding needed to process the MNIST data, even though that example could not be completed due to an H2O limitation at the time of coding.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec67"></a>The example code – MNIST</h3></div></div></div><p>Since the MNIST image <a id="id460" class="indexterm"></a>data record is so big, it presents problems while creating a Spark <a id="id461" class="indexterm"></a>SQL schema, and processing a data record. The records in this data are in CSV format, and are formed from a 28 x 28 digit image. Each line is then terminated by a label value for the image. I have created my schema by defining a function to create the schema string to represent the record, and then calling it:</p><div class="informalexample"><pre class="programlisting">  def getSchema(): String = {

    var schema = ""
    val limit = 28*28

    for (i &lt;- 1 to limit){
      schema += "P" + i.toString + " "
    }
    schema += "Label"

    schema // return value
  }

  val schemaString = getSchema()
  val schema = StructType( schemaString.split(" ")
      .map(fieldName =&gt; StructField(fieldName, IntegerType, false)))</pre></div><p>The same general <a id="id462" class="indexterm"></a>approach to deep learning can be taken to data processing as the previous example, apart from the actual processing of the raw CSV data. There are too many columns to process individually, and they all need to be converted into integers to<a id="id463" class="indexterm"></a> represent their data type. This can be done in one of two ways. In the first example, <code class="literal">var args</code> can be used to process all the elements in the row:</p><div class="informalexample"><pre class="programlisting">val trainRDD  = rawTrainData.map( rawRow =&gt; Row( rawRow.split(",").map(_.toInt): _* ))</pre></div><p>The second example uses the <code class="literal">fromSeq</code> method to process the row elements:</p><div class="informalexample"><pre class="programlisting">  val trainRDD  = rawTrainData.map(rawRow =&gt; Row.fromSeq(rawRow.split(",") .map(_.toInt)))</pre></div><p>In the next section, the H2O Flow user interface will be examined to see how it can be used to both monitor H2O and process the data.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec50"></a>H2O Flow</h2></div></div><hr /></div><p>H2O Flow is a web-based <a id="id464" class="indexterm"></a>open source user interface for H2O, and given that it is being used with Spark, Sparkling Water. It is a fully functional H2O web interface for monitoring the H2O Sparkling Water cluster plus jobs, and also for manipulating data and training models. I have created some simple example code to start the H2O interface. As in the previous Scala-based code samples, all I need to do is create a Spark, an H2O context, and then call the <code class="literal">openFlow</code> command, which will start the Flow interface.</p><p>The following Scala code example just imports classes for Spark context, configuration, and H2O. It then defines the configuration in terms of the application name and the Spark cluster URL. A Spark context is then created using the configuration object:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.h2o._

object h2o_spark_ex2  extends App
{
  val sparkMaster = "spark://hc2nn.semtech-solutions.co.nz:7077"
  val appName = "Spark h2o ex2"
  val conf = new SparkConf()

  conf.setMaster(sparkMaster)
  conf.setAppName(appName)

  val sparkCxt = new SparkContext(conf)</pre></div><p>An H2O context is then created, and started using the Spark context. The H2O context classes are imported, and the Flow user interface is started with the <code class="literal">openFlow</code> command:</p><div class="informalexample"><pre class="programlisting">  implicit val h2oContext = new org.apache.spark.h2o.H2OContext(sparkCxt).start()

  import h2oContext._

  // Open H2O UI

  openFlow</pre></div><p>Note, for the purposes of this example and to enable me to use the Flow application, I have commented out the H2O shutdown and the Spark context stop options. I would not normally do this, but I wanted to make this application long-running so that it gives me plenty of time to use the interface:</p><div class="informalexample"><pre class="programlisting">  // shutdown h20

//  water.H2O.shutdown()
//  sparkCxt.stop()

  println( " &gt;&gt;&gt;&gt;&gt; Script Finished &lt;&lt;&lt;&lt;&lt; " )

} // end application</pre></div><p>I use my Bash script <code class="literal">run_h2o.bash</code> with the application class name called <code class="literal">h2o_spark_ex2</code> as a <a id="id465" class="indexterm"></a>parameter. This script contains a call to the <code class="literal">spark-submit</code> command, which will execute the compiled application:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2r1m2 h2o_spark_1_2]$ ./run_h2o.bash h2o_spark_ex2</strong></span>
</pre></div><p>When the application runs, it lists the state of the H2O cluster and provides a URL by which the H2O Flow browser can be accessed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>15/05/20 13:00:21 INFO H2OContext: Sparkling Water started, status of context:</strong></span>
<span class="strong"><strong>Sparkling Water Context:</strong></span>
<span class="strong"><strong> * number of executors: 4</strong></span>
<span class="strong"><strong> * list of used executors:</strong></span>
<span class="strong"><strong>  (executorId, host, port)</strong></span>
<span class="strong"><strong>  ------------------------</strong></span>
<span class="strong"><strong>  (1,hc2r1m4.semtech-solutions.co.nz,54321)</strong></span>
<span class="strong"><strong>  (3,hc2r1m2.semtech-solutions.co.nz,54321)</strong></span>
<span class="strong"><strong>  (0,hc2r1m3.semtech-solutions.co.nz,54321)</strong></span>
<span class="strong"><strong>  (2,hc2r1m1.semtech-solutions.co.nz,54321)</strong></span>
<span class="strong"><strong>  ------------------------</strong></span>

<span class="strong"><strong>  Open H2O Flow in browser: http://192.168.1.108:54323 (CMD + click in Mac OSX)</strong></span>
</pre></div><p>The previous example shows that I can access the H2O interface using the port number <code class="literal">54323</code> on the host IP address <code class="literal">192.168.1.108</code>. I can simply check my host's file to confirm that the host name is <code class="literal">hc2r1m2</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ cat /etc/hosts | grep hc2</strong></span>
<span class="strong"><strong>192.168.1.103 hc2nn.semtech-solutions.co.nz   hc2nn</strong></span>
<span class="strong"><strong>192.168.1.105 hc2r1m1.semtech-solutions.co.nz   hc2r1m1</strong></span>
<span class="strong"><strong>192.168.1.108 hc2r1m2.semtech-solutions.co.nz   hc2r1m2</strong></span>
<span class="strong"><strong>192.168.1.109 hc2r1m3.semtech-solutions.co.nz   hc2r1m3</strong></span>
<span class="strong"><strong>192.168.1.110 hc2r1m4.semtech-solutions.co.nz   hc2r1m4</strong></span>
</pre></div><p>So, I can access the interface using the <code class="literal">hc2r1m2:54323</code> URL. The following screenshot shows the Flow interface with no data loaded. There are data processing and administration menu options and buttons at the top of the page. To the right, there are help options to enable you to learn more about H2O:</p><div class="mediaobject"><img src="graphics/B01989_07_04.jpg" /></div><p>The following screenshot <a id="id466" class="indexterm"></a>shows the menu options and buttons in greater detail. In the following sections, I will use a practical example to explain some of these options, but<a id="id467" class="indexterm"></a> there will not be enough space in this chapter to cover all the functionality. Check the <a class="ulink" href="http://h2o.ai/" target="_blank">http://h2o.ai/</a> website to learn about the Flow application in detail, available at <a class="ulink" href="http://h2o.ai/product/flow/" target="_blank">http://h2o.ai/product/flow/</a>:</p><div class="mediaobject"><img src="graphics/B01989_07_05.jpg" /></div><p>In greater definition, you can see that the previous menu options and buttons allow you to both administer your H2O Spark cluster, and also manipulate the data that you wish to process. The following <a id="id468" class="indexterm"></a>screenshot shows a reformatted list of the help options available, so that, if you get stuck, you can investigate solving your problem from the same interface:</p><div class="mediaobject"><img src="graphics/B01989_07_06.jpg" /></div><p>If I use the menu option, <span class="strong"><strong>Admin</strong></span> | <span class="strong"><strong>Cluster Status</strong></span>, I will obtain the following screenshot, which shows me the status of each cluster server in terms of memory, disk, load, and cores. It's a useful snapshot that provides me with a color-coded indication of the status:</p><div class="mediaobject"><img src="graphics/B01989_07_07.jpg" /></div><p>The menu option, <span class="strong"><strong>Admin</strong></span> | <span class="strong"><strong>Jobs,</strong></span> provides details of the current cluster jobs in terms of the start, end, and run times, as <a id="id469" class="indexterm"></a>well as status. Clicking on the job name provides further details, as shown next, including data processing details, and an estimated run time, which is useful. Also, if you select the <span class="strong"><strong>Refresh</strong></span> button, the display will continuously refresh until it is deselected:</p><div class="mediaobject"><img src="graphics/B01989_07_08.jpg" /></div><p>The <span class="strong"><strong>Admin</strong></span> | <span class="strong"><strong>Water Meter</strong></span> option provides a visual display of the CPU usage on each node in the cluster. As you can see in the following screenshot, my meter shows that my cluster was idle:</p><div class="mediaobject"><img src="graphics/B01989_07_09.jpg" /></div><p>Using the menu option, <span class="strong"><strong>Flow</strong></span> | <span class="strong"><strong>Upload File</strong></span>, I have uploaded some of the training data used in the previous deep learning Scala-based example. The data has been loaded into a data preview pane; I can <a id="id470" class="indexterm"></a>see a sample of the data that has been organized into cells. Also, an accurate guess has been made of the data types so that I can see which columns can be enumerated. This is useful if I want to consider classification:</p><div class="mediaobject"><img src="graphics/B01989_07_10.jpg" /></div><p>Having loaded the data, I am now presented with a <span class="strong"><strong>Frame</strong></span> display, which offers me the ability to view, inspect, build a model, create a prediction, or download the data. The data display shows information like min, max, and mean. It shows data types, labels, and a zero data count, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_07_11.jpg" /></div><p>I thought that it would be useful to create a deep learning classification model, based on this data, to compare the Scala-based approach to this H2O user interface. Using the view and inspect options, it is <a id="id471" class="indexterm"></a>possible to visually, and interactively check the data, as well as create plots relating to the data. For instance, using the previous inspect option followed by the plot columns option, I was able to create a plot of data labels versus zero counts in the column data. The following screenshot shows the result:</p><div class="mediaobject"><img src="graphics/B01989_07_12.jpg" /></div><p>By selecting the build <a id="id472" class="indexterm"></a>model option, a menu option is offered that lets me choose a model type. I will select deep learning, as I already know that this data is suited to this classification approach. The previous Scala-based model resulted in an accuracy level of 83 percent:</p><div class="mediaobject"><img src="graphics/B01989_07_13.jpg" /></div><p>I have selected the deep<a id="id473" class="indexterm"></a> learning option. Having chosen this option, I am then able to set model parameters, such as training and validation data sets, as well as choosing the data columns that my model should use (obviously, the two data sets should contain the same columns). The following screenshot displays the data sets, and the model columns being selected:</p><div class="mediaobject"><img src="graphics/B01989_07_14.jpg" /></div><p>There are a large range of basic and advanced model options available. A selection of them are shown in the following screenshot. I have set the response column to 15 as the income column. I have also set the <span class="strong"><strong>VARIABLE_IMPORTANCES</strong></span> option. Note that I don't need to enumerate the response column, as it has been done automatically:</p><div class="mediaobject"><img src="graphics/B01989_07_15.jpg" /></div><p>Note also that the epochs<a id="id474" class="indexterm"></a> or iterations option is set to <span class="strong"><strong>100</strong></span> as before. Also, the figure <code class="literal">200,200</code> for the hidden layers indicates that the network has two hidden layers, each with 200 neurons. Selecting the build model option causes the model to be created from these parameters. The following screenshot shows the model being trained, including an estimation of training time and an indication of the data processed so far.</p><div class="mediaobject"><img src="graphics/B01989_07_16.jpg" /></div><p>Viewing the model, once<a id="id475" class="indexterm"></a> trained, shows training and validation metrics, as well as a list of the important training parameters:</p><div class="mediaobject"><img src="graphics/B01989_07_17.jpg" /></div><p>Selecting the <span class="strong"><strong>Predict</strong></span> <a id="id476" class="indexterm"></a>option allows an alternative validation data set to be specified. Choosing the <span class="strong"><strong>Predict</strong></span> option using the new data set causes the already trained model to be validated against a new test dataset:</p><div class="mediaobject"><img src="graphics/B01989_07_18.jpg" /></div><p>Selecting the <span class="strong"><strong>Predict</strong></span> option causes the prediction details for the deep learning model, and dataset to be displayed as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_07_19.jpg" /></div><p>The preceding screenshot shows the test data frame and the model category, as well as the validation statistics <a id="id477" class="indexterm"></a>in terms of AUC, GINI, and MSE.</p><p>The AUC value, or area under the curve, relates to the ROC, or the receiver operator characteristics curve, which is <a id="id478" class="indexterm"></a>also shown in the following screenshot. TPR means <span class="strong"><strong>True Positive Rate</strong></span>, and FPR means <span class="strong"><strong>False Positive Rate</strong></span>. AUC is a<a id="id479" class="indexterm"></a> measure of accuracy with a value of one being perfect. So, the blue line shows greater accuracy than that of the red line:</p><div class="mediaobject"><img src="graphics/B01989_07_20.jpg" /></div><p>There is a great deal <a id="id480" class="indexterm"></a>of functionality available within this interface that I have not explained, but I hope that I have given you a feel for its power and potential. You can use this interface to inspect your data, and create reports before attempting to develop code, or as an application in its own right to delve into your data.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec51"></a>Summary</h2></div></div><hr /></div><p>My continuing theme, when examining both Apache Hadoop and Spark, is that none of these systems stand alone. They need to be integrated to form ETL-based processing systems. Data needs to be sourced and processed in Spark, and then passed to the next link in the ETL chain, or stored. I hope that this chapter has shown you that Spark functionality can be extended with extra libraries, and systems such as H2O.</p><p>Although Apache Spark MLlib (machine learning library) has a lot of functionality, the combination of H2O Sparkling Water and the Flow web interface provides an extra wealth of data analysis modeling options. Using Flow, you can also visually, and interactively process your data. I hope that this chapter shows you, even though it cannot cover all that H2O offers, that the combination of Spark and H2O widens your data processing possibilities.</p><p>I hope that you have found this chapter useful. As a next step, you might consider checking the <a class="ulink" href="http://h2o.ai/" target="_blank">http://h2o.ai/</a> website or the H2O Google group, which is available at <a class="ulink" href="https://groups.google.com/forum/#!forum/h2ostream" target="_blank">https://groups.google.com/forum/#!forum/h2ostream</a>.</p><p>The next chapter will examine the Spark-based service <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>, which will use Amazon AWS storage for Spark cluster creation in the cloud.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>Chapter 8. Spark Databricks</h2></div></div></div><p>Creating a big data analytics cluster, importing data, and creating ETL streams to cleanse and process the data are hard to do, and also expensive. The aim of Databricks is to decrease the complexity and make the process of cluster creation, and data processing easier. They have created a cloud-based platform, based on Apache Spark that automates cluster creation, and simplifies data import, processing, and visualization. Currently, the storage is based upon AWS but, in the future, they plan to expand to other cloud providers.</p><p>The same people who <a id="id481" class="indexterm"></a>designed Apache Spark are involved in the Databricks<a id="id482" class="indexterm"></a> system. At the time of writing this book, the service was only accessible via registration. I have been offered a 30-day trial period. Over the next two chapters, I will examine the service, and its components, and offer some sample code to show how it works. This chapter will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Installing Databricks</p></li><li style="list-style-type: disc"><p>AWS configuration</p></li><li style="list-style-type: disc"><p>Account management</p></li><li style="list-style-type: disc"><p>The menu system</p></li><li style="list-style-type: disc"><p>Notebooks and folders</p></li><li style="list-style-type: disc"><p>Importing jobs via libraries</p></li><li style="list-style-type: disc"><p>Development environments</p></li><li style="list-style-type: disc"><p>Databricks tables</p></li><li style="list-style-type: disc"><p>The Databricks DbUtils package</p></li></ul></div><p>Given that this book is provided in a static format, it will be difficult to fully examine functionality such as streaming.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec52"></a>Overview</h2></div></div><hr /></div><p>The Databricks service, available <a id="id483" class="indexterm"></a>at the <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a> website, is based upon the idea of a cluster. This is similar to a Spark cluster, which has already been examined and used in previous chapters. It contains a master, workers, and executors. However, the configuration and the<a id="id484" class="indexterm"></a> size of the cluster are automated, depending upon the amount of memory that you specify. Features such as security, isolation, process monitoring, and resource management are all automatically managed for you. If you have an immediate requirement for a Spark-based cluster using 200 GB of memory, for a short period of time, this service can be used to dynamically create it, and process your data. You can terminate the cluster to reduce your costs when the processing is finished.</p><p>Within a cluster, the idea of a Notebook is introduced, along with a location for you to create scripts and run programs. Folders can be created within Notebooks, which can be based upon Scala, Python, or SQL. Jobs can be created to execute the functionality, and can be called from the Notebook code or the imported libraries. Notebooks can call Notebook functionality. Also, the functionality is provided to schedule jobs, based on time or event.</p><p>This provides you with a feel of what the Databricks service provides. The following sections will explain each major item that has been introduced. Please keep in mind that what is presented here is new and evolving. Also, I used the AWS US East (North Virginia) region for this demonstration, as the Asia Sydney region currently has limitations that caused the Databricks install to fail.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec53"></a>Installing Databricks</h2></div></div><hr /></div><p>In order to create this<a id="id485" class="indexterm"></a> demonstration, I used the AWS offer of a year's free access, which was <a id="id486" class="indexterm"></a>available at <a class="ulink" href="http://aws.amazon.com/free/" target="_blank">http://aws.amazon.com/free/</a>. This has limitations such as 5 GB of S3 storage, and 750 hours of <a id="id487" class="indexterm"></a>
<span class="strong"><strong>Amazon Elastic Compute Cloud</strong></span> (<span class="strong"><strong>EC2</strong></span>), but it allowed me low-cost access and reduced my overall EC2 costs. The AWS account provides the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>An account ID</p></li><li style="list-style-type: disc"><p>An access Key ID</p></li><li style="list-style-type: disc"><p>A secret access Key</p></li></ul></div><p>These items of information are used by Databricks to access your AWS storage, install the Databricks systems, and create the cluster components that you specify. From the moment of the install, you begin to incur AWS EC2 costs, as the Databricks system uses at least two running instances without any clusters. Once you have successfully entered your AWS and billing information, you will be prompted to launch the Databricks cloud.</p><div class="mediaobject"><img src="graphics/B01989_08_01.jpg" /></div><p>Having done this, you <a id="id488" class="indexterm"></a>will be provided with a URL to access your cloud, an admin account, and password. This will allow you to access the Databricks web-based user interface, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_08_02.jpg" /></div><p>This is the welcome screen. It shows the menu bar at the top of the image, which, from left to right, contains the menu, search, help, and account icons. While using the system, there may also be a clock-faced icon that shows the recent activity. From this single interface, you may<a id="id489" class="indexterm"></a> search through help screens, and usage examples before creating your own clusters and code.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec54"></a>AWS billing</h2></div></div><hr /></div><p>Please note that, once<a id="id490" class="indexterm"></a> you have the Databricks system installed, you will start incurring the AWS EC2 storage costs. Databricks<a id="id491" class="indexterm"></a> attempts to minimize your costs by keeping EC2 resources active for a full charging period. For instance if you terminate a Databricks cluster the cluster-based EC2 instances will still exist for the hour in which AWS bills for them. In this way, Databricks can reuse them if you create a new cluster. The following screenshot shows that, although I am using a free AWS account, and though I have carefully reduced my resource usage, I have incurred AWS EC2 costs in a short period of time:</p><div class="mediaobject"><img src="graphics/B01989_08_03.jpg" /></div><p>You need to be aware of the Databricks clusters that you create, and understand that, while they exist and are used, AWS costs are being incurred. Only keep the clusters that you really require, and terminate any others.</p><p>In order to <a id="id492" class="indexterm"></a>examine the <a id="id493" class="indexterm"></a>Databricks data import functionality, I also created an AWS S3 bucket, and uploaded data files to it. This will be explained later in this chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec55"></a>Databricks menus</h2></div></div><hr /></div><p>By selecting the top-left menu icon on the <a id="id494" class="indexterm"></a>Databricks web interface, it is possible to expand the menu system. The following screenshot shows the top-level menu options, as well as the <span class="strong"><strong>Workspace</strong></span> option, expanded to a folder hierarchy of <code class="literal">/folder1/folder2/</code>. Finally, it shows the actions that can be carried out on <code class="literal">folder2</code>, that is, creating a notebook, creating a dashboard, and more.</p><div class="mediaobject"><img src="graphics/B01989_08_04.jpg" /></div><p>All of these actions <a id="id495" class="indexterm"></a>will be expanded in future sections. The next section will examine account management, before moving on to clusters.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec56"></a>Account management</h2></div></div><hr /></div><p>Account management is quite <a id="id496" class="indexterm"></a>simplified within Databricks. There is a default Administrator account and subsequent accounts can be created, but you need to know the<a id="id497" class="indexterm"></a> Administrator password to do so. Passwords need to be more than eight characters long; they should contain at least one digit, one upper case character, and one non-alphanumeric character. <span class="strong"><strong>Account</strong></span> options can be accessed from the top-right menu option, shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_08_05.jpg" /></div><p>This also allows the user to logout. By selecting the account setting, you can change your password. By selecting the <span class="strong"><strong>Accounts</strong></span> menu option, an <span class="strong"><strong>Accounts</strong></span> list is generated. There, you will find an option to <span class="strong"><strong>Add Account</strong></span>, and each account can be deleted via an <span class="strong"><strong>X</strong></span> option on each account line, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_08_06.jpg" /></div><p>It is also possible to<a id="id498" class="indexterm"></a> reset the account passwords from the accounts list. Selecting the <span class="strong"><strong>Add Account</strong></span> option creates a new account window that requires an email address, a full name, the administrator password, and the user's password. So, if you want to create a new user, you need to know your Databricks instance Administrator password. You must also follow the rules for new passwords, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Minimum of eight characters</p></li><li style="list-style-type: disc"><p>Must contain at least one digit in the range: 0-9</p></li><li style="list-style-type: disc"><p>Must contain at least one upper case character in the range: A-Z</p></li><li style="list-style-type: disc"><p>Must contain at least one non-alphanumeric character: !@#$%</p><div class="mediaobject"><img src="graphics/B01989_08_07.jpg" /></div></li></ul></div><p>The next section <a id="id499" class="indexterm"></a>will examine the <span class="strong"><strong>Clusters</strong></span> menu option, and will enable you to manage your own Databricks Spark clusters.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec57"></a>Cluster management</h2></div></div><hr /></div><p>Selecting the <span class="strong"><strong>Clusters</strong></span> menu <a id="id500" class="indexterm"></a>option provides a list of your<a id="id501" class="indexterm"></a> current Databricks clusters and their status. Of course, currently you have none. Selecting the <span class="strong"><strong>Add Cluster</strong></span> option allows you to create one. Note that the amount of memory you specify determines the size of your cluster. There is a minimum of 54 GB required to create a cluster with a single master and worker. For each additional 54 GB specified, another worker is added.</p><div class="mediaobject"><img src="graphics/B01989_08_08.jpg" /></div><p>The following<a id="id502" class="indexterm"></a> screenshot is a concatenated image, showing a new cluster called <code class="literal">semclust1</code> being created and in a <span class="strong"><strong>Pending</strong></span> state. While <span class="strong"><strong>Pending</strong></span>, the cluster has no dashboard, and the cluster nodes are not accessible.</p><div class="mediaobject"><img src="graphics/B01989_08_09.jpg" /></div><p>Once created the cluster memory is listed and it's status changes from <span class="strong"><strong>Pending</strong></span> to <span class="strong"><strong>Running</strong></span>. A default dashboard has automatically been attached, and the Spark master and worker user interfaces can be accessed. It is important to note here that Databricks automatically starts and<a id="id503" class="indexterm"></a> manages the cluster processes. There is also an <span class="strong"><strong>Option</strong></span> column to the right of this display that offers the ability to <span class="strong"><strong>Configure</strong></span>, <span class="strong"><strong>Restart</strong></span>, or <span class="strong"><strong>Terminate</strong></span> a cluster as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_08_10.jpg" /></div><p>By reconfiguring a cluster, you can change its size. By adding more memory, you can add more workers. The following screenshot shows a cluster, created at the default size of 54 GB, having its memory extended to <code class="literal">108</code> GB.</p><div class="mediaobject"><img src="graphics/B01989_08_11.jpg" /></div><p>Terminating a cluster removes it, and it cannot be recovered. So, you need to be sure that deletion is the correct course of action. Databricks prompts you to confirm your action before the termination actually takes place.</p><div class="mediaobject"><img src="graphics/B01989_08_12.jpg" /></div><p>It takes time for a cluster to be both, created and terminated. During termination, the cluster is marked <a id="id504" class="indexterm"></a>with an orange banner, and a state of <span class="strong"><strong>Terminating</strong></span>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_08_13.jpg" /></div><p>Note that the cluster type in the previous screenshot is shown to be <span class="strong"><strong>On-demand</strong></span>. When creating a cluster, it is possible to select a check box called <span class="strong"><strong>Use spot instances to create a spot cluster</strong></span>. These clusters are cheaper than the on-demand clusters, as they bid for a cheaper AWS spot price. However, they can be slower to start than the on-demand clusters.</p><p>The Spark user interfaces are the same as those you would expect on a non-Databricks Spark cluster. You can examine workers, executors, configuration, and log files. As you create clusters, they will be added to your cluster list. One of the clusters will be used as the cluster where the dashboards are run. This can be changed by using the <span class="strong"><strong>Make Dashboard Cluster</strong></span> option. As you add libraries and Notebooks to your cluster, the cluster details entry will be updated with a count of the numbers added.</p><p>The only thing that I would say about the Databricks Spark user interface option at this time, because it is familiar, is that it displays the Spark version that is used. The following screenshot, extracted from the master user interface, shows that the Spark version being used (1.3.0) is very up-to-date. At the time of writing, the latest Apache Spark release was 1.3.1, dated 17 April, 2015.</p><div class="mediaobject"><img src="graphics/B01989_08_14.jpg" /></div><p>The next section <a id="id505" class="indexterm"></a>will examine Databricks Notebooks and folders—how to create them, and how they can be used.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec58"></a>Notebooks and folders</h2></div></div><hr /></div><p>A Notebook is a <a id="id506" class="indexterm"></a>special type of Databricks folder that can be used to create Spark scripts. Notebooks <a id="id507" class="indexterm"></a>can call the Notebook<a id="id508" class="indexterm"></a> scripts to create a hierarchy of functionality. When<a id="id509" class="indexterm"></a> created, the type of Notebook must be specified (Python, Scala, or SQL), and a cluster can then specify that the Notebook functionality can be run against it. The following screenshot shows the Notebook creation.</p><div class="mediaobject"><img src="graphics/B01989_08_15.jpg" /></div><p>Note that a menu <a id="id510" class="indexterm"></a>option, to the right of a Notebook session, allows the type of Notebook that is to<a id="id511" class="indexterm"></a> be changed. The following example shows that a Python notebook can be changed to <span class="strong"><strong>Scala</strong></span>, <span class="strong"><strong>SQL</strong></span>, or <span class="strong"><strong>Markdown</strong></span>:</p><div class="mediaobject"><img src="graphics/B01989_08_16.jpg" /></div><p>Note that a Scala Notebook cannot be changed to Python, and a Python Notebook cannot be changed to Scala. The terms Python, Scala, and SQL are well understood as the development languages, however, <span class="strong"><strong>Markdown</strong></span> is new. Markdown<a id="id512" class="indexterm"></a> allows formatted documentation to be created from formatted commands in text. A simple reference can be found at <a class="ulink" href="https://forums.databricks.com/static/markdown/help.html" target="_blank">https://forums.databricks.com/static/markdown/help.html</a>.</p><p>This means that formatted comments can be added to the Notebook session as scripts are created. Notebooks are further subdivided into cells, which contain the commands to be executed. Cells can be moved within a Notebook by hovering over the top-left corner, and dragging them into position. New cells can be inserted into a cell list within a Notebook.</p><p>Also, using the <code class="literal">%sql</code> command, within a Scala or Python Notebook cell, allows SQL syntax to be used. Typically, the key combination of <span class="emphasis"><em>Shift</em></span> + <span class="emphasis"><em>Enter</em></span> causes text blocks in a Notebook or folder to be executed. Using the <code class="literal">%md</code> command allows Markdown comments to be <a id="id513" class="indexterm"></a>added within a cell. Also, comments can be added to a Notebook cell. The<a id="id514" class="indexterm"></a> menu options available at the top-right section of a Notebook cell, shown in the following screenshot, shows comment, as well as the minimize and maximize options:</p><div class="mediaobject"><img src="graphics/B01989_08_17.jpg" /></div><p>Multiple web-based sessions may share a Notebook. The actions that occur within the Notebook will be populated to each web interface viewing it. Also, the Markdown and comment options can be used to enable communication between users to aid the interactive data investigation between a distributed group.</p><div class="mediaobject"><img src="graphics/B01989_08_18.jpg" /></div><p>The previous screenshot shows the header of a Notebook session for <span class="strong"><strong>notebook1</strong></span>. It shows the Notebook name and type (<span class="strong"><strong>Scala</strong></span>). It also shows the option to lock the Notebook to make it read only, as well as the option to detach it from its cluster. The following screenshot shows the creation of a folder within a Notebook workspace:</p><div class="mediaobject"><img src="graphics/B01989_08_19.jpg" /></div><p>A drop-down menu, from the <a id="id515" class="indexterm"></a>
<span class="strong"><strong>Workspace</strong></span> main menu option, allows for the creation of<a id="id516" class="indexterm"></a> a folder—in this case, named <code class="literal">folder1</code>. The later sections will describe other options in this menu. Once created and selected, a drop-down menu from the new folder called <code class="literal">folder1</code> shows the actions associated with it in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_08_20.jpg" /></div><p>So, a folder can be exported to a DBC archive. It can be locked, or cloned to create a copy. It can also be renamed, or deleted. Items can be imported into it; for instance, files, which will be <a id="id517" class="indexterm"></a>explained by example later. Also, new notebooks, dashboards, libraries, and folders can be created within it.</p><p>In the same way as <a id="id518" class="indexterm"></a>actions can be carried out against a folder, a Notebook has a set of possible actions. The following screenshot shows the actions available via a drop-down menu for the Notebook called <code class="literal">notebook1</code>, which is currently attached to the running cluster called <code class="literal">semclust1</code>. It is possible to rename, delete, lock, or clone a Notebook. It is also possible to detach it from its current cluster, or attach it if it is detached. It is also possible to export the Notebook to a file, or a DBC archive.</p><div class="mediaobject"><img src="graphics/B01989_08_21.jpg" /></div><p>From the folder <span class="strong"><strong>Import</strong></span> option, files can be<a id="id519" class="indexterm"></a> imported to a folder. The following screenshot<a id="id520" class="indexterm"></a> shows the file drop-option window that is invoked if this option is selected. It is possible to either drop a file onto the upload pane from the local server, or click on this pane to open a navigation browser to search the local server for files to upload.</p><div class="mediaobject"><img src="graphics/B01989_08_22.jpg" /></div><p>Note that the files that are uploaded need to be of a specific type. The following screenshot shows the supported file types. This is a screenshot taken from the file browser when browsing for a file to upload. It also makes sense. The supported file types are Scala, SQL, and Python; as well as DBC archives and JAR file libraries.</p><div class="mediaobject"><img src="graphics/B01989_08_23.jpg" /></div><p>Before<a id="id521" class="indexterm"></a> leaving this <a id="id522" class="indexterm"></a>section, it should also be noted that Notebooks and folders can be dragged and dropped to change their position. The next section will examine Databricks jobs and libraries via simple worked examples.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec59"></a>Jobs and libraries</h2></div></div><hr /></div><p>Within Databricks, it is possible to<a id="id523" class="indexterm"></a> import JAR libraries and run the classes in them on your clusters. I will <a id="id524" class="indexterm"></a>create a very simple piece of Scala code to print out the first 100 elements of the Fibonacci series as <code class="literal">BigInt</code> values, locally on my Centos Linux server. I <a id="id525" class="indexterm"></a>will compile my class into a JAR file using SBT, run it locally to check the result, and then run it on my Databricks cluster to compare the results. The code<a id="id526" class="indexterm"></a> looks as following:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object db_ex1  extends App
{
  val appName = "Databricks example 1"
  val conf = new SparkConf()

  conf.setAppName(appName)

  val sparkCxt = new SparkContext(conf)

  var seed1:BigInt = 1
  var seed2:BigInt = 1
  val limit = 100
  var resultStr = seed1 + " " + seed2 + " "

  for( i &lt;- 1 to limit ){

    val fib:BigInt = seed1 + seed2
    resultStr += fib.toString + " "

    seed1 = seed2
    seed2 = fib
  }

  println()
  println( "Result : " + resultStr )
  println()

  sparkCxt.stop()

} // end application</pre></div><p>Not that the most <a id="id527" class="indexterm"></a>elegant piece of code, or the best way to create Fibonacci, but I just want a<a id="id528" class="indexterm"></a> sample JAR and class to use with Databricks. When run locally, I get the first 100 terms, which look as follows (I've clipped this data to save space):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Result : 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 121393 196418 317811 514229 832040 1346269 2178309 3524578 5702887 9227465 14930352 24157817 39088169 63245986 102334155 165580141 267914296 433494437 701408733 1134903170 1836311903 2971215073 4807526976 7778742049 12586269025 20365011074 32951280099 53316291173</strong></span>

<span class="strong"><strong>4660046610375530309 7540113804746346429 12200160415121876738 19740274219868223167 31940434634990099905 51680708854858323072 83621143489848422977 135301852344706746049 218922995834555169026 354224848179261915075 573147844013817084101 927372692193078999176</strong></span>
</pre></div><p>The library that has been created is called <code class="literal">data-bricks_2.10-1.0.jar</code>. From my folder menu, I can create a new Library using the menu drop-down option. This allows me to specify the library source as a JAR file, name the new library, and load the library JAR file from my local server. The following screenshot shows an example of this process:</p><div class="mediaobject"><img src="graphics/B01989_08_24.jpg" /></div><p>When the library<a id="id529" class="indexterm"></a> has been created, it can be attached to the cluster called <code class="literal">semclust1</code>, my<a id="id530" class="indexterm"></a> Databricks cluster, using the <span class="strong"><strong>Attach</strong></span> option. The following screenshot shows the new library in the process of attaching:</p><div class="mediaobject"><img src="graphics/B01989_08_25.jpg" /></div><p>In the following example, a job called <span class="strong"><strong>job2</strong></span> has been created by selecting the <span class="strong"><strong>jar</strong></span> option on the <span class="strong"><strong>Task</strong></span> item. For the job, the same JAR file has been loaded and the class<a id="id531" class="indexterm"></a> <code class="literal">db_ex1</code> has been assigned to run in the library. The cluster has been specified as on-demand, meaning that a cluster will be created automatically to run the job. The <span class="strong"><strong>Active runs</strong></span> section shows the job running in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_08_26.jpg" /></div><p>Once run, the job is <a id="id532" class="indexterm"></a>moved to the <span class="strong"><strong>Completed runs</strong></span> section of the display. The following<a id="id533" class="indexterm"></a> screenshot, for the same job, shows that it took <code class="literal">47</code> seconds to run, that it was launched manually, and that it succeeded.</p><div class="mediaobject"><img src="graphics/B01989_08_27.jpg" /></div><p>By selecting the run named <span class="strong"><strong>Run 1</strong></span> in the previous screenshot, it is possible to see the run output. The following screenshot shows the same result as the local run, displayed from my local server execution. I have clipped the output text to make it presentable and readable<a id="id534" class="indexterm"></a> on this page, but you can see that the output is the same.</p><div class="mediaobject"><img src="graphics/B01989_08_28.jpg" /></div><p>So, even from this very<a id="id535" class="indexterm"></a> simple example, it is obvious that it is possible to develop applications remotely, and load them onto a Databricks cluster as JAR files in order to execute. However, each time a Databricks cluster is created on AWS EC2 storage, the Spark URL changes, so the application must not hard-code details such as the Spark master URL. Databricks will automatically set the Spark URL.</p><p>When running the JAR file classes in this way, it is also possible to define class parameters. The jobs may be scheduled to run at a given time, or periodically. The job timeouts, and alert email addresses may also be specified.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec60"></a>Development environments</h2></div></div><hr /></div><p>It has been shown that scripts <a id="id536" class="indexterm"></a>can be created in Notebooks in Scala, Python, or SQL, but it is also possible to use an IDE such as IntelliJ or Eclipse to develop code. By installing an SBT plugin into this development environment, it is possible to develop code for your Databricks environment. The current release of Databricks, as I write this book, is 1.3.2d. The <span class="strong"><strong>Release Notes</strong></span> link, under <span class="strong"><strong>New Features</strong></span> on the start page, contains a link to the IDE integration, which is <code class="literal">https://dbc-xxxxxxx-xxxx.cloud.databricks.com/#shell/1547</code>.</p><p>The URL will be of this form, with the section starting with <code class="literal">dbc</code> changed to match the URL for the Databricks cloud that you will create. I won't expand on this here, but leave it to you to investigate. In the next section, I will investigate the Databricks table data processing functionality.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec61"></a>Databricks tables</h2></div></div><hr /></div><p>The Databricks <span class="strong"><strong>Tables</strong></span> menu option allows you store your data in a tabular form with an<a id="id537" class="indexterm"></a> associated schema. The <span class="strong"><strong>Tables</strong></span> menu option allows you to both create a table, and refresh your tables list, as the following screenshot shows:</p><div class="mediaobject"><img src="graphics/B01989_08_29.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec68"></a>Data import</h3></div></div></div><p>You can create tables via data import, and specify the table structure, at the same time, in terms of column names <a id="id538" class="indexterm"></a>and types. If the data that is being imported has a header, then the column names can be taken from that, although all the column types are assumed to be strings. The following screenshot shows a concatenated view of the data import options and form, available when creating a table. The import file location options are <span class="strong"><strong>S3</strong></span>, <span class="strong"><strong>DBFS</strong></span>, <span class="strong"><strong>JDBC</strong></span>, and <span class="strong"><strong>File</strong></span>.</p><div class="mediaobject"><img src="graphics/B01989_08_30.jpg" /></div><p>The previous screenshot shows <span class="strong"><strong>S3</strong></span> selected. In order to browse my <span class="strong"><strong>S3</strong></span> bucket for a file to import to a table, I will need to enter the <span class="strong"><strong>AWS Key ID</strong></span>, the <span class="strong"><strong>Secret Access Key</strong></span>, and the <span class="strong"><strong>AWS S3 Bucket Name</strong></span>. Then, I could browse, select the file, and create a table via preview. In the following screenshot, I have selected the <span class="strong"><strong>File</strong></span> option:</p><div class="mediaobject"><img src="graphics/B01989_08_31.jpg" /></div><p>I can either drop my file to import into the upload frame in the following screenshot, or click on the frame to<a id="id539" class="indexterm"></a> browse the local server to select a file to upload. Once a file is selected, it is then possible to define the data column delimiter, and whether the data contains a header row. It is possible to preview the data, and change the column names and data types. It is also possible to specify the new table name, and the file type. The following screenshot shows a sample file data load to create the table called <code class="literal">shuttle</code>:</p><div class="mediaobject"><img src="graphics/B01989_08_32.jpg" /></div><p>Once created, the menu table list can be refreshed and the table schema viewed to confirm the column names and types. In this way, a sample of the table data can also be previewed. The table can now be viewed and accessed from an SQL session. The following screenshot shows that the <span class="strong"><strong>shuttle</strong></span> table is visible using the <code class="literal">show tables</code> command:</p><div class="mediaobject"><img src="graphics/B01989_08_33.jpg" /></div><p>Once imported, the data in this table can also be accessed via an SQL session. The following screenshot shows <a id="id540" class="indexterm"></a>a simple SQL session statement to show the data extracted from the new <span class="strong"><strong>shuttle</strong></span> table:</p><div class="mediaobject"><img src="graphics/B01989_08_34.jpg" /></div><p>So, this provides the means to import multiple tables from a variety of data sources, and create a complex schema in order to filter and join the data by columns and rows, just as you would in a traditional, relational database. It provides a familiar approach to big data processing.</p><p>This section has<a id="id541" class="indexterm"></a> described the process by which tables can be created via data import, but what about creating tables programmatically, or creating tables as external objects? The following sections will provide examples of this approach to table management.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec69"></a>External tables</h3></div></div></div><p>Databricks allows you to<a id="id542" class="indexterm"></a> create tables against external resources, such as AWS S3 files, or local file system files. In this section, I will create an external table against an S3-based bucket, path, and a set of files. I will also examine both the permissions required in AWS and the access policy used. The following screenshot shows an AWS S3 bucket called <span class="strong"><strong>dbawss3test2</strong></span> being created. Permissions have been granted to everyone to access the list. I am not suggesting that you do this, but ensure that your group can access your bucket.</p><div class="mediaobject"><img src="graphics/B01989_08_35.jpg" /></div><p>Also, a policy has been added to aid access. In this case, anonymous users have been granted read-only access to the bucket and sub contents. You can create a more complex policy to limit the access to your group and assorted files. The following screenshot shows the new policy:</p><div class="mediaobject"><img src="graphics/B01989_08_36.jpg" /></div><p>With an access<a id="id543" class="indexterm"></a> policy, and a bucket created with the correct access policy, I can now create folders and upload files for use with a Databricks external table. As the following screenshot shows, I have done just that. The uploaded file has ten columns in CSV file format:</p><div class="mediaobject"><img src="graphics/B01989_08_37.jpg" /></div><p>Now that the AWS S3 resources have been set up, they need to be mounted to Databricks, as the Scala-based example shows next. I have removed my AWS and secret keys from the script for <a id="id544" class="indexterm"></a>security purposes. Your mounted directory will need to start with <code class="literal">/mnt</code> and any of the <code class="literal">/</code> characters, and your secret key value will need to be replaced with <code class="literal">%2F</code>. The<a id="id545" class="indexterm"></a> <code class="literal">dbutils.fs</code> class is being used to create the mount and the code executes within a second, as the following result shows:</p><div class="mediaobject"><img src="graphics/B01989_08_38.jpg" /></div><p>Now, an external table can be created against this mounted path and the files that it contains using a Notebook-based SQL session, as the following screenshot shows. The table called <code class="literal">s3test1</code> has been created against the files that the mounted directory contains, and a delimiter is specified as a comma, in order to parse the CSV-based content.</p><div class="mediaobject"><img src="graphics/B01989_08_39.jpg" /></div><p>The <span class="strong"><strong>Tables</strong></span> menu option<a id="id546" class="indexterm"></a> now shows that the <span class="strong"><strong>s3test1</strong></span> table exists, as shown in the following screenshot. So, it should be possible to run some SQL against this table:</p><div class="mediaobject"><img src="graphics/B01989_08_40.jpg" /></div><p>I have run a <code class="literal">SELECT</code> statement in an SQL-based Notebook session to get a row count from the external table, using the <code class="literal">COUNT(*)</code> function, as shown in the following screenshot. It can be see that the table contains <span class="strong"><strong>14500</strong></span> rows.</p><div class="mediaobject"><img src="graphics/B01989_08_41.jpg" /></div><p>I will now add another file to the S3-based folder. In this case, it is just a copy of the first file in CSV format, so the<a id="id547" class="indexterm"></a> row count in the external table should double. The following screenshot shows the file that is added:</p><div class="mediaobject"><img src="graphics/B01989_08_42.jpg" /></div><p>Running the same <code class="literal">SELECT</code> statement against the external table does indeed provide a doubled row count of <span class="strong"><strong>29000</strong></span> rows. The following screenshot shows the SQL statement, and the output:</p><div class="mediaobject"><img src="graphics/B01989_08_43.jpg" /></div><p>So, it is easily possible to create external tables within Databricks, and run SQL against content that is dynamically changed. The file structure will need to be uniform, and the S3 bucket access<a id="id548" class="indexterm"></a> must be defined if using AWS. The next section will examine the DbUtils package provided with Databricks.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec62"></a>The DbUtils package</h2></div></div><hr /></div><p>The previous Scala-based script, which<a id="id549" class="indexterm"></a> uses the DbUtils package, and creates the mount in the last section, only uses a small portion of the functionality of this package. In this section, I would like to introduce some more features of the DbUtils package, and the <span class="strong"><strong>Databricks File System</strong></span> (<span class="strong"><strong>DBFS</strong></span>). The help <a id="id550" class="indexterm"></a>option within the DbUtils package can be called within a Notebook connected to a Databricks cluster, to learn more about its structure and functionality. As the following screenshot shows, executing <code class="literal">dbutils.fs.help()</code> in a Scala Notebook provides help on fsutils, cache, and the mount-based functionality:</p><div class="mediaobject"><img src="graphics/B01989_08_44.jpg" /></div><p>It is also possible to obtain help on individual functions, as the text in the previous screenshot shows. The example in the following screenshot explains the <span class="strong"><strong>cacheTable</strong></span> function, providing descriptive text and a sample function call with the parameter and return types:</p><div class="mediaobject"><img src="graphics/B01989_08_45.jpg" /></div><p>The next section will briefly<a id="id551" class="indexterm"></a> examine the DBFS before moving on to examining more of the <code class="literal">dbutils</code> functionality.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec70"></a>Databricks file system</h3></div></div></div><p>The DBFS<a id="id552" class="indexterm"></a> can be accessed using URL's of the <code class="literal">dbfs:/*</code> form, and using the functions available within <code class="literal">dbutils.fs</code>.</p><div class="mediaobject"><img src="graphics/B01989_08_46.jpg" /></div><p>The previous screenshot shows the <code class="literal">/mnt</code> file system being examined using the <code class="literal">ls</code> function, and then showing mount directories—<code class="literal">s3data</code> and <code class="literal">s3data1</code>. These were the directories created during the <a id="id553" class="indexterm"></a>previous Scala S3 mount example.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec71"></a>Dbutils fsutils</h3></div></div></div><p>The <code class="literal">fsutils</code> group of functions, within the <code class="literal">dbutils</code> package, covers functions such as <code class="literal">cp</code>, <code class="literal">head</code>, <code class="literal">mkdirs</code>, <code class="literal">mv</code>, <code class="literal">put</code>, and <code class="literal">rm</code>. The help calls, shown previously, can provide more information about them. You can<a id="id554" class="indexterm"></a> create a directory on DBFS using the <code class="literal">mkdirs</code> call, as shown next. Note that I have created a number of directories under <code class="literal">dbfs:/</code>, named as <code class="literal">data*</code> in this session. The following example has created the directory called <code class="literal">data2</code>:</p><div class="mediaobject"><img src="graphics/B01989_08_47.jpg" /></div><p>The previous screenshot shows by executing an <code class="literal">ls</code> that there are many default directories that already exist on DBFS. For instance, see the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">/tmp</code> is a temporary area</p></li><li style="list-style-type: disc"><p>
<code class="literal">/mnt</code> is a mount point for remote directories—that is, S3</p></li><li style="list-style-type: disc"><p>
<code class="literal">/user</code> is a user storage area that currently contains Hive</p></li><li style="list-style-type: disc"><p>
<code class="literal">/mount</code> is an empty directory</p></li><li style="list-style-type: disc"><p>
<code class="literal">/FileStore</code> is a storage area for tables, JARs, and job JARs</p></li><li style="list-style-type: disc"><p>
<code class="literal">/databricks-datasets</code> is datasets provided by Databricks</p></li></ul></div><p>The <code class="literal">dbutils</code> copy command, shown next, allows a file to be copied to a DBFS location. In this instance, the <code class="literal">external1.txt</code> file had been copied to the <code class="literal">/data2</code> directory, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_08_48.jpg" /></div><p>The <a id="id555" class="indexterm"></a>
<code class="literal">head</code> function <a id="id556" class="indexterm"></a>can be used to return the first maxBytes characters from the head of a file on DBFS. The following example shows the format of the <code class="literal">external1.txt</code> file. This is useful, as it tells me that this is a CSV file, and so shows me how to process it.</p><div class="mediaobject"><img src="graphics/B01989_08_49.jpg" /></div><p>It is also possible to move files within DBFS. The following screenshot shows the <code class="literal">mv</code> command being used to move the <code class="literal">external1.txt</code> file from the directory <code class="literal">data2</code> to the directory called <code class="literal">data1</code>. The <code class="literal">ls</code> command is then used to confirm the move.</p><div class="mediaobject"><img src="graphics/B01989_08_50.jpg" /></div><p>Finally, the remove function<a id="id557" class="indexterm"></a> (<code class="literal">rm</code>) is used to remove the file called <code class="literal">external1.txt</code>, which was just moved. The<a id="id558" class="indexterm"></a> following <code class="literal">ls</code> function call shows that the file no longer exists within the <code class="literal">data1</code> directory, because there is no <code class="literal">FileInfo</code> record in the function output:</p><div class="mediaobject"><img src="graphics/B01989_08_51.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec72"></a>The DbUtils cache</h3></div></div></div><p>The cache functionality, within<a id="id559" class="indexterm"></a> DbUtils, provides the means to cache (and uncache) both tables and files to DBFS. Actually, the tables are saved as files also to the DBFS directory called <code class="literal">/FileStore</code>. The following screenshot shows that the cache functions are available:</p><div class="mediaobject"><img src="graphics/B01989_08_52.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec73"></a>The DbUtils mount</h3></div></div></div><p>The mount functionality allows <a id="id560" class="indexterm"></a>you to mount remote file systems, refresh mounts, display mount details, and unmount specific mounted directories. An example of an S3 mount was already given in the previous sections, so I won't repeat it here. The following screenshot shows the output from the <code class="literal">mounts</code> function. The <code class="literal">s3data</code> and <code class="literal">s3data1</code> mounts have been created by me. The other two mounts for root and datasets already existed. The mounts are listed in a sequence of the <code class="literal">MountInfo</code> objects. I have rearranged the text to be more meaningful, and to be better presented on the page.</p><div class="mediaobject"><img src="graphics/B01989_08_53.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec63"></a>Summary</h2></div></div><hr /></div><p>This chapter has introduced Databricks. It shows how the service can be accessed, and also shows how it uses AWS resources. Remember that, in the future, the people who invented Databricks plan to support other cloud-based platforms, such as Microsoft Azure. I thought that it was important to introduce Databricks, because the same people who were involved in the development of Apache Spark are involved in this system. The natural progression seems to be Hadoop, Spark, then Databricks.</p><p>I will continue the Databricks investigation in the next chapter, because important features, such as visualization, have not yet been examined. Also, the major Spark functionality modules called GraphX, streaming, MLlib, and SQL have not been introduced in Databricks terms. How easy is it to use these modules within Databricks to process real data? Read on to find out.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>Chapter 9. Databricks Visualization</h2></div></div></div><p>This chapter builds on the work done in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark Databricks</em></span>, and continues to investigate the functionality of the Apache Spark-based service at <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>. Although I will use Scala-based code examples in this chapter, I wish to concentrate on the <a id="id561" class="indexterm"></a>Databricks functionality rather than the traditional Spark processing modules: MLlib, GraphX, Streaming, and SQL. This chapter will explain the following Databricks areas:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data visualization using Dashboards</p></li><li style="list-style-type: disc"><p>An RDD-based report</p></li><li style="list-style-type: disc"><p>A Data stream-based report</p></li><li style="list-style-type: disc"><p>The Databricks Rest interface</p></li><li style="list-style-type: disc"><p>Moving data with Databricks</p></li></ul></div><p>So, this chapter will examine the functionality in Databricks to analytically visualize data via reports, and dashboards. It will also examine the REST interface, as I believe it to be a useful tool for both, remote access, and integration purposes. Finally, it will examine the options for moving data, and libraries, into a Databricks cloud instance.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec64"></a>Data visualization</h2></div></div><hr /></div><p>Databricks provides tools to access S3, and the local file system-based files. It offers the ability to import data into tables, as already shown. In the last chapter, raw data was imported into the shuttle<a id="id562" class="indexterm"></a> table to provide the table-based data that SQL could be run against, to filter against rows and columns, allow data to be sorted, and then aggregated. This is very useful, but we are still left looking at raw data output when images, and reports, present information that can be more readily, and visually, interpreted.</p><p>Databricks provides a visualization interface, based on the tabular result data that your SQL session produces. The following screenshot shows some SQL that has been run. The resulting data, and the visualization drop-down menu under the data, show the possible options.</p><div class="mediaobject"><img src="graphics/B01989_09_01.jpg" /></div><p>There is a range<a id="id563" class="indexterm"></a> of visualization options here, starting with the more familiar <span class="strong"><strong>Bar</strong></span> graphs, and <span class="strong"><strong>Pie</strong></span> charts through to <span class="strong"><strong>Quantiles</strong></span>, and <span class="strong"><strong>Box</strong></span> plots. I'm going to change my SQL so that I get more options to plot a graph, which is as follows:</p><div class="mediaobject"><img src="graphics/B01989_09_02.jpg" /></div><p>Then, having selected the visualization option; <span class="strong"><strong>Bar</strong></span> graph, I will select the <span class="strong"><strong>Plot</strong></span> options which will allow me<a id="id564" class="indexterm"></a> to choose the data for the graph vertices. It will also allow me to select a data column to pivot on. The following screenshot shows the values that I have chosen.</p><div class="mediaobject"><img src="graphics/B01989_09_03.jpg" /></div><p>The All fields section, from the <span class="strong"><strong>Plot</strong></span> options display, shows all of the fields available for the graph display from the SQL statement result data. The <span class="strong"><strong>Keys</strong></span> and <span class="strong"><strong>Values</strong></span> sections define the data fields that will form the graph axes. The <span class="strong"><strong>Series grouping</strong></span> field allows me to define a value, education, to pivot on. By selecting <span class="strong"><strong>Apply</strong></span>, I can now create a graph of total balance against a job type, grouped by the education type, as the following screenshot shows:</p><div class="mediaobject"><img src="graphics/B01989_09_04.jpg" /></div><p>If I were an accountant trying to determine the factors affecting wage costs, and groups of employees within the company<a id="id565" class="indexterm"></a> that cost the most, I would then see the green spike in the previous graph. It seems to indicate that the <span class="strong"><strong>management</strong></span> employees with a tertiary education are the most costly group within the data. This can be confirmed by changing the SQL to filter <a id="id566" class="indexterm"></a>on a <span class="strong"><strong>tertiary education</strong></span>, ordering the result by balance descending, and creating a new bar graph.</p><div class="mediaobject"><img src="graphics/B01989_09_05.jpg" /></div><p>Clearly, the <span class="strong"><strong>management</strong></span> grouping is approximately <span class="strong"><strong>14 million</strong></span>. Changing the display option to <span class="strong"><strong>Pie</strong></span> represents the <a id="id567" class="indexterm"></a>data as a pie graph, with clearly sized segments and colors, which visually and clearly present the data, and the most important items.</p><div class="mediaobject"><img src="graphics/B01989_09_06.jpg" /></div><p>I cannot examine all of the display options in this small chapter, but what I did want to show is the world <a id="id568" class="indexterm"></a>map graph that can be created using geographic information. I have downloaded the <code class="literal">Countries.zip</code> file from <a class="ulink" href="http://download.geonames.org/export/dump/" target="_blank">http://download.geonames.org/export/dump/</a>.</p><p>This will offer a sizeable data set of around 281 MB compressed, which can be used to create a new table. It is displayed as a world map graph. I have also sourced an ISO2 to ISO3 set of mapping data, and stored it in a Databricks table called <code class="literal">cmap</code>. This allows me to convert ISO2 country codes in the data above i.e “AU” to ISO3 country codes i.e “AUS” (needed by the map graph I am about to use). The first column in the data that we will use for the map graph, must contain the geo location data. In this instance, the country codes in the ISO 3 format. So from the countries data, I will create a count of records for each country by ISO3 code. It is also important to ensure that the plot options are set up correctly in terms of keys, and values. I have stored the downloaded country-based data in a table called <code class="literal">geo1</code>. The SQL used is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_09_07.jpg" /></div><p>As shown previously, this<a id="id569" class="indexterm"></a> gives two columns of data an ISO3-based value called <code class="literal">country</code>, and a numeric count called <code class="literal">value</code>. Setting the display option to <code class="literal">Map</code> creates a color-coded world map, shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B01989_09_08.jpg" /></div><p>These graphs show how data can be visually represented in various forms, but what can be done if a report is needed for external clients or a dashboard is required? <a id="id570" class="indexterm"></a>All this will be covered in the next section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec74"></a>Dashboards</h3></div></div></div><p>In this section, I will use<a id="id571" class="indexterm"></a> the data in the table called <code class="literal">geo1</code>, which was created in the last section for a map display. It was made to create a simple dashboard, and publish the dashboard to an external client. From the <span class="strong"><strong>Workspace</strong></span> menu, I have created a new dashboard called <code class="literal">dash1</code>. If I edit the controls tab of this dashboard, I can start to enter SQL, and create graphs, as shown in the following screenshot. Each graph is represented as a view and can be defined via SQL. It can be resized, and configured using the plot options as per the individual graphs. Use the <span class="strong"><strong>Add</strong></span> drop-down menu to add a view. The following screenshot shows that <code class="literal">view1</code> is already created, and added to <code class="literal">dash1</code>. <code class="literal">view2</code> is being defined.</p><div class="mediaobject"><img src="graphics/B01989_09_09.jpg" /></div><p>Once all the views have been added, positioned, and resized, the edit tab can be selected to present the finalized dashboard. The following screenshot now shows the finalized dashboard called <code class="literal">dash1</code> with three different graphs in different forms, and segments of the data:</p><div class="mediaobject"><img src="graphics/B01989_09_10.jpg" /></div><p>This is very useful for giving a view of the data, but this dashboard is within the Databricks cloud environment. What if I want a customer to see this? There is a <span class="strong"><strong>publish</strong></span> menu option in the top-right part of the dashboard screen, which allows you to publish the dashboard. This displays the dashboard under a new publicly published URL, as shown in the following screenshot. Note the new URL at the top of the following screenshot. You can now share this URL with your customers to present results. There are also options to periodically update the display to represent updates in the underlying data.</p><div class="mediaobject"><img src="graphics/B01989_09_11.jpg" /></div><p>This gives you an idea of the available display options. All of the reports, and dashboards created so far have been<a id="id572" class="indexterm"></a> based upon SQL, and the data returned. In the next section, I will show that reports can be created programmatically using a Scala-based Spark RDD, and streamed data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec75"></a>An RDD-based report</h3></div></div></div><p>The following Scala-based<a id="id573" class="indexterm"></a> example uses a user-defined class type called <code class="literal">birdType</code>, based on the bird name, and the volume encountered. An RDD is created of the bird type records, and then converted into a data frame. The data frame is then displayed. Databricks allows the displayed data to be presented as a table or using plot options as a graph. The following image shows the Scala that is used:</p><div class="mediaobject"><img src="graphics/B01989_09_12.jpg" /></div><p>The bar graph, which this<a id="id574" class="indexterm"></a> Scala example allows to be created, is shown in the following screenshot. The previous Scala code and the following screenshot are less important than the fact that this graph has been created programmatically using a data frame:</p><div class="mediaobject"><img src="graphics/B01989_09_13.jpg" /></div><p>This opens up the possibility of programmatically creating data frames, and temporary tables from calculation-based data sources. It also allows for streamed data to be processed, and the refresh functionality of dashboards to be used, to constantly present a window of streamed data. The next section will examine a stream-based example of report generation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec76"></a>A stream-based report</h3></div></div></div><p>In this section, I will use Databricks capability to upload a JAR-based library, so that we can run a Twitter-based<a id="id575" class="indexterm"></a> streaming Apache Spark example. In order to do this, I<a id="id576" class="indexterm"></a> must first create a Twitter account, and a sample application at: <a class="ulink" href="https://apps.twitter.com/" target="_blank">https://apps.twitter.com/</a>.</p><p>The following screenshot shows that I have created an application called <code class="literal">My example app</code>. This is necessary, because I need to create the necessary access keys, and tokens to create a Scala-based twitter feed.</p><div class="mediaobject"><img src="graphics/B01989_09_14.jpg" /></div><p>If I now select the application name, I can see the application details. This provides a menu option, which provides access to the application details, settings, access tokens, and permissions. There is also a button which says <span class="strong"><strong>Test OAuth</strong></span>, this enables the access and token keys that will be created to be tested. The following screenshot shows the application menu options:</p><div class="mediaobject"><img src="graphics/B01989_09_15.jpg" /></div><p>By selecting the <span class="strong"><strong>Keys and Access Tokens</strong></span> menu option, the access keys, and the access tokens can be<a id="id577" class="indexterm"></a> generated for the application. Each of the application settings and tokens, in this section, have an API key, and a secret key. The top of the form, in the following screenshot, shows the consumer key, and consumer secret (of course, the key and account details have been removed from these images for security reasons).</p><div class="mediaobject"><img src="graphics/B01989_09_16.jpg" /></div><p>There are also<a id="id578" class="indexterm"></a> options in the previous screenshot to regenerate the keys, and set permissions. The next screenshot shows the application access token details. There is an access token, and an access token secret. It also has the options to regenerate the values, and revoke access:</p><div class="mediaobject"><img src="graphics/B01989_09_17.jpg" /></div><p>Using these four<a id="id579" class="indexterm"></a> alpha numeric value strings, it is possible to write a Scala example to access a Twitter stream. The values that will be needed are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Consumer Key</p></li><li style="list-style-type: disc"><p>Consumer Secret</p></li><li style="list-style-type: disc"><p>Access Token</p></li><li style="list-style-type: disc"><p>Access Token Secret</p></li></ul></div><p>In the following code sample, I will remove my own key values for security reasons. You just need to add your own values to get the code to work. I have developed my library, and run the code locally to check whether it will work. I did this before loading it to Databricks, in order to reduce time, and costs due to debugging. My Scala code sample looks like the following code. First, I define a package, import Spark streaming, and twitter resources. Then, I define an object class called <code class="literal">twitter1</code>, and create a main function:</p><div class="informalexample"><pre class="programlisting">package nz.co.semtechsolutions

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.streaming._
import org.apache.spark.streaming.twitter._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.sql._
import org.apache.spark.sql.types.{StructType,StructField,StringType}

object twitter1 {

  def main(args: Array[String]) {</pre></div><p>Next, I create a Spark configuration object using an application name. I have not used a Spark master URL, as I<a id="id580" class="indexterm"></a> will let both, <code class="literal">spark-submit</code>, and Databricks assign the default URL. From this, I will create a Spark context, and define the Twitter consumer, and access values:</p><div class="informalexample"><pre class="programlisting">    val appName = "Twitter example 1"
    val conf    = new SparkConf()

    conf.setAppName(appName)
    val sc = new SparkContext(conf)

    val consumerKey       = "QQpl8xx"
    val consumerSecret    = "0HFzxx"
    val accessToken       = "323xx"
    val accessTokenSecret = "Ilxx"</pre></div><p>I set the Twitter access properties using the <code class="literal">System.setProperty</code> call, and use it to set the four <code class="literal">twitter4j</code> <code class="literal">oauth</code> access properties using the access keys, which were generated previously:</p><div class="informalexample"><pre class="programlisting">    System.setProperty("twitter4j.oauth.consumerKey", consumerKey)
    System.setProperty("twitter4j.oauth.consumerSecret",
       consumerSecret)
    System.setProperty("twitter4j.oauth.accessToken", accessToken)
    System.setProperty("twitter4j.oauth.accessTokenSecret",
       accessTokenSecret)</pre></div><p>A streaming context is created from the Spark context, which is used to create a Twitter-based Spark DStream. The stream is split by spaces to create words, and it gets filtered by the words starting with <code class="literal">#</code>, to select hash tags:</p><div class="informalexample"><pre class="programlisting">    val ssc    = new StreamingContext(sc, Seconds(5) )
    val stream = TwitterUtils.createStream(ssc,None)
       .window( Seconds(60) )

    // split out the hash tags from the stream

    val hashTags = stream.flatMap( status =&gt; status.getText.split(" ").filter(_.startsWith("#")))</pre></div><p>The function used below to get a singleton SQL Context is defined at the end of this example. So, for each RDD in the stream of hash tags, a single SQL context is created. This is used to import implicits which allows an RDD to be implicitly converted to a data frame using <code class="literal">toDF</code>. A <a id="id581" class="indexterm"></a>data frame is created from each <code class="literal">rdd</code> called <code class="literal">dfHashTags</code>, and this is then used to register a temporary table. I have then run some SQL against the table to get a count of rows. The count of rows is then printed. The horizontal banners in the code are just used to enable easier viewing of the output of results when using <code class="literal">spark-submit</code>:</p><div class="informalexample"><pre class="programlisting">hashTags.foreachRDD{ rdd =&gt;

val sqlContext = SQLContextSingleton.getInstance(rdd.sparkContext)
import sqlContext.implicits._

val dfHashTags = rdd.map(hashT =&gt; hashRow(hashT) ).toDF()

dfHashTags.registerTempTable("tweets")

val tweetcount = sqlContext.sql("select count(*) from tweets")

println("\n============================================")
println(  "============================================\n")

println("Count of hash tags in stream table : "
   + tweetcount.toString )

tweetcount.map(c =&gt; "Count of hash tags in stream table : "
   + c(0).toString ).collect().foreach(println)

println("\n============================================")
println(  "============================================\n")

} // for each hash tags rdd</pre></div><p>I have also output a list of the top five tweets by volume in my current tweet stream data window. You might<a id="id582" class="indexterm"></a> recognize the following code sample. It is from the Spark examples on GitHub. Again, I have used the banner to help with the results that will be seen in the output:</p><div class="informalexample"><pre class="programlisting">val topCounts60 = hashTags.map((_, 1))
   .reduceByKeyAndWindow(_ + _, Seconds(60))
.map{case (topic, count) =&gt; (count, topic)}
.transform(_.sortByKey(false))

topCounts60.foreachRDD(rdd =&gt; {

  val topList = rdd.take(5)

  println("\n===========================================")
  println(  "===========================================\n")
  println("\nPopular topics in last 60 seconds (%s total):"
     .format(rdd.count()))
  topList.foreach{case (count, tag) =&gt; println("%s (%s tweets)"
     .format(tag, count))}
  println("\n===========================================")
  println(  "==========================================\n")
})</pre></div><p>Then, I have used <code class="literal">start</code> and <code class="literal">awaitTermination</code>, via the Spark stream context <code class="literal">ssc</code>, to start the application, and keep it running until stopped:</p><div class="informalexample"><pre class="programlisting">    ssc.start()
    ssc.awaitTermination()

  } // end main
} // end twitter1</pre></div><p>Finally, I have defined the singleton SQL context function, and the <code class="literal">dataframe</code> <code class="literal">case</code> <code class="literal">class</code> for each row in the hash tag data stream <code class="literal">rdd</code>:</p><div class="informalexample"><pre class="programlisting">object SQLContextSingleton {
  @transient private var instance: SQLContext = null

  def getInstance(sparkContext: SparkContext):
    SQLContext = synchronized {
    if (instance == null) {
      instance = new SQLContext(sparkContext)
    }
    instance
  }
}
case class hashRow( hashTag: String)</pre></div><p>I compiled this Scala<a id="id583" class="indexterm"></a> application code using SBT into a JAR file called <code class="literal">data-bricks_2.10-1.0.jar</code>. My <code class="literal">SBT</code> file looks as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn twitter1]$  cat twitter.sbt</strong></span>

<span class="strong"><strong>name := "Databricks"</strong></span>
<span class="strong"><strong>version := "1.0"</strong></span>
<span class="strong"><strong>scalaVersion := "2.10.4"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "streaming" % "1.3.1" from "file:///usr/local/spark/lib/spark-assembly-1.3.1-hadoop2.3.0.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.spark" % "sql" % "1.3.1" from "file:///usr/local/spark/lib/spark-assembly-1.3.1-hadoop2.3.0.jar"</strong></span>
<span class="strong"><strong>libraryDependencies += "org.apache.spark.streaming" % "twitter" % "1.3.1" from file:///usr/local/spark/lib/spark-examples-1.3.1-hadoop2.3.0.jar</strong></span>
</pre></div><p>I downloaded the correct version of Apache Spark onto my cluster to match the current version used by Databricks at this time (1.3.1). I then installed it under <code class="literal">/usr/local/</code> on each node in my cluster, and ran it in local mode with spark as the cluster manager. My <code class="literal">spark-submit</code> script looks as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn twitter1]$ more run_twitter.bash</strong></span>
<span class="strong"><strong>#!/bin/bash</strong></span>

<span class="strong"><strong>SPARK_HOME=/usr/local/spark</strong></span>
<span class="strong"><strong>SPARK_BIN=$SPARK_HOME/bin</strong></span>
<span class="strong"><strong>SPARK_SBIN=$SPARK_HOME/sbin</strong></span>

<span class="strong"><strong>JAR_PATH=/home/hadoop/spark/twitter1/target/scala-2.10/data-bricks_2.10-1.0.jar</strong></span>
<span class="strong"><strong>CLASS_VAL=nz.co.semtechsolutions.twitter1</strong></span>

<span class="strong"><strong>TWITTER_JAR=/usr/local/spark/lib/spark-examples-1.3.1-hadoop2.3.0.jar</strong></span>

<span class="strong"><strong>cd $SPARK_BIN</strong></span>

<span class="strong"><strong>./spark-submit \</strong></span>
<span class="strong"><strong>  --class $CLASS_VAL \</strong></span>
<span class="strong"><strong>  --master spark://hc2nn.semtech-solutions.co.nz:7077  \</strong></span>
<span class="strong"><strong>  --executor-memory 100M \</strong></span>
<span class="strong"><strong>  --total-executor-cores 50 \</strong></span>
<span class="strong"><strong>  --jars $TWITTER_JAR \</strong></span>
<span class="strong"><strong>  $JAR_PATH</strong></span>
</pre></div><p>I won't go through the details, as it has been covered quite a few times, except to note that the class value is now <code class="literal">nz.co.semtechsolutions.twitter1</code>. This is the package class name, plus the<a id="id584" class="indexterm"></a> application object class name. So, when I run it locally, I get an output as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>======================================</strong></span>
<span class="strong"><strong>Count of hash tags in stream table : 707</strong></span>
<span class="strong"><strong>======================================</strong></span>
<span class="strong"><strong>Popular topics in last 60 seconds (704 total):</strong></span>
<span class="strong"><strong>#KCAMÉXICO (139 tweets)</strong></span>
<span class="strong"><strong>#BE3 (115 tweets)</strong></span>
<span class="strong"><strong>#Fallout4 (98 tweets)</strong></span>
<span class="strong"><strong>#OrianaSabatini (69 tweets)</strong></span>
<span class="strong"><strong>#MartinaStoessel (61 tweets)</strong></span>
<span class="strong"><strong>======================================</strong></span>
</pre></div><p>This tells me that the application library works. It connects to Twitter, creates a data stream, is able to filter the data into hash tags, and creates a temporary table using the data. So, having created a JAR library for Twitter data streaming, and proving that it works, I'm now able to load it onto the Databricks cloud. The following screenshot shows that a job has been created from the Databricks cloud jobs menu called <code class="literal">joblib1</code>. The <span class="strong"><strong>Set Jar</strong></span> option has been used to upload the JAR library that was just created. The full package-based name to the <code class="literal">twitter1</code> application object class has been specified.</p><div class="mediaobject"><img src="graphics/B01989_09_18.jpg" /></div><p>The following <a id="id585" class="indexterm"></a>screenshot shows the <code class="literal">joblib1</code> job, which is ready to run. A Spark-based cluster will be created on demand, as soon as the job is executed using the <span class="strong"><strong>Run Now</strong></span> option, under the <span class="strong"><strong>Active runs</strong></span> section. No scheduling options have been specified, although the job can be defined to run at a given date and time.</p><div class="mediaobject"><img src="graphics/B01989_09_19.jpg" /></div><p>I selected the <span class="strong"><strong>Run Now</strong></span> option to start the job run, as shown in the following screenshot. This shows that there is now an active run called <code class="literal">Run 1</code> for this job. It has been running for six seconds. It was<a id="id586" class="indexterm"></a> launched manually, and is pending while a on-demand cluster is created. By selecting the run name <code class="literal">Run 1</code>, I can see details about the job, especially the logged output.</p><div class="mediaobject"><img src="graphics/B01989_09_20.jpg" /></div><p>The following screenshot shows an example of the output for <code class="literal">Run 1</code> of <code class="literal">joblib1</code>. It shows the time started and duration, it also shows the running status and job details in terms of class and JAR file. It would have shown class parameters, but there were none in this case. It also shows the details of the 54 GB on-demand cluster. More importantly, it shows the list of the top five tweet hash tag values.</p><div class="mediaobject"><img src="graphics/B01989_09_21.jpg" /></div><p>The following screenshot <a id="id587" class="indexterm"></a>shows the same job run output window in the Databricks cloud instance. But this shows the output from the SQL <code class="literal">count(*)</code>, showing the number of tweet hash tags in the current data stream tweet window from the temporary table.</p><div class="mediaobject"><img src="graphics/B01989_09_22.jpg" /></div><p>So, this proves that I <a id="id588" class="indexterm"></a>can create an application library locally, using Twitter-based Apache Spark streaming, and convert the data stream into data frames, and a temporary table. It shows that I can reduce costs by developing locally, and then port my library to my Databricks cloud. I am aware that I have neither visualized the temporary table, nor the DataFrame in this example, into a Databricks graph, but time scales did not allow me to do this. Also, another thing that I would have done, if I had time, would be to checkpoint, or periodically save the stream to file, in case of application failure. However, this topic is covered in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Apache Spark Streaming</em></span> with an example, so you can take a look there if you are interested. In the next section, I will examine the Databricks REST API, which will allow better integration between your external applications, and your Databricks cloud instance.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec65"></a>REST interface</h2></div></div><hr /></div><p>Databricks provides a <a id="id589" class="indexterm"></a>REST interface for Spark cluster-based manipulation. It allows for cluster management, library management, command execution, and the execution of contexts. To be able to access the REST API, the port <code class="literal">34563</code> must be accessible for your instance in the AWS EC2-based Databricks cloud. The following Telnet command shows an attempt to access the port <code class="literal">34563</code> of my Databricks cloud instance. Note that the Telnet attempt has been successful:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[hadoop@hc2nn ~]$ telnet dbc-bff687af-08b7.cloud.databricks.com 34563</strong></span>
<span class="strong"><strong>Trying 52.6.229.109...</strong></span>
<span class="strong"><strong>Connected to dbc-bff687af-08b7.cloud.databricks.com.</strong></span>
<span class="strong"><strong>Escape character is '^]'.</strong></span>
</pre></div><p>If you do not receive a Telnet session, then contact Databricks via <code class="email">&lt;<a class="email" href="mailto:help@databricks.com">help@databricks.com</a>&gt;</code>. The next <a id="id590" class="indexterm"></a>sections provide examples of REST interface access to your instance on the Databricks cloud.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec77"></a>Configuration</h3></div></div></div><p>In order to use the<a id="id591" class="indexterm"></a> interface, I needed to whitelist the IP address that I use to access my Databricks cluster instance. This is the IP address of the machine from which I will be running the REST API commands. By whitelisting the IP addresses, Databricks can ensure that a secure list of users access each Databricks cloud instance.</p><p>I contacted Databricks support via the previous help email address, but there is also a Whitelist IP Guide, found in the <span class="strong"><strong>Workspace</strong></span> menu in your cloud instance:</p><p>
<span class="strong"><strong>Workspace</strong></span> | <span class="strong"><strong>databricks_guide</strong></span> | <span class="strong"><strong>DevOps Utilitie</strong></span>s | <span class="strong"><strong>Whitelist IP</strong></span>.</p><p>REST API calls can now be submitted to my Databricks cloud instance, from the Linux command line, using the Linux <code class="literal">curl</code> command. The example general form of the <code class="literal">curl</code> command is shown next using my Databricks cloud instance username, password, cloud instance URL, REST API path, and parameters.</p><p>The Databricks forum, and the previous help email address can be used to gain further information. The following sections will provide some REST API worked examples:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl –u  '&lt;user&gt;:&lt;paswd&gt;' &lt;dbc url&gt; -d "&lt;parameters&gt;"</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec78"></a>Cluster management</h3></div></div></div><p>You will still need to <a id="id592" class="indexterm"></a>create Databricks Spark clusters from your cloud instance user interface. The list REST API command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/api/1.0/clusters/list</strong></span>
</pre></div><p>It needs no parameters. This command will provide a list of your clusters, their status, IP addresses, names, and the port numbers that they run on. The following output shows that the cluster <code class="literal">semclust1</code> is in a pending state in the process of being created:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -u 'xxxx:yyyyy' 'https://dbc-bff687af-08b7.cloud.databricks.com:34563/api/1.0/clusters/list'</strong></span>

<span class="strong"><strong> [{"id":"0611-014057-waist9","name":"semclust1","status":"Pending","driverIp":"","jdbcPort":10000,"numWorkers":0}]</strong></span>
</pre></div><p>The same REST API command run when the cluster is available, shows that the cluster called <code class="literal">semcust1</code> is running, and has one worker:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[{"id":"0611-014057-waist9","name":"semclust1","status":"Running","driverIp":"10.0.196.161","jdbcPort":10000,"numWorkers":1}]</strong></span>
</pre></div><p>Terminating this <a id="id593" class="indexterm"></a>cluster, and creating a new one called <code class="literal">semclust</code> changes the results of the REST API call as shown:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -u 'xxxx:yyyy' 'https://dbc-bff687af-08b7.cloud.databricks.com:34563/api/1.0/clusters/list'</strong></span>

<span class="strong"><strong>[{"id":"0611-023105-moms10","name":"semclust", "status":"Pending","driverIp":"","jdbcPort":10000,"numWorkers":0},</strong></span>
<span class="strong"><strong> {"id":"0611-014057-waist9","name":"semclust1","status":"Terminated","driverIp":"10.0.196.161","jdbcPort":10000,"numWorkers":1}]</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec79"></a>The execution context</h3></div></div></div><p>With these API calls, you <a id="id594" class="indexterm"></a>can create, show the status of, or delete an execution context. The REST API calls are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">/api/1.0/contexts/create</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">/api/1.0/contexts/status</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">/api/1.0/contexts/destroy</code>
</p></li></ul></div><p>In the following REST API call example, submitted via <code class="literal">curl</code>, a Scala context has been created for the cluster <code class="literal">semclust</code> identified by it's cluster ID.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -u 'xxxx:yyyy' https://dbc-bff687af-08b7.cloud.databricks.com:34563/api/1.0/contexts/create -d "language=scala&amp;clusterId=0611-023105-moms10"</strong></span>
</pre></div><p>The result returned is either an error, or a context ID. The following three example return values show an error caused by an invalid URL, and two successful calls returning context IDs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>{"error":"ClusterNotFoundException: Cluster not found: semclust1"}</strong></span>
<span class="strong"><strong>{"id":"8689178710930730361"}</strong></span>
<span class="strong"><strong>{"id":"2876384417314129043"}</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec80"></a>Command execution</h3></div></div></div><p>These commands allow<a id="id595" class="indexterm"></a> you to run a command, list a command status, cancel a command, or show the results of a command. The REST API calls are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>/api/1.0/commands/execute</p></li><li style="list-style-type: disc"><p>/api/1.0/commands/cancel</p></li><li style="list-style-type: disc"><p>/api/1.0/commands/status</p></li></ul></div><p>The following example shows an SQL statement being run against an existing table called <code class="literal">cmap</code>. The context must exist, and must be of the SQL type. The parameters have been passed on to the HTTP GET call via a <code class="literal">–d</code> option. The parameters are language, the cluster ID, the context ID, and the SQL command. The command ID is returned as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -u 'admin:FirmWare1$34' https://dbc-bff687af-08b7.cloud.databricks.com:34563/api/1.0/commands/execute -d</strong></span>
<span class="strong"><strong>"language=sql&amp;clusterId=0611-023105-moms10&amp;contextId=7690632266172649068&amp;command=select count(*) from cmap"</strong></span>

<span class="strong"><strong>{"id":"d8ec4989557d4a4ea271d991a603a3af"}</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec81"></a>Libraries</h3></div></div></div><p>The REST API also allows for libraries to be uploaded to a cluster and their statuses checked. The <a id="id596" class="indexterm"></a>REST API call paths are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">/api/1.0/libraries/upload</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">/api/1.0/libraries/list</code>
</p></li></ul></div><p>An example is given next of a library upload to the cluster instance called <code class="literal">semclust</code>. The parameters passed on to the HTTP GET API call via a <code class="literal">–d</code> option are the language, cluster ID, the library name and URI. A successful call results in the name and URI of the library, which is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -u 'xxxx:yyyy' https://dbc-bff687af-08b7.cloud.databricks.com:34563/api/1.0/libraries/upload</strong></span>
<span class="strong"><strong> -d "language=scala&amp;clusterId=0611-023105-moms10&amp;name=lib1&amp;uri=file:///home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar"</strong></span>

<span class="strong"><strong>{"name":"lib1","uri":"file:///home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar"}</strong></span>
</pre></div><p>Note that this REST API can change by content and version overtime, so check in the Databricks forum, and use the previous help email address to check the API details with Databricks support. I do<a id="id597" class="indexterm"></a> think though that, with these simple example calls, it is clear that this REST API can be used to integrate Databricks with the external systems, and ETL chains. In the next section, I will provide an overview of data movement within the Databricks cloud.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec66"></a>Moving data</h2></div></div><hr /></div><p>Some of the methods of moving data in and out of Databricks have already been explained in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark Databricks</em></span> and <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Databricks Visualization</em></span>. What I would<a id="id598" class="indexterm"></a> like to do in this section is provide an overview of all of the methods available for moving data. I will examine the options for tables, workspaces, jobs, and Spark code.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec82"></a>The table data</h3></div></div></div><p>The table import functionality<a id="id599" class="indexterm"></a> for Databricks cloud allows data to be imported from an AWS <span class="strong"><strong>S3</strong></span> bucket, from the <a id="id600" class="indexterm"></a>
<span class="strong"><strong>Databricks file system</strong></span> (<span class="strong"><strong>DBFS</strong></span>), via JDBC and finally from a local file. This section gives an overview of each type of import, starting with <span class="strong"><strong>S3</strong></span>. Importing the table data from AWS <span class="strong"><strong>S3</strong></span> requires the AWS Key, the AWS secret key, and the <span class="strong"><strong>S3</strong></span> bucket name. The following screenshot shows an example. I have already provided an example of <span class="strong"><strong>S3</strong></span> bucket creation, including adding an access policy, so I will not cover it again.</p><div class="mediaobject"><img src="graphics/B01989_09_23.jpg" /></div><p>Once the form<a id="id601" class="indexterm"></a> details are added, you will be able to browse your <span class="strong"><strong>S3</strong></span> bucket for a data source. Selecting <code class="literal">DBFS</code> as a table data source enables your <code class="literal">DBFS</code> folders, and files to be browsed. Once a data source is selected, it can display a preview as the following screenshot shows:</p><div class="mediaobject"><img src="graphics/B01989_09_24.jpg" /></div><p>Selecting <code class="literal">JDBC</code> as <a id="id602" class="indexterm"></a>a table data source allows you to specify a remote SQL database as a data source. Just add an access <span class="strong"><strong>URL</strong></span>, <span class="strong"><strong>Username</strong></span>, and <span class="strong"><strong>Password</strong></span>. Also, add some SQL to define the table, and columns to source. There is also an option of adding extra properties to the call via the <span class="strong"><strong>Add Property</strong></span> button, as the following screenshot shows:</p><div class="mediaobject"><img src="graphics/B01989_09_25.jpg" /></div><p>Selecting the <span class="strong"><strong>File</strong></span> option to populate a Databricks cloud instance table, from a file, creates a drop down or browse. This upload method was used <a id="id603" class="indexterm"></a>previously to upload CSV-based data into a table. Once the data source is specified, it is possible to specify a data separator string or header row, define column names or column types and preview the data before creating the table.</p><div class="mediaobject"><img src="graphics/B01989_09_26.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec83"></a>Folder import</h3></div></div></div><p>From either a workspace, or a folder drop-down menu, it is possible to import an item. The following <a id="id604" class="indexterm"></a>screenshot shows a compound image from the <span class="strong"><strong>Import Item</strong></span> menu option:</p><div class="mediaobject"><img src="graphics/B01989_09_27.jpg" /></div><p>This creates a file drop or browse window, which when clicked, allows you to browse the local server for the items to import. Selecting the <code class="literal">All Supported Types</code> option shows that the items to import can be JAR files, dbc archives, Scala, Python, or SQL files.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec84"></a>Library import</h3></div></div></div><p>The following screenshot<a id="id605" class="indexterm"></a> shows the <span class="strong"><strong>New Library</strong></span> functionality, from the Workspace and folder menu options. This allows an externally created and tested library to be loaded to your Databricks cloud instance. The library can be in the form of a Java or Scala JAR file, a Python Egg or a Maven coordinate for repository access. In the following screenshot, a JAR file is being selected from the local server via a browse window. This functionality has been used in this chapter to test stream-based Scala<a id="id606" class="indexterm"></a> programming:</p><div class="mediaobject"><img src="graphics/B01989_09_28.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec67"></a>Further reading</h2></div></div><hr /></div><p>Before summing up this chapter, and the last for cloud-based Apache Spark usage in Databricks, I wanted to mention some<a id="id607" class="indexterm"></a> resources for gaining extra information on both, Apache Spark, and Databricks. First, there is the Databricks forum <a id="id608" class="indexterm"></a>available at: <a class="ulink" href="http://forums.databricks.com/" target="_blank">forums.databricks.com/</a> for questions, and answers related to the use of <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>. Also, within your Databricks instance, under the Workspace menu option, there will be a Databricks guide that contains a lot of useful information. The Apache Spark website at <a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a> also contains <a id="id609" class="indexterm"></a>a lot of useful information, as well as module-based API documentation. Finally, there is the Spark mailing list, <code class="email">&lt;<a class="email" href="mailto:user@spark.apache.org">user@spark.apache.org</a>&gt;</code>, which provides a great deal of Spark usage information, and problem solving.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec68"></a>Summary</h2></div></div><hr /></div><p>
<a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark Databricks</em></span> and <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Databricks Visualization</em></span>, have provided an introduction to Databricks in terms of cloud installation, and the use of Notebooks and folders. Account and cluster management have been examined. Also, job creation, the idea of remote library creation, and importing have been examined. The functionality of the Databricks <code class="literal">dbutils</code> package, and the Databricks file system was explained in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark Databricks</em></span>. Tables, and an example of data import was also shown so that SQL can be run against a dataset.</p><p>The idea of data visualization has been examined, and a variety of graphs have also been created. Dashboards have been made to show how easy it is to both, create, and share this kind of data presentation. The Databricks REST interface has been shown via worked examples, as an aid to using a Databricks cloud instance remotely, and integrating it with external systems. Finally, the data and library movement options have been examined in terms of workspace, folders, and tables.</p><p>You might ask why I have committed two chapters to a cloud-based service such as Databricks. The reason is that Databricks seems to be a logical, cloud-based progression, from Apache Spark. It is supported by the people who originally developed Apache Spark and although in it's infancy as a service and subject to change still capable of providing a Spark cloud based production service. This means that a company wishing to use a Spark could use Databricks and grow their cloud as demand grows and have access to dynamic Spark-based machine learning, graph processing, SQL, streaming and visualization functionality.</p><p>As ever, these Databricks chapters have just scratched the surface of the functionality available. The next step will be to create an AWS and Databricks account yourself, and use the information provided here to gain practical experience.</p><p>As this is the last chapter, I will provide my contact details again. I would be interested in the ways that people are using Apache Spark. I would be interested in the size of clusters you are creating, and the data that you are processing. Are you using Spark as a processing engine? Or are you building systems on top of it? You can connect with me at LinkedIn at: <a class="ulink" href="http://linkedin.com/profile/view?id=73219349" target="_blank">linkedin.com/profile/view?id=73219349</a>.</p><p>You can contact me via my website at <code class="literal">semtech-solutions.co.nz</code> or finally, by email at: <code class="email">&lt;<a class="email" href="mailto:info@semtech-solutions.co.nz">info@semtech-solutions.co.nz</a>&gt;</code>.</p><p>Finally, I maintain a list of open-source-software-related presentations when I have the time. Anyone is free to use, and download them. They are available on SlideShare at: <a class="ulink" href="http://www.slideshare.net/mikejf12/presentations" target="_blank">http://www.slideshare.net/mikejf12/presentations</a>.</p><p>If you have any challenging opportunities or problems, please feel free to contact me using the previous details.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>AAN<ul><li>about / <a href="#ch02lvl1sec16" title="K-Means in practice" class="link">K-Means in practice</a>, <a href="#ch02lvl1sec17" title="ANN – Artificial Neural Networks" class="link">ANN – Artificial Neural Networks</a></li><li>theory / <a href="#ch02lvl1sec17" title="Theory" class="link">Theory</a></li><li>Spark server, sparkling / <a href="#ch02lvl1sec17" title="Building the Spark server" class="link">Building the Spark server</a></li><li>using / <a href="#ch02lvl1sec17" title="ANN in practice" class="link">ANN in practice</a></li></ul></li>
        <li>account management, Databricks<ul><li>about / <a href="#ch08lvl1sec56" title="Account management" class="link">Account management</a></li></ul></li>
        <li>Amazon AWS<ul><li>URL / <a href="#ch01lvl1sec10" title="Amazon EC2" class="link">Amazon EC2</a></li><li>pricing, URL / <a href="#ch01lvl1sec10" title="Amazon EC2" class="link">Amazon EC2</a></li></ul></li>
        <li>Amazon EC2<ul><li>about / <a href="#ch01lvl1sec10" title="Amazon EC2" class="link">Amazon EC2</a></li><li>URL / <a href="#ch01lvl1sec10" title="Amazon EC2" class="link">Amazon EC2</a></li></ul></li>
        <li>Amazon Elastic Compute Cloud (EC2) / <a href="#ch08lvl1sec53" title="Installing Databricks" class="link">Installing Databricks</a></li>
        <li>Apache Giraph / <a href="#ch05lvl1sec30" title="Overview" class="link">Overview</a></li>
        <li>Apache Kafka<ul><li>URL / <a href="#ch03lvl1sec21" title="Kafka" class="link">Kafka</a></li><li>about / <a href="#ch03lvl1sec21" title="Kafka" class="link">Kafka</a></li><li>JAR library file, URL / <a href="#ch03lvl1sec21" title="Kafka" class="link">Kafka</a></li></ul></li>
        <li>Apache Mesos / <a href="#ch01lvl1sec10" title="Apache Mesos" class="link">Apache Mesos</a></li>
        <li>Apache Spark<ul><li>overview / <a href="#ch01lvl1sec08" title="Overview" class="link">Overview</a></li><li>URL / <a href="#ch01lvl1sec08" title="Overview" class="link">Overview</a>, <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a>, <a href="#ch09lvl1sec67" title="Further reading" class="link">Further reading</a></li><li>Spark Machine Learning / <a href="#ch01lvl1sec08" title="Spark Machine Learning" class="link">Spark Machine Learning</a></li><li>stream processing / <a href="#ch01lvl1sec08" title="Spark Streaming" class="link">Spark Streaming</a></li><li>SQL module / <a href="#ch01lvl1sec08" title="Spark SQL" class="link">Spark SQL</a></li><li>graph processing / <a href="#ch01lvl1sec08" title="Spark graph processing" class="link">Spark graph processing</a></li><li>extended eco system / <a href="#ch01lvl1sec08" title="Extended ecosystem" class="link">Extended ecosystem</a></li><li>future / <a href="#ch01lvl1sec08" title="The future of Spark" class="link">The future of Spark</a></li><li>cluster design / <a href="#ch01lvl1sec09" title="Cluster design" class="link">Cluster design</a></li><li>cluster management / <a href="#ch01lvl1sec10" title="Cluster management" class="link">Cluster management</a></li><li>performance, examining / <a href="#ch01lvl1sec11" title="Performance" class="link">Performance</a></li><li>SQL context / <a href="#ch04lvl1sec23" title="The SQL context" class="link">The SQL context</a></li><li>used, for accessing HBase / <a href="#ch06lvl1sec37" title="Accessing HBase with Spark" class="link">Accessing HBase with Spark</a></li><li>used, for accessing Cassandra / <a href="#ch06lvl1sec38" title="Accessing Cassandra with Spark" class="link">Accessing Cassandra with Spark</a></li><li>Titan, accessing with / <a href="#ch06lvl1sec39" title="Accessing Titan with Spark" class="link">Accessing Titan with Spark</a></li></ul></li>
        <li>Apache Spark streaming<ul><li>overview / <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a></li><li>URL / <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a></li><li>errors / <a href="#ch03lvl1sec20" title="Errors and recovery" class="link">Errors and recovery</a></li><li>recovery / <a href="#ch03lvl1sec20" title="Errors and recovery" class="link">Errors and recovery</a></li><li>HDFS-based checkpoint, setting up / <a href="#ch03lvl1sec20" title="Checkpointing" class="link">Checkpointing</a></li><li>data sources / <a href="#ch03lvl1sec21" title="Streaming sources" class="link">Streaming sources</a></li></ul></li>
        <li>Apache YARN / <a href="#ch01lvl1sec10" title="Apache YARN" class="link">Apache YARN</a></li>
        <li>architecture, H2O / <a href="#ch07lvl1sec45" title="Architecture" class="link">Architecture</a></li>
        <li>Artificial Neural Net (ANN) / <a href="#ch07lvl1sec46" title="Sourcing the data" class="link">Sourcing the data</a></li>
        <li>AWS<ul><li>URL / <a href="#ch08lvl1sec53" title="Installing Databricks" class="link">Installing Databricks</a></li></ul></li>
        <li>AWS billing / <a href="#ch08lvl1sec54" title="AWS billing" class="link">AWS billing</a></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>BaseConfiguration method / <a href="#ch06lvl1sec39" title="Alternative Groovy configuration" class="link">Alternative Groovy configuration</a></li>
        <li>Bruce Penn<ul><li>URL / <a href="#ch01lvl1sec11" title="The Hadoop file system" class="link">The Hadoop file system</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>Cassandra<ul><li>Titan, accessing with / <a href="#ch06lvl1sec38" title="Titan with Cassandra" class="link">Titan with Cassandra</a></li><li>installing / <a href="#ch06lvl1sec38" title="Installing Cassandra" class="link">Installing Cassandra</a></li><li>accessing, with Apache Spark / <a href="#ch06lvl1sec38" title="Accessing Cassandra with Spark" class="link">Accessing Cassandra with Spark</a></li></ul></li>
        <li>classifications, with Naïve Bayes<ul><li>about / <a href="#ch02lvl1sec15" title="Classification with Naïve Bayes" class="link">Classification with Naïve Bayes</a>, <a href="#ch02lvl1sec15" title="Naïve Bayes in practice" class="link">Naïve Bayes in practice</a></li><li>theory / <a href="#ch02lvl1sec15" title="Theory" class="link">Theory</a></li></ul></li>
        <li>closeness centrality algorithm<ul><li>about / <a href="#ch05lvl1sec32" title="The closeness centrality algorithm" class="link">The closeness centrality algorithm</a></li></ul></li>
        <li>Cloudera<ul><li>URL / <a href="#ch04lvl1sec28" title="Local Hive Metastore server" class="link">Local Hive Metastore server</a></li></ul></li>
        <li>cluster design, Apache Spark / <a href="#ch01lvl1sec09" title="Cluster design" class="link">Cluster design</a></li>
        <li>clustering, with K-Means<ul><li>about / <a href="#ch02lvl1sec16" title="Clustering with K-Means" class="link">Clustering with K-Means</a></li><li>theory / <a href="#ch02lvl1sec16" title="Theory" class="link">Theory</a></li></ul></li>
        <li>cluster management<ul><li>about / <a href="#ch01lvl1sec10" title="Cluster management" class="link">Cluster management</a></li><li>local mode / <a href="#ch01lvl1sec10" title="Local" class="link">Local</a></li><li>standalone mode / <a href="#ch01lvl1sec10" title="Standalone" class="link">Standalone</a></li><li>Apache YARN / <a href="#ch01lvl1sec10" title="Apache YARN" class="link">Apache YARN</a></li><li>Apache Mesos / <a href="#ch01lvl1sec10" title="Apache Mesos" class="link">Apache Mesos</a></li><li>Amazon EC2 / <a href="#ch01lvl1sec10" title="Amazon EC2" class="link">Amazon EC2</a></li></ul></li>
        <li>cluster management, Databricks<ul><li>about / <a href="#ch08lvl1sec57" title="Cluster management" class="link">Cluster management</a></li></ul></li>
        <li>connected components algorithm<ul><li>about / <a href="#ch05lvl1sec32" title="The connected components algorithm" class="link">The connected components algorithm</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>dashboards / <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a></li>
        <li>data<ul><li>importing / <a href="#ch04lvl1sec24" title="Importing and saving data" class="link">Importing and saving data</a></li><li>saving / <a href="#ch04lvl1sec24" title="Importing and saving data" class="link">Importing and saving data</a></li><li>text files, processing / <a href="#ch04lvl1sec24" title="Processing the Text files" class="link">Processing the Text files</a></li><li>JSON files, processing / <a href="#ch04lvl1sec24" title="Processing the JSON files" class="link">Processing the JSON files</a></li><li>Parquet files, processing / <a href="#ch04lvl1sec24" title="Processing the Parquet files" class="link">Processing the Parquet files</a></li><li>sourcing / <a href="#ch07lvl1sec46" title="Sourcing the data" class="link">Sourcing the data</a></li><li>quality / <a href="#ch07lvl1sec47" title="Data Quality" class="link">Data Quality</a></li><li>moving / <a href="#ch09lvl1sec66" title="Moving data" class="link">Moving data</a></li><li>table data, importing / <a href="#ch09lvl1sec66" title="The table data" class="link">The table data</a></li><li>folder, importing / <a href="#ch09lvl1sec66" title="Folder import" class="link">Folder import</a></li><li>library, importing / <a href="#ch09lvl1sec66" title="Library import" class="link">Library import</a></li></ul></li>
        <li>databases / <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a></li>
        <li>Databricks<ul><li>URL / <a href="#ch01lvl1sec08" title="The future of Spark" class="link">The future of Spark</a>, <a href="#ch01lvl1sec10" title="Amazon EC2" class="link">Amazon EC2</a>, <a href="#ch01lvl1sec12" title="Cloud" class="link">Cloud</a>, <a href="#ch08lvl1sec52" title="Overview" class="link">Overview</a>, <a href="#ch09lvl1sec67" title="Further reading" class="link">Further reading</a></li><li>overview / <a href="#ch08lvl1sec52" title="Overview" class="link">Overview</a></li><li>installing / <a href="#ch08lvl1sec53" title="Installing Databricks" class="link">Installing Databricks</a></li><li>AWS billing / <a href="#ch08lvl1sec54" title="AWS billing" class="link">AWS billing</a></li><li>menu / <a href="#ch08lvl1sec55" title="Databricks menus" class="link">Databricks menus</a></li><li>account management / <a href="#ch08lvl1sec56" title="Account management" class="link">Account management</a></li><li>cluster management / <a href="#ch08lvl1sec57" title="Cluster management" class="link">Cluster management</a></li><li>Notebooks / <a href="#ch08lvl1sec58" title="Notebooks and folders" class="link">Notebooks and folders</a></li><li>folder / <a href="#ch08lvl1sec58" title="Notebooks and folders" class="link">Notebooks and folders</a></li><li>jobs / <a href="#ch08lvl1sec59" title="Jobs and libraries" class="link">Jobs and libraries</a></li><li>libraries / <a href="#ch08lvl1sec59" title="Jobs and libraries" class="link">Jobs and libraries</a></li><li>references / <a href="#ch09lvl1sec67" title="Further reading" class="link">Further reading</a></li></ul></li>
        <li>Databricks file system (DBFS) / <a href="#ch09lvl1sec66" title="The table data" class="link">The table data</a></li>
        <li>Databricks tables<ul><li>about / <a href="#ch08lvl1sec61" title="Databricks tables" class="link">Databricks tables</a></li><li>creating, via data import / <a href="#ch08lvl1sec61" title="Data import" class="link">Data import</a></li><li>external tables / <a href="#ch08lvl1sec61" title="External tables" class="link">External tables</a></li></ul></li>
        <li>DataFrames<ul><li>about / <a href="#ch04lvl1sec25" title="DataFrames" class="link">DataFrames</a></li></ul></li>
        <li>data sources, Apache Spark streaming<ul><li>Kafka / <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a></li><li>Flume / <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a>, <a href="#ch03lvl1sec21" title="Flume" class="link">Flume</a></li><li>HDFS / <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a></li><li>about / <a href="#ch03lvl1sec21" title="Streaming sources" class="link">Streaming sources</a></li><li>TCP stream / <a href="#ch03lvl1sec21" title="TCP stream" class="link">TCP stream</a></li><li>file streams / <a href="#ch03lvl1sec21" title="File streams" class="link">File streams</a></li><li>Apache Kafka / <a href="#ch03lvl1sec21" title="Kafka" class="link">Kafka</a></li></ul></li>
        <li>DataStax Spark Cassandra connector<ul><li>URL / <a href="#ch06lvl1sec38" title="The Spark Cassandra connector" class="link">The Spark Cassandra connector</a></li></ul></li>
        <li>data visualization<ul><li>about / <a href="#ch09lvl1sec64" title="Data visualization" class="link">Data visualization</a></li><li>dashboards / <a href="#ch09lvl1sec64" title="Dashboards" class="link">Dashboards</a></li><li>RDD-based report / <a href="#ch09lvl1sec64" title="An RDD-based report" class="link">An RDD-based report</a></li><li>stream-based report / <a href="#ch09lvl1sec64" title="A stream-based report" class="link">A stream-based report</a></li></ul></li>
        <li>DBFS<ul><li>accessing / <a href="#ch08lvl1sec62" title="Databricks file system" class="link">Databricks file system</a></li></ul></li>
        <li>dbutils.fs class<ul><li>about / <a href="#ch08lvl1sec61" title="External tables" class="link">External tables</a></li></ul></li>
        <li>dbutils package<ul><li>about / <a href="#ch08lvl1sec62" title="The DbUtils package" class="link">The DbUtils package</a></li><li>DBFS / <a href="#ch08lvl1sec62" title="The DbUtils package" class="link">The DbUtils package</a></li><li>fsutils group / <a href="#ch08lvl1sec62" title="Dbutils fsutils" class="link">Dbutils fsutils</a></li><li>cache functionality / <a href="#ch08lvl1sec62" title="The DbUtils cache" class="link">The DbUtils cache</a></li><li>mount functionality / <a href="#ch08lvl1sec62" title="The DbUtils mount" class="link">The DbUtils mount</a></li></ul></li>
        <li>deep learning<ul><li>about / <a href="#ch07lvl1sec49" title="Deep learning" class="link">Deep learning</a></li><li>URL / <a href="#ch07lvl1sec49" title="Deep learning" class="link">Deep learning</a></li><li>Scala-based H2O Sparkling Water example / <a href="#ch07lvl1sec49" title="Example code – income" class="link">Example code – income</a></li><li>MNIST / <a href="#ch07lvl1sec49" title="The example code – MNIST" class="link">The example code – MNIST</a></li></ul></li>
        <li>development environments, Databricks<ul><li>about / <a href="#ch08lvl1sec60" title="Development environments" class="link">Development environments</a></li></ul></li>
        <li>discrete stream (DStream) / <a href="#ch03lvl1sec19" title="Overview" class="link">Overview</a></li>
        <li>Docker<ul><li>URL / <a href="#ch05lvl1sec32" title="Installing Docker" class="link">Installing Docker</a></li><li>installing / <a href="#ch05lvl1sec32" title="Installing Docker" class="link">Installing Docker</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>end of file markers (EOF) / <a href="#ch06lvl1sec39" title="Using Cassandra" class="link">Using Cassandra</a></li>
        <li>environment, H2O<ul><li>processing / <a href="#ch07lvl1sec42" title="The processing environment" class="link">The processing environment</a></li></ul></li>
        <li>environment configuration, MLlib<ul><li>architecture / <a href="#ch02lvl1sec14" title="Architecture" class="link">Architecture</a></li><li>development environment / <a href="#ch02lvl1sec14" title="The development environment" class="link">The development environment</a></li><li>Spark, installing / <a href="#ch02lvl1sec14" title="Installing Spark" class="link">Installing Spark</a></li></ul></li>
        <li>Extract, Transform, Load (ETL)<ul><li>about / <a href="#ch02lvl1sec14" title="Architecture" class="link">Architecture</a></li></ul></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>False Positive Rate (FPR) / <a href="#ch07lvl1sec50" title="H2O Flow" class="link">H2O Flow</a></li>
        <li>Flume<ul><li>about / <a href="#ch03lvl1sec21" title="Flume" class="link">Flume</a></li><li>URL / <a href="#ch03lvl1sec21" title="Flume" class="link">Flume</a></li></ul></li>
        <li>folder / <a href="#ch08lvl1sec58" title="Notebooks and folders" class="link">Notebooks and folders</a></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>graph, creating<ul><li>counting example / <a href="#ch05lvl1sec31" title="Example 1 – counting" class="link">Example 1 – counting</a></li><li>filtering example / <a href="#ch05lvl1sec31" title="Example 2 – filtering" class="link">Example 2 – filtering</a></li><li>PageRank algorithm / <a href="#ch05lvl1sec31" title="Example 3 – PageRank" class="link">Example 3 – PageRank</a></li><li>triangle counting / <a href="#ch05lvl1sec31" title="Example 4 – triangle counting" class="link">Example 4 – triangle counting</a></li><li>connected components / <a href="#ch05lvl1sec31" title="Example 5 – connected components" class="link">Example 5 – connected components</a></li></ul></li>
        <li>GraphInputFormat class / <a href="#ch06lvl1sec39" title="Using HBase" class="link">Using HBase</a></li>
        <li>graph processing, Apache Spark / <a href="#ch01lvl1sec08" title="Spark graph processing" class="link">Spark graph processing</a></li>
        <li>GraphX<ul><li>overview / <a href="#ch05lvl1sec30" title="Overview" class="link">Overview</a></li><li>coding / <a href="#ch05lvl1sec31" title="GraphX coding" class="link">GraphX coding</a></li></ul></li>
        <li>GraphX coding<ul><li>about / <a href="#ch05lvl1sec31" title="GraphX coding" class="link">GraphX coding</a></li><li>environment / <a href="#ch05lvl1sec31" title="Environment" class="link">Environment</a></li><li>graph, creating / <a href="#ch05lvl1sec31" title="Creating a graph" class="link">Creating a graph</a></li></ul></li>
        <li>Gremlin language / <a href="#ch06lvl1sec35" title="TinkerPop" class="link">TinkerPop</a></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>H2O<ul><li>overview / <a href="#ch07lvl1sec41" title="Overview" class="link">Overview</a></li><li>environment, processing / <a href="#ch07lvl1sec42" title="The processing environment" class="link">The processing environment</a></li><li>system versions, URL / <a href="#ch07lvl1sec42" title="The processing environment" class="link">The processing environment</a></li><li>installing / <a href="#ch07lvl1sec43" title="Installing H2O" class="link">Installing H2O</a></li><li>Sparkling Water download option, URL / <a href="#ch07lvl1sec43" title="Installing H2O" class="link">Installing H2O</a></li><li>build environment / <a href="#ch07lvl1sec44" title="The build environment" class="link">The build environment</a></li><li>architecture / <a href="#ch07lvl1sec45" title="Architecture" class="link">Architecture</a></li><li>URL / <a href="#ch07lvl1sec45" title="Architecture" class="link">Architecture</a></li><li>performance tuning / <a href="#ch07lvl1sec48" title="Performance tuning" class="link">Performance tuning</a></li></ul></li>
        <li>H2O flow<ul><li>about / <a href="#ch07lvl1sec50" title="H2O Flow" class="link">H2O Flow</a></li><li>URL / <a href="#ch07lvl1sec50" title="H2O Flow" class="link">H2O Flow</a></li></ul></li>
        <li>hadoop / <a href="#ch02lvl1sec14" title="The development environment" class="link">The development environment</a></li>
        <li>Hadoop file system / <a href="#ch01lvl1sec11" title="The Hadoop file system" class="link">The Hadoop file system</a></li>
        <li>Hadoop Gremlin<ul><li>URL / <a href="#ch06lvl1sec39" title="TinkerPop's Hadoop Gremlin" class="link">TinkerPop's Hadoop Gremlin</a></li></ul></li>
        <li>HBase<ul><li>Titan, accessing with / <a href="#ch06lvl1sec37" title="Titan with HBase" class="link">Titan with HBase</a></li><li>accessing, with Apache Spark / <a href="#ch06lvl1sec37" title="Accessing HBase with Spark" class="link">Accessing HBase with Spark</a></li></ul></li>
        <li>head function / <a href="#ch08lvl1sec62" title="Dbutils fsutils" class="link">Dbutils fsutils</a></li>
        <li>Hernan Amiune<ul><li>URL / <a href="#ch02lvl1sec15" title="Theory" class="link">Theory</a></li></ul></li>
        <li>Hive<ul><li>using / <a href="#ch04lvl1sec28" title="Using Hive" class="link">Using Hive</a></li><li>local Metastore server / <a href="#ch04lvl1sec28" title="Local Hive Metastore server" class="link">Local Hive Metastore server</a></li><li>-based Metastore server / <a href="#ch04lvl1sec28" title="A Hive-based Metastore server" class="link">A Hive-based Metastore server</a></li></ul></li>
        <li>Hive-based Metastore server<ul><li>using / <a href="#ch04lvl1sec28" title="A Hive-based Metastore server" class="link">A Hive-based Metastore server</a></li></ul></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>JavaScript Object Notation (JSON) files<ul><li>processing / <a href="#ch04lvl1sec24" title="Processing the JSON files" class="link">Processing the JSON files</a></li></ul></li>
        <li>jobs<ul><li>about / <a href="#ch08lvl1sec59" title="Jobs and libraries" class="link">Jobs and libraries</a></li></ul></li>
      </ul>
      <h2>K</h2>
      <ul>
        <li>K-Means<ul><li>clustering / <a href="#ch02lvl1sec16" title="Clustering with K-Means" class="link">Clustering with K-Means</a></li><li>using / <a href="#ch02lvl1sec16" title="K-Means in practice" class="link">K-Means in practice</a></li></ul></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>LabeledPoint<ul><li>URL / <a href="#ch02lvl1sec15" title="Naïve Bayes in practice" class="link">Naïve Bayes in practice</a></li></ul></li>
        <li>libraries<ul><li>about / <a href="#ch08lvl1sec59" title="Jobs and libraries" class="link">Jobs and libraries</a></li></ul></li>
        <li>local Hive Metastore server<ul><li>using / <a href="#ch04lvl1sec28" title="Local Hive Metastore server" class="link">Local Hive Metastore server</a></li></ul></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>markdown<ul><li>URL / <a href="#ch08lvl1sec58" title="Notebooks and folders" class="link">Notebooks and folders</a></li></ul></li>
        <li>Mazerunner, for Neo4j<ul><li>about / <a href="#ch05lvl1sec32" title="Mazerunner for Neo4j" class="link">Mazerunner for Neo4j</a></li><li>Docker, installing / <a href="#ch05lvl1sec32" title="Installing Docker" class="link">Installing Docker</a></li><li>Neo4j browser / <a href="#ch05lvl1sec32" title="The Neo4j browser" class="link">The Neo4j browser</a></li><li>algorithms / <a href="#ch05lvl1sec32" title="The Mazerunner algorithms" class="link">The Mazerunner algorithms</a></li></ul></li>
        <li>Mazerunner algorithms<ul><li>about / <a href="#ch05lvl1sec32" title="The Mazerunner algorithms" class="link">The Mazerunner algorithms</a></li><li>PageRank algorithm / <a href="#ch05lvl1sec32" title="The PageRank algorithm" class="link">The PageRank algorithm</a></li><li>closeness centrality algorithm / <a href="#ch05lvl1sec32" title="The closeness centrality algorithm" class="link">The closeness centrality algorithm</a></li><li>triangle count algorithm / <a href="#ch05lvl1sec32" title="The triangle count algorithm" class="link">The triangle count algorithm</a></li><li>connected components algorithm / <a href="#ch05lvl1sec32" title="The connected components algorithm" class="link">The connected components algorithm</a></li><li>strongly connected components algorithm / <a href="#ch05lvl1sec32" title="The strongly connected components algorithm" class="link">The strongly connected components algorithm</a></li></ul></li>
        <li>MLlib<ul><li>environment configuration / <a href="#ch02lvl1sec14" title="The environment configuration" class="link">The environment configuration</a></li></ul></li>
        <li>MNIST<ul><li>URL / <a href="#ch07lvl1sec46" title="Sourcing the data" class="link">Sourcing the data</a></li><li>about / <a href="#ch07lvl1sec49" title="The example code – MNIST" class="link">The example code – MNIST</a></li></ul></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>Naïve Bayes<ul><li>classification / <a href="#ch02lvl1sec15" title="Classification with Naïve Bayes" class="link">Classification with Naïve Bayes</a></li><li>using / <a href="#ch02lvl1sec15" title="Naïve Bayes in practice" class="link">Naïve Bayes in practice</a></li><li>URL / <a href="#ch02lvl1sec15" title="Naïve Bayes in practice" class="link">Naïve Bayes in practice</a></li></ul></li>
        <li>Neo4j browser<ul><li>about / <a href="#ch05lvl1sec32" title="The Neo4j browser" class="link">The Neo4j browser</a></li><li>URL / <a href="#ch05lvl1sec32" title="The Neo4j browser" class="link">The Neo4j browser</a></li></ul></li>
        <li>Notebook / <a href="#ch08lvl1sec58" title="Notebooks and folders" class="link">Notebooks and folders</a></li>
      </ul>
      <h2>O</h2>
      <ul>
        <li>OOM (Out of Memory) messages / <a href="#ch01lvl1sec11" title="Memory" class="link">Memory</a></li>
        <li>Oryx system<ul><li>URL / <a href="#ch01lvl1sec12" title="Cloud" class="link">Cloud</a></li></ul></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>P (Spam|Buy) / <a href="#ch02lvl1sec15" title="Theory" class="link">Theory</a></li>
        <li>PageRank algorithm<ul><li>about / <a href="#ch05lvl1sec32" title="The PageRank algorithm" class="link">The PageRank algorithm</a></li></ul></li>
        <li>Parquet files<ul><li>about / <a href="#ch04lvl1sec24" title="Importing and saving data" class="link">Importing and saving data</a></li><li>processing / <a href="#ch04lvl1sec24" title="Processing the Parquet files" class="link">Processing the Parquet files</a></li></ul></li>
        <li>performance<ul><li>examining / <a href="#ch01lvl1sec11" title="Performance" class="link">Performance</a></li><li>cluster structure / <a href="#ch01lvl1sec11" title="The cluster structure" class="link">The cluster structure</a></li><li>Hadoop file system / <a href="#ch01lvl1sec11" title="The Hadoop file system" class="link">The Hadoop file system</a></li><li>data locality / <a href="#ch01lvl1sec11" title="Data locality" class="link">Data locality</a></li><li>OOM (Out of Memory) messages, avoiding / <a href="#ch01lvl1sec11" title="Memory" class="link">Memory</a></li><li>code, tuning / <a href="#ch01lvl1sec11" title="Coding" class="link">Coding</a></li></ul></li>
        <li>PostgreSQL connector library<ul><li>URL, for download / <a href="#ch04lvl1sec28" title="A Hive-based Metastore server" class="link">A Hive-based Metastore server</a></li></ul></li>
        <li>PredictionIO<ul><li>URL / <a href="#ch01lvl1sec12" title="Cloud" class="link">Cloud</a></li></ul></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>remove function(rm) / <a href="#ch08lvl1sec62" title="Dbutils fsutils" class="link">Dbutils fsutils</a></li>
        <li>REST interface<ul><li>about / <a href="#ch09lvl1sec65" title="REST interface" class="link">REST interface</a></li><li>configuration / <a href="#ch09lvl1sec65" title="Configuration" class="link">Configuration</a></li><li>cluster management / <a href="#ch09lvl1sec65" title="Cluster management" class="link">Cluster management</a></li><li>execution context / <a href="#ch09lvl1sec65" title="The execution context" class="link">The execution context</a></li><li>command execution / <a href="#ch09lvl1sec65" title="Command execution" class="link">Command execution</a></li><li>libraries / <a href="#ch09lvl1sec65" title="Libraries" class="link">Libraries</a></li></ul></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>SeldonIO<ul><li>URL / <a href="#ch01lvl1sec12" title="Cloud" class="link">Cloud</a></li></ul></li>
        <li>Sister property / <a href="#ch05lvl1sec30" title="Overview" class="link">Overview</a></li>
        <li>Sparkling Water component, H2O<ul><li>URL / <a href="#ch07lvl1sec42" title="The processing environment" class="link">The processing environment</a>, <a href="#ch07lvl1sec45" title="Architecture" class="link">Architecture</a></li></ul></li>
        <li>Spark Machine Learning / <a href="#ch01lvl1sec08" title="Spark Machine Learning" class="link">Spark Machine Learning</a></li>
        <li>SparkOnHBase module<ul><li>URL / <a href="#ch06lvl1sec37" title="Spark on HBase" class="link">Spark on HBase</a></li></ul></li>
        <li>Spark SQL / <a href="#ch01lvl1sec08" title="Spark SQL" class="link">Spark SQL</a></li>
        <li>SQL<ul><li>using / <a href="#ch04lvl1sec26" title="Using SQL" class="link">Using SQL</a></li></ul></li>
        <li>SQL context<ul><li>about / <a href="#ch04lvl1sec23" title="The SQL context" class="link">The SQL context</a></li></ul></li>
        <li>streaming, Apache Spark / <a href="#ch01lvl1sec08" title="Spark Streaming" class="link">Spark Streaming</a></li>
        <li>stream processing / <a href="#ch01lvl1sec08" title="Spark Streaming" class="link">Spark Streaming</a></li>
        <li>strongly connected components algorithm<ul><li>about / <a href="#ch05lvl1sec32" title="The strongly connected components algorithm" class="link">The strongly connected components algorithm</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>tertiary education / <a href="#ch09lvl1sec64" title="Data visualization" class="link">Data visualization</a></li>
        <li>textFile method / <a href="#ch04lvl1sec24" title="Processing the Text files" class="link">Processing the Text files</a></li>
        <li>text files<ul><li>processing / <a href="#ch04lvl1sec24" title="Processing the Text files" class="link">Processing the Text files</a></li></ul></li>
        <li>TinkerPop<ul><li>about / <a href="#ch06lvl1sec35" title="TinkerPop" class="link">TinkerPop</a></li><li>URL / <a href="#ch06lvl1sec35" title="TinkerPop" class="link">TinkerPop</a></li></ul></li>
        <li>Titan<ul><li>about / <a href="#ch06lvl1sec34" title="Titan" class="link">Titan</a></li><li>URL / <a href="#ch06lvl1sec34" title="Titan" class="link">Titan</a>, <a href="#ch06lvl1sec36" title="Installing Titan" class="link">Installing Titan</a></li><li>installing / <a href="#ch06lvl1sec36" title="Installing Titan" class="link">Installing Titan</a></li><li>accessing, with HBase / <a href="#ch06lvl1sec37" title="Titan with HBase" class="link">Titan with HBase</a></li><li>accessing, with Cassandra / <a href="#ch06lvl1sec38" title="Titan with Cassandra" class="link">Titan with Cassandra</a></li><li>accessing, with Apache Spark / <a href="#ch06lvl1sec39" title="Accessing Titan with Spark" class="link">Accessing Titan with Spark</a></li></ul></li>
        <li>Titan, accessing with Apache Spark<ul><li>about / <a href="#ch06lvl1sec39" title="Accessing Titan with Spark" class="link">Accessing Titan with Spark</a></li><li>Gremlin shell / <a href="#ch06lvl1sec39" title="Gremlin and Groovy" class="link">Gremlin and Groovy</a></li><li>Groovy commands, executing / <a href="#ch06lvl1sec39" title="Gremlin and Groovy" class="link">Gremlin and Groovy</a></li><li>TinkerPop Hadoop Gremlin package / <a href="#ch06lvl1sec39" title="TinkerPop's Hadoop Gremlin" class="link">TinkerPop's Hadoop Gremlin</a></li><li>alternative Groovy configuration / <a href="#ch06lvl1sec39" title="Alternative Groovy configuration" class="link">Alternative Groovy configuration</a></li><li>Cassandra, using / <a href="#ch06lvl1sec39" title="Using Cassandra" class="link">Using Cassandra</a></li><li>HBase, using / <a href="#ch06lvl1sec39" title="Using HBase" class="link">Using HBase</a></li><li>file system, using / <a href="#ch06lvl1sec39" title="Using the filesystem" class="link">Using the filesystem</a></li></ul></li>
        <li>Titan, accessing with Cassandra<ul><li>about / <a href="#ch06lvl1sec38" title="Titan with Cassandra" class="link">Titan with Cassandra</a></li><li>Cassandra, installing / <a href="#ch06lvl1sec38" title="Installing Cassandra" class="link">Installing Cassandra</a></li><li>Gremlin Cassandra script / <a href="#ch06lvl1sec38" title="The Gremlin Cassandra script" class="link">The Gremlin Cassandra script</a></li><li>Spark Cassandra connector / <a href="#ch06lvl1sec38" title="The Spark Cassandra connector" class="link">The Spark Cassandra connector</a></li></ul></li>
        <li>Titan, accessing with HBase<ul><li>about / <a href="#ch06lvl1sec37" title="Titan with HBase" class="link">Titan with HBase</a></li><li>HBase cluster, using / <a href="#ch06lvl1sec37" title="The HBase cluster" class="link">The HBase cluster</a></li><li>Gremlin HBase script / <a href="#ch06lvl1sec37" title="The Gremlin HBase script" class="link">The Gremlin HBase script</a></li><li>SparkOnHBase module, using / <a href="#ch06lvl1sec37" title="Spark on HBase" class="link">Spark on HBase</a></li></ul></li>
        <li> TitanFactory.open method / <a href="#ch06lvl1sec39" title="Using Cassandra" class="link">Using Cassandra</a></li>
        <li>triangle count algorithm<ul><li>about / <a href="#ch05lvl1sec32" title="The triangle count algorithm" class="link">The triangle count algorithm</a></li></ul></li>
        <li>True Positive Rate (TPR) / <a href="#ch07lvl1sec50" title="H2O Flow" class="link">H2O Flow</a></li>
        <li>Twitter<ul><li>URL / <a href="#ch09lvl1sec64" title="A stream-based report" class="link">A stream-based report</a></li></ul></li>
      </ul>
      <h2>U</h2>
      <ul>
        <li>user-defined functions (UDFs)<ul><li>about / <a href="#ch04lvl1sec27" title="User-defined functions" class="link">User-defined functions</a></li></ul></li>
      </ul>
      <h2>V</h2>
      <ul>
        <li>velox system<ul><li>URL / <a href="#ch01lvl1sec12" title="Cloud" class="link">Cloud</a></li></ul></li>
        <li>Vendor AP / <a href="#ch06lvl1sec35" title="TinkerPop" class="link">TinkerPop</a></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
